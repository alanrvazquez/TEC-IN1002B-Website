[
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#agenda",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#agenda",
    "title": "Data preprocessing: Part II",
    "section": "Agenda",
    "text": "Agenda\n\n\nTransforming predictors\nStandarization\nDimension reduction"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#before-we-start",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#before-we-start",
    "title": "Data preprocessing: Part II",
    "section": "Before we start",
    "text": "Before we start\n\nLet’s import scikit-learn into Python together with the other relevant libraries.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import power_transform, StandardScaler \n\nWe will not use all the functions from the scikit-learn library. Instead, we will use specific functions from the sub-libraries preprocessing, feature_selection, model_selection and impute."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#categorical-predictors",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#categorical-predictors",
    "title": "Data preprocessing: Part II",
    "section": "Categorical predictors",
    "text": "Categorical predictors\nA categorical predictor takes on values that are nominal categories.\n\nFor example:\n\nType of school: Public or private.\nTreatment: New or placebo.\nGrade: Passed or not passed.\n\n\n\nThe categories can be represented by names, labels or even numbers. Their use in regression requires dummy variables, which are numeric variables."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#dummy-variables",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#dummy-variables",
    "title": "Data preprocessing: Part II",
    "section": "Dummy variables",
    "text": "Dummy variables\n\n\nThe traditional choice for a dummy variable is a binary variable, which can only take the values 0 and 1.\n\n\nInitially, a categorical variable with \\(k\\) categories requires \\(k\\) dummy variables."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#example-2",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#example-2",
    "title": "Data preprocessing: Part II",
    "section": "Example 2",
    "text": "Example 2\nA market analyst is studying quality characteristics of cars. Specifically, the analyst is investigating the miles per gallon (mpg) of cars can be predicted using:\n\n\\(X_1:\\) cylinders. Number of cylinders between 4 and 8\n\\(X_2:\\) displacement. Engine displacement (cu. inches)\n\\(X_3:\\) horsepower. Engine horsepower\n\\(X_4:\\) weight. Vehicle weight (lbs.)\n\\(X_5:\\) acceleration. Time to accelerate from 0 to 60 mph (sec.)\n\\(X_6:\\) origin. Origin of car (American, European, Japanese)"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#section",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#section",
    "title": "Data preprocessing: Part II",
    "section": "",
    "text": "The dataset is in the file “auto.xlsx”. Let’s read the data using pandas.\n\n# Load the Excel file into a pandas DataFrame.\nauto_data = pd.read_excel(\"auto.xlsx\")\n\n# Set categorical variables.\nauto_data['origin'] = pd.Categorical(auto_data['origin'])"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#section-1",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#section-1",
    "title": "Data preprocessing: Part II",
    "section": "",
    "text": "observation\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nyear\norigin\nname\n\n\n\n\n0\n1\n18.0\n8\n307.0\n130\n3504\n12.0\n70\nAmerican\nchevrolet chevelle malibu\n\n\n1\n2\n15.0\n8\n350.0\n165\n3693\n11.5\n70\nAmerican\nbuick skylark 320\n\n\n2\n3\n18.0\n8\n318.0\n150\n3436\n11.0\n70\nAmerican\nplymouth satellite\n\n\n3\n4\n16.0\n8\n304.0\n150\n3433\n12.0\n70\nAmerican\namc rebel sst"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#dealing-with-missing-values",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#dealing-with-missing-values",
    "title": "Data preprocessing: Part II",
    "section": "Dealing with missing values",
    "text": "Dealing with missing values\n\nThe dataset has missing values. In this example, we remove each row with at least one missing value.\n\n\n# Remove rows with missing values.\ncomplete_Auto = auto_data.dropna()"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#example-2-cont.",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#example-2-cont.",
    "title": "Data preprocessing: Part II",
    "section": "Example 2 (cont.)",
    "text": "Example 2 (cont.)\nCategorical predictor: Origin of a car. Three categories: American, European and Japanese.\nInitially, 3 dummy variables are required:\n\\[d_1 =\n\\begin{cases}\n1 \\text{ if observation is from an American car}\\\\\n0 \\text{ otherwise}\n\\end{cases}\\] \\[d_2 =\n\\begin{cases}\n1 \\text{ if observation is from an European car}\\\\\n0 \\text{ otherwise}\n\\end{cases}\\] \\[d_3 =\n\\begin{cases}\n1 \\text{ if observation is from a Japanese car}\\\\\n0 \\text{ otherwise}\n\\end{cases}\\]"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#section-2",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#section-2",
    "title": "Data preprocessing: Part II",
    "section": "",
    "text": "The variable Origin would then be replaced by the three dummy variables\n\n\n\nOrigin (\\(X\\))\n\\(d_1\\)\n\\(d_2\\)\n\\(d_3\\)\n\n\n\n\nAmerican\n1\n0\n0\n\n\nAmerican\n1\n0\n0\n\n\nEuropean\n0\n1\n0\n\n\nEuropean\n0\n1\n0\n\n\nAmerican\n1\n0\n0\n\n\nJapanese\n0\n0\n1\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#a-drawback",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#a-drawback",
    "title": "Data preprocessing: Part II",
    "section": "A drawback",
    "text": "A drawback\n\n\n\nA drawback with the initial dummy variables is that they are linearly dependent. That is, \\(d_1 + d_2 + d_3 = 1\\).\nTherefore, we can determine the value of \\(d_1 = 1- d_2 - d_3.\\)\nPredictive models such as linear regression are sensitive to linear dependencies among predictors.\nThe solution is to drop one of the predictor, say, \\(d_1\\), from the data."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#section-3",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#section-3",
    "title": "Data preprocessing: Part II",
    "section": "",
    "text": "The variable Origin would then be replaced by the three dummy variables.\n\n\n\nOrigin (\\(X\\))\n\\(d_2\\)\n\\(d_3\\)\n\n\n\n\nAmerican\n0\n0\n\n\nAmerican\n0\n0\n\n\nEuropean\n1\n0\n\n\nEuropean\n1\n0\n\n\nAmerican\n0\n0\n\n\nJapanese\n0\n1\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#dummy-variables-in-python",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#dummy-variables-in-python",
    "title": "Data preprocessing: Part II",
    "section": "Dummy variables in Python",
    "text": "Dummy variables in Python\n\nWe can get the dummy variables of a categorical variable using the function pd.get_dummies() from pandas.\nThe input of the function is the categorical variable.\nThe function has an extra argument called drop_first to drop the first dummy variable. It also has the argument dtype to show the values as integers.\n\ndummy_data = pd.get_dummies(complete_Auto['origin'], drop_first = True, \n                            dtype = 'int')"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#section-4",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#section-4",
    "title": "Data preprocessing: Part II",
    "section": "",
    "text": "dummy_data.head()\n\n\n\n\n\n\n\n\nEuropean\nJapanese\n\n\n\n\n0\n0\n0\n\n\n1\n0\n0\n\n\n2\n0\n0\n\n\n3\n0\n0\n\n\n4\n0\n0"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#section-5",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#section-5",
    "title": "Data preprocessing: Part II",
    "section": "",
    "text": "Now, to add the dummy variables to the dataset, we use the function concat() from pandas.\n\naugmented_auto = pd.concat([complete_Auto, dummy_data], axis = 1)\naugmented_auto.head()\n\n\n\n\n\n\n\n\nobservation\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nyear\norigin\nname\nEuropean\nJapanese\n\n\n\n\n0\n1\n18.0\n8\n307.0\n130\n3504\n12.0\n70\nAmerican\nchevrolet chevelle malibu\n0\n0\n\n\n1\n2\n15.0\n8\n350.0\n165\n3693\n11.5\n70\nAmerican\nbuick skylark 320\n0\n0\n\n\n2\n3\n18.0\n8\n318.0\n150\n3436\n11.0\n70\nAmerican\nplymouth satellite\n0\n0\n\n\n3\n4\n16.0\n8\n304.0\n150\n3433\n12.0\n70\nAmerican\namc rebel sst\n0\n0\n\n\n4\n5\n17.0\n8\n302.0\n140\n3449\n10.5\n70\nAmerican\nford torino\n0\n0"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#transforming-numerical-predictors",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#transforming-numerical-predictors",
    "title": "Data preprocessing: Part II",
    "section": "Transforming numerical predictors",
    "text": "Transforming numerical predictors\n\nA common problem with a predictor is that it may have a skewed distribution. That is, a distribution that accumulates many observations in smaller or larger values of the predictor.\nFor example, consider the distribution of the predictor DIS (weighted mean of distances to five Boston employment centers) in the Boston_data.\n\n# Load Excel file (make sure the file is in your Colab)\nBoston_data = pd.read_excel('BostonHousing.xlsx')\n\n# Drop the categorical variable.\nBoston_data['CHAS'] = pd.Categorical(Boston_data['CHAS'])"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#section-6",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#section-6",
    "title": "Data preprocessing: Part II",
    "section": "",
    "text": "Code\nplt.figure(figsize=(7,4)) # Create space for figure.\nsns.histplot(data = Boston_data, x = 'DIS') # Create the histogram.\nplt.title(\"Histogram of DIS\") # Plot title.\nplt.xlabel(\"DIS\") # X label\nplt.show() # Display the plot\n\n\n\n\n\n\n\n\n\nWe see that many of the predictor values are on the right of the plot, while few are on the left."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#problems-with-strong-skewness",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#problems-with-strong-skewness",
    "title": "Data preprocessing: Part II",
    "section": "Problems with strong skewness",
    "text": "Problems with strong skewness\n\nStrong skewness in a predictor can:\n\nDistort the relationship with the response.\nViolate the assumptions of the linear regression model (e.g., linearity, homoscedasticity).\nLead to influential outliers or poor model fit.\n\nTo correct the skewness of a predictor’s distribution, we can transform the predictor using the Box-Cox transformation"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#box-cox-transformation",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#box-cox-transformation",
    "title": "Data preprocessing: Part II",
    "section": "Box-Cox Transformation",
    "text": "Box-Cox Transformation\n\nThe Box-Cox transformation uses a family of power transformations on a predictor \\(X\\) such that \\(X' = X^{\\lambda}\\), where \\(\\lambda\\) is a parameter to be determined using the data. When \\(\\lambda = 0,\\) this means \\(X' = \\ln(X)\\), where \\(\\ln(\\cdot)\\) is the natural logarithm.\nThe Box-Cox transformation is the element of this family that results in a transformed variable that follows a normal distribution (approximately).\nThe variable \\(X\\) must be strictly positive!"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#in-python",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#in-python",
    "title": "Data preprocessing: Part II",
    "section": "In Python",
    "text": "In Python\nWe apply the Box-Cox transformation using the power_transformation function of the scikit-learn library.\n\nDIS_transform = power_transform(Boston_data[['DIS']], method = \"box-cox\")\nDIS_transform_df = pd.DataFrame(DIS_transform, columns=['DIS_boxcox'])\nDIS_transform_df.head(3)\n\n\n\n\n\n\n\n\nDIS_boxcox\n\n\n\n\n0\n0.445390\n\n\n1\n0.789572\n\n\n2\n0.789572\n\n\n\n\n\n\n\nIn the code above, we use the pd.DataFrame() to turn the transformed column into a pandas dataframe."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#section-7",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#section-7",
    "title": "Data preprocessing: Part II",
    "section": "",
    "text": "Original predictor\n\n\nCode\nplt.figure(figsize=(7,4)) # Create space for figure.\nsns.histplot(data = Boston_data, x = 'DIS') # Create the histogram.\nplt.title(\"Histogram of DIS\") # Plot title.\nplt.xlabel(\"DIS\") # X label\nplt.show() # Display the plot\n\n\n\n\n\n\n\n\n\n\nBox-Cox transformation\n\n\nCode\nplt.figure(figsize=(7,4)) # Create space for figure.\nsns.histplot(data = DIS_transform_df, x = 'DIS_boxcox') # Create the histogram.\nplt.title(\"Histogram of transformed DIS\") # Plot title.\nplt.xlabel(\"DIS (Box-Cox transformation\") # X label\nplt.show() # Display the plot"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#discussion",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#discussion",
    "title": "Data preprocessing: Part II",
    "section": "Discussion",
    "text": "Discussion\n\nThe Box-Cox transformation can:\n\nMake the distribution of a predictor more symmetric.\nHelp to linearize the relationship with the response.\n\nAfter transformation, the predictor \\(X'\\) must be used instead of the original predictor \\(X\\)."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#predictors-with-different-units",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#predictors-with-different-units",
    "title": "Data preprocessing: Part II",
    "section": "Predictors with different units",
    "text": "Predictors with different units\nMany good predictive models have issues with numeric predictors with different units:\n\nMethods such as K-nearest neighbors are based on the distance between observations. If the predictors are on different units or scales, then some predictors will have a larger weight for computing the distance.\nOther methods such as LASSO use the variances of the predictors in their calculations. Predictors with different scales will have different variances and so, those with a higher variance will play a bigger role in the calculations."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#section-8",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#section-8",
    "title": "Data preprocessing: Part II",
    "section": "",
    "text": "In a nutshell, some predictors will have a higher impact in the model due to its unit and not its information provided to it."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#standarization-1",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#standarization-1",
    "title": "Data preprocessing: Part II",
    "section": "Standarization",
    "text": "Standarization\n\nStandardization refers to centering and scaling each numeric predictor individually. It puts every predictor on the same scale.\nTo center a predictor variable, the average predictor value is subtracted from all the values.\nTherefore, the centered predictor has a zero mean (that is, its average value is zero)."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#section-9",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#section-9",
    "title": "Data preprocessing: Part II",
    "section": "",
    "text": "To scale a predictor, each of its value is divided by its standard deviation.\nScaling the data coerce the values to have a common standard deviation of one.\nIn mathematical terms, we standardize a predictor as:\n\\[{\\color{blue} \\tilde{x}_{i}} = \\frac{{ x_{i} - \\bar{x}}}{ \\sqrt{\\frac{1}{n -1} \\sum_{i=1}^{n} (x_{i} - \\bar{x})^2}},\\]\nwith \\(\\bar{x} = \\sum_{i=1}^n \\frac{x_i}{n}\\)."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#example-2-cont.-1",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#example-2-cont.-1",
    "title": "Data preprocessing: Part II",
    "section": "Example 2 (cont.)",
    "text": "Example 2 (cont.)\n\nWe concentrate on the five numerical predictors in the complete_Auto dataset.\n\n# Select specific variables.\ncomplete_sbAuto = complete_Auto[['cylinders', 'displacement', \n                                 'horsepower', 'weight', \n                                 'acceleration']]"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#two-predictors-in-original-units",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#two-predictors-in-original-units",
    "title": "Data preprocessing: Part II",
    "section": "Two predictors in original units",
    "text": "Two predictors in original units\nConsider the complete_sbAuto dataset created previously. Consider two points in the plot: \\((175, 5140)\\) and \\((69, 1613)\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe distance between these points is \\(\\sqrt{(69 - 175)^2 + (1613-5140)^2}\\) \\(= \\sqrt{11236 + 12439729}\\) \\(= 3528.592\\)."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#standarization-in-python",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#standarization-in-python",
    "title": "Data preprocessing: Part II",
    "section": "Standarization in Python",
    "text": "Standarization in Python\n\nTo standardize numerical predictors, we use the function StandardScaler(). Moreover, we apply the function to the variables using the function fit_transform().\n\n\nscaler = StandardScaler()\nXs = scaler.fit_transform(complete_sbAuto)"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#section-10",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#section-10",
    "title": "Data preprocessing: Part II",
    "section": "",
    "text": "Unfortunately, the resulting object is not a pandas data frame. We then convert this object to this format. \n\nscaled_df = pd.DataFrame(Xs, columns = complete_sbAuto.columns)\nscaled_df.head()\n\n\n\n\n\n\n\n\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\n\n\n\n\n0\n1.483947\n1.077290\n0.664133\n0.620540\n-1.285258\n\n\n1\n1.483947\n1.488732\n1.574594\n0.843334\n-1.466724\n\n\n2\n1.483947\n1.182542\n1.184397\n0.540382\n-1.648189\n\n\n3\n1.483947\n1.048584\n1.184397\n0.536845\n-1.285258\n\n\n4\n1.483947\n1.029447\n0.924265\n0.555706\n-1.829655"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#two-predictors-in-standardized-units",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#two-predictors-in-standardized-units",
    "title": "Data preprocessing: Part II",
    "section": "Two predictors in standardized units",
    "text": "Two predictors in standardized units\nIn the new scale, the two points are now: \\((1.82, 2.53)\\) and \\((-0.91, -1.60)\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe distance between these points is \\(\\sqrt{(-0.91 - 1.82)^2 + (-1.60-2.53)^2}\\) \\(= \\sqrt{7.45 + 17.05} = 4.95\\)."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#discussion-1",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#discussion-1",
    "title": "Data preprocessing: Part II",
    "section": "Discussion",
    "text": "Discussion\n\n\nStandardized predictors are generally used to improve the numerical stability of some calculations.\nIt is generally recommended to always standardize numeric predictors. Perhaps the only exception would be if we consider a linear regression model.\nA drawback of these transformations is the loss of interpretability since the data are no longer in the original units.\nStandardizing the predictors does not affect their correlation."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#dimension-reduction-1",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#dimension-reduction-1",
    "title": "Data preprocessing: Part II",
    "section": "Dimension reduction",
    "text": "Dimension reduction\nHere, we will discuss methods to reduce the number of predictors in a dataset. In other words, to reduce the dimension of the dataset.\nHigh-dimensional datasets can cause problems when training or fitting linear regression models. It also happens that a high-dimensional dataset can also have poorer predictive performance than a dataset with few, well selected predictors.\nIn particular, we will discuss the methods:\n\nFiltering predictors based on near-zero variance and correlation.\nPrincipal component analysis."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#removing-predictors",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#removing-predictors",
    "title": "Data preprocessing: Part II",
    "section": "Removing predictors",
    "text": "Removing predictors\n\nThere are potential advantages to removing predictors prior to modeling:\n\n\nFewer predictors means decreased computational time and complexity.\nIf two predictors are highly-correlated, they are measuring the same underlying information. So, removing one should not compromise the performance of the model.\n\n\n\nHere, we will see two techniques to remove predictors."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#near-zero-variance-predictors",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#near-zero-variance-predictors",
    "title": "Data preprocessing: Part II",
    "section": "Near-Zero variance predictors",
    "text": "Near-Zero variance predictors\n\nA near-zero variance predictor variable is one that has only a handful of unique values that occur with very low frequencies.\n\n\nIf the predictor has a single unique value, then it is called a zero-variance predictor variable.\n\n\n\nSince the values of this predictor variable do not vary or change at all, this predictor does not provide any information to the model and must be discarded."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#example-3",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#example-3",
    "title": "Data preprocessing: Part II",
    "section": "Example 3",
    "text": "Example 3\n\nWe consider a data set related to Glass identification. The data has 214 glass samples labeled as one of seven glass categories. There are nine predictors including the refractive index (RI) and percentages of eight elements: Na, Mg, Al, Si, K, Ca, Ba, and Fe.\nLet’s read the data set. Note that this is a modified version of the original data.\n\n# Load Excel file (make sure the file is in your Colab)\nglass_data = pd.read_excel('glass.xlsx')\n\n# Drop the categorical variable.\nsb_glass = glass_data.drop(columns=['Type'])"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#in-python-1",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#in-python-1",
    "title": "Data preprocessing: Part II",
    "section": "In Python",
    "text": "In Python\nWe use the function VarianceThreshold() to set the threshold for determining a low sample variance. We also use other functions such as selector.fit() and selector.get_support().\n\n# Set threshold\nselector = VarianceThreshold(threshold=0.01)\n\n# Apply threshold.\nselector.fit(sb_glass)\n\n# Identify predictors with low variance.\nlow_variance_cols = sb_glass.columns[~selector.get_support()]\n\n# Print the list of predictors.\nprint(f\"Low variance columns: {low_variance_cols}\")\n\nLow variance columns: Index(['RI', 'Fe'], dtype='object')"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#section-11",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#section-11",
    "title": "Data preprocessing: Part II",
    "section": "",
    "text": "After identifying predictors with low variability, we remove them from the problem because they do not add much to the problem. To this end, we use the command below.\n\n\n# Removing problematic predictors\nsb_reduced_glass = sb_glass.drop(columns=low_variance_cols)"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#between-predictor-correlation",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#between-predictor-correlation",
    "title": "Data preprocessing: Part II",
    "section": "Between-predictor correlation",
    "text": "Between-predictor correlation\n\nCollinearity is the technical term for the situation where two predictors have a substantial correlation with each other.\nIf two or more predictors are highly correlated (either negatively or positively), then methods such as the linear regression model will not work!\nTo visualize the severity of collinearity between predictors, we calculate and visualize the correlation matrix."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#example-2-cont.-2",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#example-2-cont.-2",
    "title": "Data preprocessing: Part II",
    "section": "Example 2 (cont.)",
    "text": "Example 2 (cont.)\n\nWe concentrate on the five numerical predictors in the complete_Auto dataset.\n\n# Select specific variables.\ncomplete_sbAuto = complete_Auto[['cylinders', 'displacement', \n                                 'horsepower', 'weight', \n                                 'acceleration']]"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#correlation-matrix",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#correlation-matrix",
    "title": "Data preprocessing: Part II",
    "section": "Correlation matrix",
    "text": "Correlation matrix\n\nIn Python, we calculate the correlation matrix using the command below.\n\ncorrelation_matrix = complete_sbAuto.corr()\nprint(correlation_matrix)\n\n              cylinders  displacement  horsepower    weight  acceleration\ncylinders      1.000000      0.950823    0.842983  0.897527     -0.504683\ndisplacement   0.950823      1.000000    0.897257  0.932994     -0.543800\nhorsepower     0.842983      0.897257    1.000000  0.864538     -0.689196\nweight         0.897527      0.932994    0.864538  1.000000     -0.416839\nacceleration  -0.504683     -0.543800   -0.689196 -0.416839      1.000000"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#section-12",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#section-12",
    "title": "Data preprocessing: Part II",
    "section": "",
    "text": "Next, we plot the correlation matrix using the function heatmap() from seaborn. The argument annot shows the actual value of the pair-wise correlations, and cmap shows a nice color theme.\n\nplt.figure(figsize=(3,3))\nsns.heatmap(correlation_matrix, cmap='coolwarm', annot = True)"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#section-13",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#section-13",
    "title": "Data preprocessing: Part II",
    "section": "",
    "text": "The predictors cylinders and displacement are highly correlated. In fact, their correlation is 0.95."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part2.slides.html#in-practice",
    "href": "PreProcessing/PreProcessing_Part2.slides.html#in-practice",
    "title": "Data preprocessing: Part II",
    "section": "In practice",
    "text": "In practice\n\nWe deal with collinearity by removing the minimum number of predictors to ensure that all pairwise correlations are below a certain threshold, say, 0.75.\nWe can identify the variables that are highly correlated using quite complex code. However, here we will do it manually using the correlation map.\n\n# Dataset without highly correlated predictors.\nreduced_auto = complete_sbAuto[ ['weight', 'acceleration']]"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IN1002B Introduction to Data Science Projects",
    "section": "",
    "text": "Course topics\n\nModule 2\n\nIntroduction to Data Science (slides)\nTools: Part 1 (slides) (colab)\nTools: Part 2 (slides) (colab)\nPreprocessing: Part 1 (slides) (colab)\nPreprocessing: Part 2 (slides)\n\n\n\nModule 3\n\nIntroduction to Linear Regression (slides) (colab)\nModel Inference and Evaluation (slides) (colab)\nAdditional Topics (slides) (colab)\nLogistic Regression (slides) (colab)\n\n\n\n\nAbout the author\nAlan R. Vazquez (website) is a Research Professor at the Department of Industrial Engineering at Tecnologico de Monterrey, Monterrey campus.\n\nLicense\n\nIN1002B Introduction to Data Science Projects by Alan Roberto Vazquez is licensed under CC BY-NC-SA 4.0"
  },
  {
    "objectID": "index.slides.html#course-topics",
    "href": "index.slides.html#course-topics",
    "title": "IN1002B Introduction to Data Science Projects",
    "section": "Course topics",
    "text": "Course topics\nModule 2\n\nIntroduction to Data Science (slides)\nTools: Part 1 (slides) (colab)\nTools: Part 2 (slides) (colab)\nPreprocessing: Part 1 (slides) (colab)\nPreprocessing: Part 2 (slides)\n\nModule 3\n\nIntroduction to Linear Regression (slides) (colab)\nModel Inference and Evaluation (slides) (colab)\nAdditional Topics (slides) (colab)\nLogistic Regression (slides) (colab)"
  },
  {
    "objectID": "index.slides.html#about-the-author",
    "href": "index.slides.html#about-the-author",
    "title": "IN1002B Introduction to Data Science Projects",
    "section": "About the author",
    "text": "About the author\nAlan R. Vazquez (website) is a Research Professor at the Department of Industrial Engineering at Tecnologico de Monterrey, Monterrey campus.\nLicense\n\nIN1002B Introduction to Data Science Projects by Alan Roberto Vazquez is licensed under CC BY-NC-SA 4.0"
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#agenda",
    "href": "IntroductionDS/Introduction.slides.html#agenda",
    "title": "Introduction to Data Science",
    "section": "Agenda",
    "text": "Agenda\n\n\nIntroduction to data science\nBasic notation and terminology"
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#data-science-is",
    "href": "IntroductionDS/Introduction.slides.html#data-science-is",
    "title": "Introduction to Data Science",
    "section": "Data science is …",
    "text": "Data science is …\na multidisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from vast amounts of structured and unstructured data."
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#in-2004",
    "href": "IntroductionDS/Introduction.slides.html#in-2004",
    "title": "Introduction to Data Science",
    "section": "In 2004 …",
    "text": "In 2004 …\nHurricane Frances was sweeping through the Caribbean and threatening to make a direct hit on Florida’s Atlantic coast.\n\n\n\n\n\n\n\n\n\n\n\n\n\nResidents headed for higher ground, but in Arkansas, Wal-Mart executives saw a big opportunity for one of their newest data-driven weapons: predictive technology."
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#section",
    "href": "IntroductionDS/Introduction.slides.html#section",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "A week before the storm made landfall, Linda M. Dillman, Wal-Mart’s chief information officer, pressed her staff to create forecasts based on what had happened when Hurricane Charley hit several weeks earlier.\n\nBacked by trillions of bytes of shopper history stored in Wal-Mart’s data warehouse, she said, the company could “start predicting what’s going to happen, rather than waiting for it to happen,” as she put it."
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#the-result",
    "href": "IntroductionDS/Introduction.slides.html#the-result",
    "title": "Introduction to Data Science",
    "section": "The result",
    "text": "The result\n\n\nThe New York Times reported\n\n\n“… Experts analyzed the data and found that stores would indeed need certain products, and not just the usual flashlights.”\n\n\n\nDillman said\n\n\n“We didn’t know in the past that strawberry Pop-Tarts increase their sales, like seven times their normal sales rate, before a hurricane.”"
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#the-scheme-of-data-science",
    "href": "IntroductionDS/Introduction.slides.html#the-scheme-of-data-science",
    "title": "Introduction to Data Science",
    "section": "The scheme of data science",
    "text": "The scheme of data science"
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#business-understanding",
    "href": "IntroductionDS/Introduction.slides.html#business-understanding",
    "title": "Introduction to Data Science",
    "section": "Business understanding",
    "text": "Business understanding\n\n\nBusiness understanding refers to defining the business problem to be solved.\nThe goal is to reframe the business problem as a data science problem.\nOften, reframing the problem and designing a solution is an iterative process."
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#common-data-science-problems",
    "href": "IntroductionDS/Introduction.slides.html#common-data-science-problems",
    "title": "Introduction to Data Science",
    "section": "Common data science problems",
    "text": "Common data science problems\n\nRegression attempts to estimate or predict, for each individual, the numerical value of some variable for that individual. For example, “How much will a given customer use the service?”\n\nClassification (or class probability estimation) attempts to predict, for each individual in a population, which of a (small) set of classes this individual belongs to. For example, “Among all customers of T-Mobile, which are likely to respond to a given offer?”"
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#other-problems",
    "href": "IntroductionDS/Introduction.slides.html#other-problems",
    "title": "Introduction to Data Science",
    "section": "Other problems",
    "text": "Other problems\n\nClustering attempts to group individuals in a population together by their similarity, but not driven by any specific purpose. For example, “Do our customers form natural groups or segments?”"
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#discussion",
    "href": "IntroductionDS/Introduction.slides.html#discussion",
    "title": "Introduction to Data Science",
    "section": "Discussion",
    "text": "Discussion\n\n\nOften, recasting the problem and designing a solution is an iterative process.\nThe initial formulation may not be complete or optimal, so multiple iterations may be necessary for an acceptable solution formulation.\nThey key to a great success is a creative problem formulation by some analyst regarding how to cast the business problem as one or more data science problems."
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#data-understanding-i",
    "href": "IntroductionDS/Introduction.slides.html#data-understanding-i",
    "title": "Introduction to Data Science",
    "section": "Data understanding I",
    "text": "Data understanding I\n\n\n\nIf the goal is to solve a business problem, the data that makes up the raw material available from which the solution will be built.\nThe available data rarely matches the problem.\nFor example, historical data is often collected for purposes unrelated to the current business problem or for no explicit purpose at all."
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#data-understanding-ii",
    "href": "IntroductionDS/Introduction.slides.html#data-understanding-ii",
    "title": "Introduction to Data Science",
    "section": "Data understanding II",
    "text": "Data understanding II\n\n\nThe costs of data vary. Some data will be available for free while others will require effort to obtain.\n\n\n\nA critical part of the data understanding phase is estimating the costs and benefits of each data source and deciding wether further investment is merited.\nEven after all datasets are acquired, collating them may require additional effort."
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#our-goal",
    "href": "IntroductionDS/Introduction.slides.html#our-goal",
    "title": "Introduction to Data Science",
    "section": "Our goal",
    "text": "Our goal\n\n\nOur goal is to turn data into information that answers useful questions."
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#data-classes",
    "href": "IntroductionDS/Introduction.slides.html#data-classes",
    "title": "Introduction to Data Science",
    "section": "Data classes",
    "text": "Data classes\n\n\n\nText\n\n\nImages\n\nVideo\n\n\nAudio"
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#numerical-data",
    "href": "IntroductionDS/Introduction.slides.html#numerical-data",
    "title": "Introduction to Data Science",
    "section": "Numerical data",
    "text": "Numerical data\n\nData science methodology is based on numerical data given in tables.\n\n\nIn fact, texts, images, videos or audios are transformed into this format to process them.\n\n\nIn this course, we will assume that the data is in a table."
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#notation-and-terminology",
    "href": "IntroductionDS/Introduction.slides.html#notation-and-terminology",
    "title": "Introduction to Data Science",
    "section": "Notation and terminology",
    "text": "Notation and terminology\n\nExplanatory variables or predictors:\n\n\\(X\\) represents an explanatory variable or predictor.\n\\(\\boldsymbol{X} = (X_1, X_2, \\ldots, X_p)\\) represents a whole collection of \\(p\\) predictors.\n\nOutcome or response:\n\n\\(Y\\) represents the response variable, which we’ll try to predict."
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#two-main-problems",
    "href": "IntroductionDS/Introduction.slides.html#two-main-problems",
    "title": "Introduction to Data Science",
    "section": "Two main problems",
    "text": "Two main problems\n\nRegression problems. The response \\(Y\\) is quantitative. For example, a person’s income, the value of a house, the blood pressure of a patient.\nClassification problems. The response \\(Y\\) is qualitative and has \\(K\\) different categories. For example, the brand of a product purchased (A, B, C), or whether a person defaults on a debt (yes or no).\nThe predictors (\\(\\boldsymbol{X}\\)) can be qualitative or quantitative."
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#example",
    "href": "IntroductionDS/Introduction.slides.html#example",
    "title": "Introduction to Data Science",
    "section": "Example",
    "text": "Example\n\nWhat factors explain the presence of Type II diabetes on a person?\n\n\\(Y\\) is a 1 if a person has Type II diabetes, a 0 if not.\nThe predictors (\\(\\boldsymbol{X}\\)) might include: income, zip code, age, weight, height, gender and race."
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#a-statistical-model",
    "href": "IntroductionDS/Introduction.slides.html#a-statistical-model",
    "title": "Introduction to Data Science",
    "section": "A statistical model",
    "text": "A statistical model\n\nA statistical model is a mathematical equation that embodies statistical assumptions concerning the generation of data.\nTechnically, it has the following form:\n\\[Y = f(\\boldsymbol{X}) + \\epsilon \\]\nwhere \\(Y\\) is a quantitative response, \\(f(\\boldsymbol{X})\\) is the function that relates the predictors, \\(\\boldsymbol{X}\\), to the \\(Y\\), and \\(\\epsilon\\) is the (random) error term."
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#true-and-estimated-models",
    "href": "IntroductionDS/Introduction.slides.html#true-and-estimated-models",
    "title": "Introduction to Data Science",
    "section": "True and estimated models",
    "text": "True and estimated models\n\n\\(f(\\boldsymbol{X})\\) represents the TRUTH. The true relationship between \\(\\boldsymbol{X}\\) and \\(Y\\).\n\nUnknown\nVery complex\n\n\n\\(\\hat{f}(\\boldsymbol{X})\\) represents an approximation or estimate of the true model constructed using data.\n\nIdeally, interpretable (but not necessarily)"
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#two-datasets",
    "href": "IntroductionDS/Introduction.slides.html#two-datasets",
    "title": "Introduction to Data Science",
    "section": "Two datasets",
    "text": "Two datasets\n\n“Training” data are data used to construct \\(\\hat{f}(\\boldsymbol{X})\\).\n“Testing” data are data that were NOT used in the fitting process, but are used to test how well your model performs on unseen data."
  },
  {
    "objectID": "IntroductionDS/Introduction.slides.html#yogi-berra",
    "href": "IntroductionDS/Introduction.slides.html#yogi-berra",
    "title": "Introduction to Data Science",
    "section": "Yogi Berra",
    "text": "Yogi Berra\n\n\nIt’s though to make predictions, especially about the future."
  },
  {
    "objectID": "Tools/Tools2.slides.html#agenda",
    "href": "Tools/Tools2.slides.html#agenda",
    "title": "Tools for Data Science: Part II",
    "section": "Agenda",
    "text": "Agenda\n\n\nReview of data types and summary statistics\nData visualizations"
  },
  {
    "objectID": "Tools/Tools2.slides.html#types-of-data-i",
    "href": "Tools/Tools2.slides.html#types-of-data-i",
    "title": "Tools for Data Science: Part II",
    "section": "Types of data I",
    "text": "Types of data I\n\nWhen a numerical quantity designating how much or how many is assigned to each item in the sample, the resulting set of values is numerical or quantitative.\n\nHeight (in ft).\nWeight (in lbs).\nAge (in years)."
  },
  {
    "objectID": "Tools/Tools2.slides.html#types-of-data-ii",
    "href": "Tools/Tools2.slides.html#types-of-data-ii",
    "title": "Tools for Data Science: Part II",
    "section": "Types of data II",
    "text": "Types of data II\n\nWhen sample items are placed into categories and category names are assigned to the sample items, the data are categorical or qualitative.\n\nHair color.\nCountry of origin.\nZIP code."
  },
  {
    "objectID": "Tools/Tools2.slides.html#data-types",
    "href": "Tools/Tools2.slides.html#data-types",
    "title": "Tools for Data Science: Part II",
    "section": "Data types",
    "text": "Data types"
  },
  {
    "objectID": "Tools/Tools2.slides.html#example-1",
    "href": "Tools/Tools2.slides.html#example-1",
    "title": "Tools for Data Science: Part II",
    "section": "Example 1",
    "text": "Example 1\nLet’s load the data in “penguins.xlsx”.\n\n# Load pandas.\nimport pandas as pd\n\n# Load the Excel file into a pandas DataFrame.\npenguins_data = pd.read_excel(\"penguins.xlsx\")\n\n# Print the first 4 rows of the dataset.\npenguins_data.head(4)\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007"
  },
  {
    "objectID": "Tools/Tools2.slides.html#section",
    "href": "Tools/Tools2.slides.html#section",
    "title": "Tools for Data Science: Part II",
    "section": "",
    "text": "In Python, we check the type of each variable in a dataset using the function info().\n\npenguins_data.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 8 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            344 non-null    object \n 1   island             344 non-null    object \n 2   bill_length_mm     342 non-null    float64\n 3   bill_depth_mm      342 non-null    float64\n 4   flipper_length_mm  342 non-null    float64\n 5   body_mass_g        342 non-null    float64\n 6   sex                333 non-null    object \n 7   year               344 non-null    int64  \ndtypes: float64(4), int64(1), object(3)\nmemory usage: 21.6+ KB"
  },
  {
    "objectID": "Tools/Tools2.slides.html#general-python-formats",
    "href": "Tools/Tools2.slides.html#general-python-formats",
    "title": "Tools for Data Science: Part II",
    "section": "General Python formats",
    "text": "General Python formats\n\n\nfloat64 format for numerical variables with decimals.\nint64 format for numerical variables with integers.\nobject format for general variables with characters."
  },
  {
    "objectID": "Tools/Tools2.slides.html#define-categorical-variables",
    "href": "Tools/Tools2.slides.html#define-categorical-variables",
    "title": "Tools for Data Science: Part II",
    "section": "Define categorical variables",
    "text": "Define categorical variables\nTechnically, the variable sex in penguins_data is categorical. To explicitly tell this to Python, we use the following code.\n\npenguins_data['sex'] = pd.Categorical(penguins_data['sex'])\n\nSetting sex to categorical allows us to use effective visualization for this data.\nWe do the same for the other categorical variables species and island.\n\npenguins_data['species'] = pd.Categorical(penguins_data['species'])\npenguins_data['island'] = pd.Categorical(penguins_data['island'])"
  },
  {
    "objectID": "Tools/Tools2.slides.html#section-1",
    "href": "Tools/Tools2.slides.html#section-1",
    "title": "Tools for Data Science: Part II",
    "section": "",
    "text": "Let’s check the type of variables again.\n\npenguins_data.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 8 columns):\n #   Column             Non-Null Count  Dtype   \n---  ------             --------------  -----   \n 0   species            344 non-null    category\n 1   island             344 non-null    category\n 2   bill_length_mm     342 non-null    float64 \n 3   bill_depth_mm      342 non-null    float64 \n 4   flipper_length_mm  342 non-null    float64 \n 5   body_mass_g        342 non-null    float64 \n 6   sex                333 non-null    category\n 7   year               344 non-null    int64   \ndtypes: category(3), float64(4), int64(1)\nmemory usage: 14.9 KB"
  },
  {
    "objectID": "Tools/Tools2.slides.html#summary-statistics",
    "href": "Tools/Tools2.slides.html#summary-statistics",
    "title": "Tools for Data Science: Part II",
    "section": "Summary statistics",
    "text": "Summary statistics\n\nA sample is often a long list of numbers. To help make the important features of a sample stand out, we compute summary statistics.\nFor numerical data, the most popular summary statistics are:\n\n\nSample mean\nSample variance and sample standard deviation\nSample quartiles\nSample maximum and minimum"
  },
  {
    "objectID": "Tools/Tools2.slides.html#sample-mean",
    "href": "Tools/Tools2.slides.html#sample-mean",
    "title": "Tools for Data Science: Part II",
    "section": "Sample mean",
    "text": "Sample mean\n\nLet \\(y_1, y_2, \\ldots, y_n\\) be an observed sample of size \\(n\\).\nThe sample mean is\n\\[\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n} y_i = \\frac{y_1 + y_2 + \\cdots + y_n}{n}.\\]\nThe sample mean gives an indication of the center of the data."
  },
  {
    "objectID": "Tools/Tools2.slides.html#in-python",
    "href": "Tools/Tools2.slides.html#in-python",
    "title": "Tools for Data Science: Part II",
    "section": "In Python",
    "text": "In Python\n\nThe sample mean is calculated using the function .agg() with “mean”.\n\nbill_length_mean = (penguins_data\n                    .filter(['bill_length_mm'], axis = 1)\n                    .agg(\"mean\")\n                    )\nprint(bill_length_mean)\n\nbill_length_mm    43.92193\ndtype: float64\n\n\nWe use the function print to show the number. Otherwise, Python will show the computer type of value stored in bill_length_mean."
  },
  {
    "objectID": "Tools/Tools2.slides.html#section-2",
    "href": "Tools/Tools2.slides.html#section-2",
    "title": "Tools for Data Science: Part II",
    "section": "",
    "text": "You can also round the result to, say, three decimals.\n\nprint( round(bill_length_mean, 3) )\n\nbill_length_mm    43.922\ndtype: float64"
  },
  {
    "objectID": "Tools/Tools2.slides.html#sample-variance",
    "href": "Tools/Tools2.slides.html#sample-variance",
    "title": "Tools for Data Science: Part II",
    "section": "Sample variance",
    "text": "Sample variance\n\nLet \\(y_1, y_2, \\ldots, y_n\\) be an observed sample of size \\(n\\). The sample mean is\n\\[\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (y_i - \\bar{y})^2 = \\frac{(y_1 - \\bar{y})^2  + \\cdots + (y_n - \\bar{y})^2}{n-1}\n\\]\n\nThe sample variance is like an average of the squared differences between each observation and the sample mean.\nIt gives an indication of how spread out the data are."
  },
  {
    "objectID": "Tools/Tools2.slides.html#section-3",
    "href": "Tools/Tools2.slides.html#section-3",
    "title": "Tools for Data Science: Part II",
    "section": "",
    "text": "In Python, the sample variance is calculated using the function agg() with “var”.\n\nbill_length_var = (penguins_data\n                    .filter(['bill_length_mm'], axis = 1)\n                    .agg(\"var\")\n                    )\nprint( round(bill_length_var, 3) )\n\nbill_length_mm    29.807\ndtype: float64"
  },
  {
    "objectID": "Tools/Tools2.slides.html#sample-standard-deviation",
    "href": "Tools/Tools2.slides.html#sample-standard-deviation",
    "title": "Tools for Data Science: Part II",
    "section": "Sample standard deviation",
    "text": "Sample standard deviation\nA drawback of the sample variance is that it is not on the same scale as the actual observations.\nTo obtain a measure of spread whose units are the same as those of the sample, we simply take the squared root of the sample variance\n\\[\ns = \\left(\\frac{1}{n-1} \\sum_{i=1}^{n} (y_i - \\bar{y})^2 \\right)^{1/2}\n\\]\nThis quantity is known as the sample standard deviation. It is in the same units as the observations."
  },
  {
    "objectID": "Tools/Tools2.slides.html#section-4",
    "href": "Tools/Tools2.slides.html#section-4",
    "title": "Tools for Data Science: Part II",
    "section": "",
    "text": "In Python, the sample variance is calculated using the function agg() with “std”.\n\nbill_length_std = (penguins_data\n                    .filter(['bill_length_mm'], axis = 1)\n                    .agg(\"std\")\n                    )\nprint( round(bill_length_std, 3) )\n\nbill_length_mm    5.46\ndtype: float64"
  },
  {
    "objectID": "Tools/Tools2.slides.html#sample-quartiles",
    "href": "Tools/Tools2.slides.html#sample-quartiles",
    "title": "Tools for Data Science: Part II",
    "section": "Sample quartiles",
    "text": "Sample quartiles\n\nThe sample median is the middle number of the ordered data values.\n\nSample quartiles divide the data as nearly as possible into quarters:\n\n\nFirst quartile (\\(Q_1\\)) is the median of the lower half of the data.\nSecond quartile (\\(Q_2\\)) is the median of the data.\nThird quartile (\\(Q_3\\)) is the median of the upper half of the data."
  },
  {
    "objectID": "Tools/Tools2.slides.html#section-6",
    "href": "Tools/Tools2.slides.html#section-6",
    "title": "Tools for Data Science: Part II",
    "section": "",
    "text": "In Python, the quartiles are calculated using the function quantile().\n\n# Set the quantiles.\nset_quantiles = [0.25, 0.5, 0.75]\n# Compute the quantiles.\n(penguins_data\n .filter(['bill_length_mm'], axis = 1)\n .agg(\"quantile\", q = set_quantiles)\n)\n\n\n\n\n\n\n\n\nbill_length_mm\n\n\n\n\n0.25\n39.225\n\n\n0.50\n44.450\n\n\n0.75\n48.500"
  },
  {
    "objectID": "Tools/Tools2.slides.html#sample-maximum-and-minimum",
    "href": "Tools/Tools2.slides.html#sample-maximum-and-minimum",
    "title": "Tools for Data Science: Part II",
    "section": "Sample maximum and minimum",
    "text": "Sample maximum and minimum\nOther relevant summary statistics are the maximum and minimum, which are calculated using the functions max() and min(), respectively.\n\nbill_length_max = (penguins_data\n                   .filter(['bill_length_mm'], axis = 1)\n                   .agg(\"max\")\n                  )\nprint(bill_length_max)\n\nbill_length_mm    59.6\ndtype: float64\n\n\n\nbill_length_min = (penguins_data\n                   .filter(['bill_length_mm'], axis = 1)\n                   .agg(\"min\")\n                  )\nprint(bill_length_min)\n\nbill_length_mm    32.1\ndtype: float64"
  },
  {
    "objectID": "Tools/Tools2.slides.html#summary-statistics-for-categorical-data",
    "href": "Tools/Tools2.slides.html#summary-statistics-for-categorical-data",
    "title": "Tools for Data Science: Part II",
    "section": "Summary statistics for categorical data",
    "text": "Summary statistics for categorical data\n\nThe most commonly used statistical summaries for categorical data are:\n\nThe frequency of a category is the number of observations that belong to that category.\nThe relative frequency is the frequency divided by the total number of observations."
  },
  {
    "objectID": "Tools/Tools2.slides.html#frequency-table",
    "href": "Tools/Tools2.slides.html#frequency-table",
    "title": "Tools for Data Science: Part II",
    "section": "Frequency table",
    "text": "Frequency table\nSummarizes a categorical variable by counting the values per category.\n\n\n\n(penguins_data\n  .filter(['species'], axis = 1)\n  .value_counts()\n)  \n\nspecies  \nAdelie       152\nGentoo       124\nChinstrap     68\nName: count, dtype: int64\n\n\n\n\n\n\nEspecie\nFrecuencia\n\n\n\n\nAdelie\n152\n\n\nChinstrap\n68\n\n\nGentoo\n124\n\n\nTotal\n344\n\n\n\n\n\nFrequency: Number of observations in each category.\nTotal: Total sum of observations.\n\n\n\nVentajas de las frequencias.\nResumen claro y conciso de los datos categóricos.\nFacilita la identificación de patrones y tendencias.\nAyuda en la toma de decisiones informadas."
  },
  {
    "objectID": "Tools/Tools2.slides.html#relative-frequency-table",
    "href": "Tools/Tools2.slides.html#relative-frequency-table",
    "title": "Tools for Data Science: Part II",
    "section": "Relative Frequency Table",
    "text": "Relative Frequency Table\nSummarizes a categorical variable by calculating the proportion of values per category.\n\n\n\n(penguins_data\n .filter(['species'], axis = 1)\n .value_counts(normalize = True)\n)\n\nspecies  \nAdelie       0.441860\nGentoo       0.360465\nChinstrap    0.197674\nName: proportion, dtype: float64\n\n\n\n\n\n\nSpecie\nRelative Frequency\n\n\n\n\nAdelie\n0.4418605\n\n\nChinstrap\n0.1976744\n\n\nGentoo\n0.3604651\n\n\nSuma\n1\n\n\n\n\n\nRelative frequency: Number of observations in each category divided by the total.\n\n\nLa ventaja de la frequencia relativa es que se puede interpretar como una probabilidad. Lo que da mas información."
  },
  {
    "objectID": "Tools/Tools2.slides.html#example-2",
    "href": "Tools/Tools2.slides.html#example-2",
    "title": "Tools for Data Science: Part II",
    "section": "Example 2",
    "text": "Example 2\n\nA criminologist is developing a rule-based system to classify the types of glasses encountered in criminal investigations.\nThe data consist of 214 glass samples labeled as one of seven class categories.\nThere are nine predictors, including refractive index and percentages of eight elements: Na, Mg, AL, Is, K, Ca, Ba, and Fe. The response is the type of glass."
  },
  {
    "objectID": "Tools/Tools2.slides.html#section-7",
    "href": "Tools/Tools2.slides.html#section-7",
    "title": "Tools for Data Science: Part II",
    "section": "",
    "text": "The dataset is in the file “glass.xlsx”. Let’s load it using pandas.\n\n# Load the Excel file into a pandas DataFrame.\nglass_data = pd.read_excel(\"glass.xlsx\")\n\n\nThe variable Type is categorical. So, let’s ensure Python knows this using the code below.\n\nglass_data['Type'] = pd.Categorical(glass_data['Type'])"
  },
  {
    "objectID": "Tools/Tools2.slides.html#matplotlib-library",
    "href": "Tools/Tools2.slides.html#matplotlib-library",
    "title": "Tools for Data Science: Part II",
    "section": "matplotlib library",
    "text": "matplotlib library\n\nmatplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python\nIt is widely used in the data science community for plotting data in various formats\nIdeal for creating simple visualizations like line plots, bar charts, scatter plots, and more\nhttps://matplotlib.org/"
  },
  {
    "objectID": "Tools/Tools2.slides.html#seaborn-library",
    "href": "Tools/Tools2.slides.html#seaborn-library",
    "title": "Tools for Data Science: Part II",
    "section": "seaborn library",
    "text": "seaborn library\n\nseaborn is a Python library built on top of Matplotlib\nDesigned to make statistical data visualization easy and beautiful\nIdeal for creating informative and attractive visualizations with minimal code\nhttps://seaborn.pydata.org/index.html"
  },
  {
    "objectID": "Tools/Tools2.slides.html#importing-the-libraries",
    "href": "Tools/Tools2.slides.html#importing-the-libraries",
    "title": "Tools for Data Science: Part II",
    "section": "Importing the libraries",
    "text": "Importing the libraries\n\nThe matplotlib and seaborn libraries are pre-installed in Google Colab. However, we need to inform Google Colab that we want to use them and its functions using the following command:\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nSimilar to pandas, the command as sns allows us to have a short name for seaborn. Similarly, we rename matplotlib as plt."
  },
  {
    "objectID": "Tools/Tools2.slides.html#histogram",
    "href": "Tools/Tools2.slides.html#histogram",
    "title": "Tools for Data Science: Part II",
    "section": "Histogram",
    "text": "Histogram\n\nGraphical display that gives an idea of the “shape” of the sample, indicating regions where sample points are concentrated and regions where they are sparse.\n\nThe bars of the histogram touch each other. A space indicates that there are no observations in that interval."
  },
  {
    "objectID": "Tools/Tools2.slides.html#histogram-of-na",
    "href": "Tools/Tools2.slides.html#histogram-of-na",
    "title": "Tools for Data Science: Part II",
    "section": "Histogram of Na",
    "text": "Histogram of Na\nTo create a histogram, we use the function histplot() from seabron.\n\n\nCode\nplt.figure(figsize=(7,4)) # Create space for figure.\nsns.histplot(data = glass_data, x = 'Na') # Create the histogram.\nplt.title(\"Histogram of Na\") # Plot title.\nplt.xlabel(\"Na\") # X label\nplt.show() # Display the plot"
  },
  {
    "objectID": "Tools/Tools2.slides.html#box-plot",
    "href": "Tools/Tools2.slides.html#box-plot",
    "title": "Tools for Data Science: Part II",
    "section": "Box plot",
    "text": "Box plot\n\nA box plot is a graphic that presents the median, the first and third quartiles, and any “outliers” present in the sample.\n\nThe interquartile range (IQR) is the difference between the third quartile and the first quartile (\\(Q_3 - Q_1\\)). This is the distance needed to span the middle half of the data."
  },
  {
    "objectID": "Tools/Tools2.slides.html#anatomy-of-a-box-plot",
    "href": "Tools/Tools2.slides.html#anatomy-of-a-box-plot",
    "title": "Tools for Data Science: Part II",
    "section": "Anatomy of a box plot",
    "text": "Anatomy of a box plot\n\nSee also https://towardsdatascience.com/why-1-5-in-iqr-method-of-outlier-detection-5d07fdc82097"
  },
  {
    "objectID": "Tools/Tools2.slides.html#box-plot-of-na",
    "href": "Tools/Tools2.slides.html#box-plot-of-na",
    "title": "Tools for Data Science: Part II",
    "section": "Box plot of Na",
    "text": "Box plot of Na\nTo create a boxplot, we use the function boxplot() from seabron.\n\n\nCode\nplt.figure(figsize=(7,4)) # Create space for the figure.\nsns.boxplot(data = glass_data, y = 'Na') # Create boxplot.\nplt.title(\"Box plot of Na\") # Add title.\nplt.show() # Show the plot."
  },
  {
    "objectID": "Tools/Tools2.slides.html#outliers",
    "href": "Tools/Tools2.slides.html#outliers",
    "title": "Tools for Data Science: Part II",
    "section": "Outliers",
    "text": "Outliers\n\nOutliers are points that are much larger or smaller than the rest of the sample points.\nOutliers may be data entry errors or they may be points that really are different from the rest.\nOutliers should not be deleted without considerable thought—sometimes calculations and analyses will be done with and without outliers and then compared."
  },
  {
    "objectID": "Tools/Tools2.slides.html#scatter-plot",
    "href": "Tools/Tools2.slides.html#scatter-plot",
    "title": "Tools for Data Science: Part II",
    "section": "Scatter plot",
    "text": "Scatter plot\n\nData for which items consists of a pair of numeric values is called bivariate. The graphical summary for bivariate data is a scatterplot.\nThe variables \\(X\\) and \\(Y\\) are placed on the horizontal and vertical axes, respectively. Each point on the graph marks the position of a pair of values of \\(X\\) and \\(Y\\).\nA scatterplot allows us to explore lineal and nonlinear relationships between two variables."
  },
  {
    "objectID": "Tools/Tools2.slides.html#scatter-plot-of-na-versus-ri",
    "href": "Tools/Tools2.slides.html#scatter-plot-of-na-versus-ri",
    "title": "Tools for Data Science: Part II",
    "section": "Scatter plot of Na versus RI",
    "text": "Scatter plot of Na versus RI\nTo create a scatter plot, we use the function scatter() from seabron. In this function, you must state the\n\n\nCode\nplt.figure(figsize=(7,4)) # Create space for the plot.\nsns.scatterplot(data = glass_data, x = 'Na', y = 'RI') # Show the plot.\nplt.title(\"Scatter plot of Na vs RI\") # Set plot title.\nplt.xlabel(\"Na\") # Set label for X axis.\nplt.ylabel(\"RI\") # Set label for Y axis.\nplt.show() # Show plot."
  },
  {
    "objectID": "Tools/Tools2.slides.html#bar-charts",
    "href": "Tools/Tools2.slides.html#bar-charts",
    "title": "Tools for Data Science: Part II",
    "section": "Bar charts",
    "text": "Bar charts\nBar charts are commonly used to describe qualitative data classified into various categories based on sector, region, different time periods, or other such factors.\nDifferent sectors, different regions, or different time periods are then labeled as specific categories.\nA bar chart is constructed by creating categories that are represented by labeling each category and which are represented by intervals of equal length on a horizontal axis.\nThe count or frequency within the corresponding category is represented by a bar of height proportional to the frequency."
  },
  {
    "objectID": "Tools/Tools2.slides.html#section-8",
    "href": "Tools/Tools2.slides.html#section-8",
    "title": "Tools for Data Science: Part II",
    "section": "",
    "text": "We create the bar chart using the function countplot() from seaborn.\n\n\nCode\n# Create plot.\nplt.figure(figsize=(7,4)) # Create space for the plot.\nsns.countplot(data = glass_data, x = 'Type') # Show the plot.\nplt.title(\"Bar chart of Type of Glasses\") # Set plot title.\nplt.ylabel(\"Frequency\") # Set label for Y axis.\nplt.show() # Show plot."
  },
  {
    "objectID": "Tools/Tools2.slides.html#saving-plots",
    "href": "Tools/Tools2.slides.html#saving-plots",
    "title": "Tools for Data Science: Part II",
    "section": "Saving plots",
    "text": "Saving plots\n\nWe save a figure using the save.fig function from matplotlib. The dpi argument of this function sets the resolution of the image. The higher the dpi, the better the resolution.\n\nplt.figure(figsize=(5, 7))\nsns.countplot(data = glass_data, x = 'Type')\nplt.title('Frequency of Each Category')\nplt.ylabel('Frequency')\nplt.xlabel('Category')\nplt.savefig('bar_chart.png',dpi=300)"
  },
  {
    "objectID": "Tools/Tools2.slides.html#improving-the-figure",
    "href": "Tools/Tools2.slides.html#improving-the-figure",
    "title": "Tools for Data Science: Part II",
    "section": "Improving the figure",
    "text": "Improving the figure\n\nWe can also use other functions to improve the aspect of the figure:\n\nplt.title(fontsize): Font size of the title.\nplt.ylabel(fontsize): Font size of y axis title.\nplt.xlabel(fontsize): Font size of x axis title.\nplt.yticks(fontsize): Font size of the y axis labels.\nplt.xticks(fontsize): Font size of the x axis labels."
  },
  {
    "objectID": "Tools/Tools2.slides.html#section-9",
    "href": "Tools/Tools2.slides.html#section-9",
    "title": "Tools for Data Science: Part II",
    "section": "",
    "text": "plt.figure(figsize=(5, 5))\nsns.countplot(data = glass_data, x = 'Type')\nplt.title('Relative Frequency of Each Category', fontsize = 12)\nplt.ylabel('Relative Frequency', fontsize = 12)\nplt.xlabel('Category', fontsize = 15)\nplt.xticks(fontsize = 12)\nplt.yticks(fontsize = 12)\nplt.savefig('bar_chart.png',dpi=300)"
  },
  {
    "objectID": "Classification/Classification.slides.html#agenda",
    "href": "Classification/Classification.slides.html#agenda",
    "title": "Logistic Regression",
    "section": "Agenda",
    "text": "Agenda\n\n\nIntroduction\nLogistic regression\nEstimating a logistic regression model\nClassification performance"
  },
  {
    "objectID": "Classification/Classification.slides.html#load-the-libraries",
    "href": "Classification/Classification.slides.html#load-the-libraries",
    "title": "Logistic Regression",
    "section": "Load the libraries",
    "text": "Load the libraries\nBefore we start, let’s import the data science libraries into Python.\n\n# Importing necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n\nHere, we will introduce the functions confusion_matrix(), ConfusionMatrixDisplay() and accuracy_score() to evaluate the performance of a logistic regression classifier."
  },
  {
    "objectID": "Classification/Classification.slides.html#two-main-problems",
    "href": "Classification/Classification.slides.html#two-main-problems",
    "title": "Logistic Regression",
    "section": "Two main problems",
    "text": "Two main problems\n\nRegression problems. The response \\(Y\\) is quantitative. For example, person’s income, the value of a house, the blood pressure of a patient.\nClassification problems. The response \\(Y\\) is qualitative and involves \\(K\\) different categories. For example, the brand of a product purchased (A, B, C) whether a person defaults on a debt (yes or no).\nThe predictors (\\(\\boldsymbol{X}\\)) can be qualitative or quantitative."
  },
  {
    "objectID": "Classification/Classification.slides.html#terminology",
    "href": "Classification/Classification.slides.html#terminology",
    "title": "Logistic Regression",
    "section": "Terminology",
    "text": "Terminology\n\nExplanatory variables or predictors:\n\n\\(X\\) represents an explanatory variable or predictor.\n\\(\\boldsymbol{X} = (X_1, X_2, \\ldots, X_p)\\) represents a collection of \\(p\\) predictors."
  },
  {
    "objectID": "Classification/Classification.slides.html#section",
    "href": "Classification/Classification.slides.html#section",
    "title": "Logistic Regression",
    "section": "",
    "text": "Response:\n\n\n\\(Y\\) is a categorical variable taking 2 categories or classes.\nFor example, \\(Y\\) can take 0 or 1, A or B, no or yes, spam or not spam.\nWhen the classes are strings, it is customary to code them to 0 and 1.\n\nThe target class is the one for which \\(Y = 1\\).\nThe reference class is the one for which \\(Y = 0\\)."
  },
  {
    "objectID": "Classification/Classification.slides.html#section-1",
    "href": "Classification/Classification.slides.html#section-1",
    "title": "Logistic Regression",
    "section": "",
    "text": "Goal: Find the best function \\(C(\\boldsymbol{X})\\) for predicting \\(Y = \\{0, 1\\}\\) from \\(\\boldsymbol{X}\\).\n\n\nTo achieve this goal, we will consider functions \\(C(\\boldsymbol{X})\\) that predict the probability that \\(Y\\) takes the value of 1.\n\n\n\nA probability for each class can be very useful for gauging the model’s confidence about the predicted classification."
  },
  {
    "objectID": "Classification/Classification.slides.html#example-1",
    "href": "Classification/Classification.slides.html#example-1",
    "title": "Logistic Regression",
    "section": "Example 1",
    "text": "Example 1\nConsider a spam e-mail filter where \\(Y\\) is the type of e-mail.\n\nThe target class is spam. In this case, \\(Y=1\\).\nThe reference class is not spam. In this case, \\(Y=0\\).\n\n\n\n\n\n\n\n\n\nBoth e-mails would be classified as spam. However, we’d have more confidence in our classification for the second email."
  },
  {
    "objectID": "Classification/Classification.slides.html#section-2",
    "href": "Classification/Classification.slides.html#section-2",
    "title": "Logistic Regression",
    "section": "",
    "text": "Technically, \\(C(\\boldsymbol{X})\\) will work with the conditional probability:\n\\[P(Y = 1 | X_1 = x_1, X_2 = x_2, \\ldots, X_p = x_p) = P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\]\nIn words, this is the probability that \\(Y\\) takes a value of 1 given that the predictors \\(\\boldsymbol{X}\\) have taken the values \\(\\boldsymbol{x} = (x_1, x_2, \\ldots, x_p)\\).\n\n\nThe conditional probability that \\(Y\\) takes the value of 0 is\n\\[P(Y = 0 | \\boldsymbol{X} = \\boldsymbol{x}) = 1 - P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x}).\\]"
  },
  {
    "objectID": "Classification/Classification.slides.html#bayes-classifier",
    "href": "Classification/Classification.slides.html#bayes-classifier",
    "title": "Logistic Regression",
    "section": "Bayes Classifier",
    "text": "Bayes Classifier\n\nIt turns out that, if we know the true structure of \\(P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\), we can build a good classification function called the Bayes classifier:\n\\[C(\\boldsymbol{X}) =\n    \\begin{cases}\n      1, & \\text{if}\\ P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x}) &gt; 0.5 \\\\\n      0, & \\text{if}\\ P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x}) \\leq 0.5\n    \\end{cases}.\\]\nThis function classifies to the most probable class using the conditional distribution \\(P(Y | \\boldsymbol{X} = \\boldsymbol{x})\\)."
  },
  {
    "objectID": "Classification/Classification.slides.html#section-3",
    "href": "Classification/Classification.slides.html#section-3",
    "title": "Logistic Regression",
    "section": "",
    "text": "HOWEVER, we don’t (and will never) know the true form of \\(P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\)!\n\n\nTo overcome this issue, we have a standard solution:\n\nImpose an structure on \\(P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\).\nThis structure produces the Logistic Regression classifier.\n\n\n\nBefore we introduce logistic regression, we will present another motivating example."
  },
  {
    "objectID": "Classification/Classification.slides.html#example-2",
    "href": "Classification/Classification.slides.html#example-2",
    "title": "Logistic Regression",
    "section": "Example 2",
    "text": "Example 2\nConsider the task of identifying old-swiss counterfeit banknotes. The response under study is\n\\[Y =\n    \\begin{cases}\n      1, & \\text{if it is a counterfeit banknote} \\\\\n      0, & \\text{otherwise}\n    \\end{cases}.\\]\nWe have four predictors:\n\n\n\\(X_1\\): Left, which is the length of left edge (mm)\n\\(X_2\\): Right, which is the length of right edge (mm)\n\\(X_3\\): Top, which is the distance from the image to top edge\n\\(X_4\\): Bottom, which is the distance from image to bottom"
  },
  {
    "objectID": "Classification/Classification.slides.html#old-swiss-1000-franc-banknote",
    "href": "Classification/Classification.slides.html#old-swiss-1000-franc-banknote",
    "title": "Logistic Regression",
    "section": "Old-Swiss 1000-franc banknote",
    "text": "Old-Swiss 1000-franc banknote"
  },
  {
    "objectID": "Classification/Classification.slides.html#dataset",
    "href": "Classification/Classification.slides.html#dataset",
    "title": "Logistic Regression",
    "section": "Dataset",
    "text": "Dataset\nThe data is in the file “banknotes.xlsx”.\n\nbank_data = pd.read_excel(\"banknotes.xlsx\")\n# Set response variable as categorical.\nbank_data['Status'] = pd.Categorical(bank_data['Status'])\nbank_data.head()\n\n\n\n\n\n\n\n\nStatus\nLeft\nRight\nBottom\nTop\n\n\n\n\n0\ngenuine\n131.0\n131.1\n9.0\n9.7\n\n\n1\ngenuine\n129.7\n129.7\n8.1\n9.5\n\n\n2\ngenuine\n129.7\n129.7\n8.7\n9.6\n\n\n3\ngenuine\n129.7\n129.6\n7.5\n10.4\n\n\n4\ngenuine\n129.6\n129.7\n10.4\n7.7"
  },
  {
    "objectID": "Classification/Classification.slides.html#data-pre-processing",
    "href": "Classification/Classification.slides.html#data-pre-processing",
    "title": "Logistic Regression",
    "section": "Data pre-processing",
    "text": "Data pre-processing\n\nBefore we start, we ensure that there are no highly correlated predictors in the dataset. To this end, we construct the correlation plot.\n\n# Remove response.\npredictors_full = bank_data.drop(columns = ['Status'])\n\n# Compute correlation matrix.\ncorrelation_matrix = predictors_full.corr()\n\n# Plot the correlation matrix\nsns.heatmap(correlation_matrix.abs(), cmap = 'coolwarm', annot = True)\nplt.title(\"Heatmap on correlations\") # Set plot title."
  },
  {
    "objectID": "Classification/Classification.slides.html#section-4",
    "href": "Classification/Classification.slides.html#section-4",
    "title": "Logistic Regression",
    "section": "",
    "text": "Text(0.5, 1.0, 'Heatmap on correlations')"
  },
  {
    "objectID": "Classification/Classification.slides.html#section-5",
    "href": "Classification/Classification.slides.html#section-5",
    "title": "Logistic Regression",
    "section": "",
    "text": "The correlation plot shows that the predictors Left (\\(X_1\\)) and Right (\\(X_2\\)) have an absolute correlation of 0.74. Using a threshold for high correlation of 0.70, we thus remove one predictor from the database, say, Right.\n\nConsequently, our database for this problem is\n\nbank_data = bank_data.drop(columns = ['Right'])"
  },
  {
    "objectID": "Classification/Classification.slides.html#logistic-regression-lr",
    "href": "Classification/Classification.slides.html#logistic-regression-lr",
    "title": "Logistic Regression",
    "section": "Logistic Regression (LR)",
    "text": "Logistic Regression (LR)\n\nBasic Idea: Impose an structure on \\(P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\):\n\n\\[P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x}) = \\frac{e^{\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p} }{1 + e^{\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p}}\\]\n\n\nThis structure is known as the logistic function.\nThe logistic function is always between 0 and 1."
  },
  {
    "objectID": "Classification/Classification.slides.html#why-logistic-regression",
    "href": "Classification/Classification.slides.html#why-logistic-regression",
    "title": "Logistic Regression",
    "section": "Why logistic regression?",
    "text": "Why logistic regression?\nLet’s use some algebra to reveal some of interesting facts about logistic regression.\n\nWe start from\n\\[P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x}) = \\frac{e^{\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p} }{1 + e^{\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p}}.\\]\n\n\nNext, we have that\n\\[e^{\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p} = \\frac{P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})}{1 - P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})}.\\]"
  },
  {
    "objectID": "Classification/Classification.slides.html#the-odds-ratio",
    "href": "Classification/Classification.slides.html#the-odds-ratio",
    "title": "Logistic Regression",
    "section": "The Odds ratio",
    "text": "The Odds ratio\nThe quantity \\[e^{\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p} = \\frac{P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})}{1 - P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})}\\] is known as the “odds” ratio.\n\nThe odds ratio is the probability that \\(Y = 1\\) divided by the probability that \\(Y = 0\\), given that the predictors \\(\\boldsymbol{X}\\) have taken the values \\(\\boldsymbol{x} = (x_1, x_2, \\ldots, x_p)\\)."
  },
  {
    "objectID": "Classification/Classification.slides.html#section-6",
    "href": "Classification/Classification.slides.html#section-6",
    "title": "Logistic Regression",
    "section": "",
    "text": "Consider the banknote classification problem where \\(Y = 1\\) implies counterfeit and \\(Y = 0\\) genuine note. We have three predictors in \\(\\boldsymbol{X} =  (X_1, X_3, X_4)\\) summarizing specific characteristics of a banknote.\n\nThe odds ratio is \\(e^{\\beta_0 + \\beta_1 X_1 + \\beta_3 X_3 + \\beta_p X_4} = \\frac{P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})}{1 - P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})}\\)\n\n\nIf the ratio is 1, the odds are 50-50.\nIf the ratio is greater than 1, the banknote is more likely to be counterfeit than genuine.\nIf the ratio is smaller than 1, the banknote is more likely to be genuine than counterfeit."
  },
  {
    "objectID": "Classification/Classification.slides.html#the-logg-odds",
    "href": "Classification/Classification.slides.html#the-logg-odds",
    "title": "Logistic Regression",
    "section": "The logg-odds",
    "text": "The logg-odds\n\nIf we take logarithm on both sides, we obtain the “log-odds” or “logit”:\n\\[\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p = \\ln \\left( \\frac{P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x} )}{1 - P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})} \\right).\\]\n\n\nThe log-odds can take any real value.\nThe log-odds is a linear combination of the predictors.\nThe log-odds resembles a multiple lienar regression model."
  },
  {
    "objectID": "Classification/Classification.slides.html#interpretation-of-coefficients",
    "href": "Classification/Classification.slides.html#interpretation-of-coefficients",
    "title": "Logistic Regression",
    "section": "Interpretation of coefficients",
    "text": "Interpretation of coefficients\nThe log-odds allows us to interpret the coefficients of the logistic regression model.\n\\[\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p = \\ln \\left( \\frac{P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x} )}{1 - P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})} \\right).\\]\n\n\\(\\beta_0\\) can be interpreted as the average value of the log-odds ratio given that all predictors are equal to zero.\n\\(\\beta_j\\) can be interpreted as the average change in the log-odds ratio given by a one-unit increase in \\(X_j\\), when all the other predictors have fixed values."
  },
  {
    "objectID": "Classification/Classification.slides.html#section-7",
    "href": "Classification/Classification.slides.html#section-7",
    "title": "Logistic Regression",
    "section": "",
    "text": "If \\(\\beta_j\\) is positive, increasing the value of \\(X_j\\) will be associated with increasing the odds ratio or \\(P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\). In other words, increasing the value of \\(X_j\\) increases, on average, the probability that \\(Y = 1\\).\n\nIf \\(\\beta_j\\) is negative, increasing \\(X_j\\) will be associated with decreasing \\(P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x})\\), and with increasing \\(P(Y = 0 | \\boldsymbol{X} = \\boldsymbol{x})\\). In other words, increasing the value of \\(X_j\\) increases, on average, the probability that \\(Y = 0\\).\n\nhttps://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/"
  },
  {
    "objectID": "Classification/Classification.slides.html#the-logistic-regression-model",
    "href": "Classification/Classification.slides.html#the-logistic-regression-model",
    "title": "Logistic Regression",
    "section": "The logistic regression model",
    "text": "The logistic regression model\n\nThe logistic regression model is\n\\[P(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x}) = \\frac{e^{\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p} }{1 + e^{\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p}},\\]\nwhere the values of the coefficients \\(\\beta_0\\) and \\(\\beta_j\\) are unknown. Therefore, we need to calculate estimates for them.\n\n\\(\\hat{\\beta}_0\\) is the estimate for \\(\\beta_0\\).\n\\(\\hat{\\beta}_j\\) is the estimate for \\(\\beta_j\\)."
  },
  {
    "objectID": "Classification/Classification.slides.html#how-do-we-estimate-the-coefficients",
    "href": "Classification/Classification.slides.html#how-do-we-estimate-the-coefficients",
    "title": "Logistic Regression",
    "section": "How do we estimate the coefficients?",
    "text": "How do we estimate the coefficients?\n\nWe use the training data!\n\n\n\n\n\n\n\n\n\nStatus\nLeft\nBottom\nTop\n\n\n\n\n0\ngenuine\n131.0\n9.0\n9.7\n\n\n1\ngenuine\n129.7\n8.1\n9.5\n\n\n2\ngenuine\n129.7\n8.7\n9.6\n\n\n3\ngenuine\n129.7\n7.5\n10.4"
  },
  {
    "objectID": "Classification/Classification.slides.html#generate-training-data",
    "href": "Classification/Classification.slides.html#generate-training-data",
    "title": "Logistic Regression",
    "section": "Generate training data",
    "text": "Generate training data\nWe split the current data set into a training and a validation dataset. To this end, we use the function train_test_split() from scikit-learn.\n\n# Set full matrix of predictors.\nX_full = bank_data.drop(columns = ['Status'])\n\n# Set full matrix of responses.\nY_full = bank_data['Status']\n\n# Split the dataset.\nX_train, X_test, Y_train, Y_test = train_test_split(X_full, Y_full, \n                                                    test_size=0.3)\n\nThe parameter test_size sets the portion of the dataset that will go to the validation set."
  },
  {
    "objectID": "Classification/Classification.slides.html#section-8",
    "href": "Classification/Classification.slides.html#section-8",
    "title": "Logistic Regression",
    "section": "",
    "text": "The function makes a clever partition of the data using the empirical distribution of the response.\nTechnically, it splits the data so that the distribution of the response under the training and validation sets is similar.\nUsually, the proportion of the dataset that goes to the test set is 20% or 30%.\nWe will later use the validation dataset to evaluate the classification performance of the estimated logistic regression model for classifying unobserved data."
  },
  {
    "objectID": "Classification/Classification.slides.html#coefficient-estimation",
    "href": "Classification/Classification.slides.html#coefficient-estimation",
    "title": "Logistic Regression",
    "section": "Coefficient estimation",
    "text": "Coefficient estimation\nWe estimate the coefficients in the logistic function using maximum likelihood estimation.\nEssentially, we optimize a non-linear objective function using the so-called Iteratively Re-weighted Least Squares (IRLS) algorithm.\n\nThe IRLS algorithm (and consequently maximum likelihood estimation and Logistic Regression) fails when:\n\nThere is severe multicollinearity among the predictors.\nWe can perfectly separate the observations belonging to the two groups defined by \\(Y\\)."
  },
  {
    "objectID": "Classification/Classification.slides.html#in-python",
    "href": "Classification/Classification.slides.html#in-python",
    "title": "Logistic Regression",
    "section": "In Python",
    "text": "In Python\nUsing the training dataset, we estimate a logistic regression classifier using the function Logit() from statsmodel. To this end, we first define the target category using the get_dummies().\n\n# Create dummy variables.\nY_dummies = pd.get_dummies(Y_train, dtype = 'int')\n\n# Select target variable.\nY_target_train = Y_dummies['counterfeit']\n\nWe also define the matrix of predictors with the intercept.\n\n# Add the intercept to the predictor matrix.\nX_train_int = sm.add_constant(X_train)"
  },
  {
    "objectID": "Classification/Classification.slides.html#section-9",
    "href": "Classification/Classification.slides.html#section-9",
    "title": "Logistic Regression",
    "section": "",
    "text": "After that, we fit the model using the function Logit().\n\n# Create logistic regression object.\nlogit = sm.Logit(Y_target_train, X_train_int)\n\n# Train the model using the training set.\nlogit_model = logit.fit()\n\n# Show estimated coefficients.\nprint(logit_model.params)\n\nOptimization terminated successfully.\n         Current function value: 0.052631\n         Iterations 12\nconst    -103.499580\nLeft       -0.068777\nBottom      4.602238\nTop         6.545577\ndtype: float64"
  },
  {
    "objectID": "Classification/Classification.slides.html#parameter-testing",
    "href": "Classification/Classification.slides.html#parameter-testing",
    "title": "Logistic Regression",
    "section": "Parameter testing",
    "text": "Parameter testing\n\nWe can construct significance tests for each coefficient in the logistic regression model. They are called Wald tests.\n\nWald tests allow us to test the hypothesis:\n\\[H_0: \\beta_j = 0 \\text{ versus } H_1: \\beta_j \\neq 0\\]"
  },
  {
    "objectID": "Classification/Classification.slides.html#section-10",
    "href": "Classification/Classification.slides.html#section-10",
    "title": "Logistic Regression",
    "section": "",
    "text": "Wald tests use the following test statistic:\n\\[W_j = \\frac{\\hat{\\beta}_j}{\\mbox{SE}(\\hat{\\beta}_j) } \\sim N(0,1).\\]\n\n\\(\\hat{\\beta}_j\\) is the estimate of the coefficient \\(\\beta_j\\).\n\\(\\mbox{SE}(\\hat{\\beta}_j)\\) is the standard error of the estimate \\(\\hat{\\beta}_j\\) (due to repeated random sampling)."
  },
  {
    "objectID": "Classification/Classification.slides.html#important-predictors",
    "href": "Classification/Classification.slides.html#important-predictors",
    "title": "Logistic Regression",
    "section": "Important predictors",
    "text": "Important predictors\n\nWe can use the p-values of these tests to determine which predictor is important.\n\nThat is, a predictor is important if its p-value is, say, smaller than \\(\\alpha = 0.05\\).\n\nFor Wald tests to work well, the number of observations in the training data should be large."
  },
  {
    "objectID": "Classification/Classification.slides.html#section-11",
    "href": "Classification/Classification.slides.html#section-11",
    "title": "Logistic Regression",
    "section": "",
    "text": "The summary() function of statsmodel contains the Wald tests of the coefficients in the logistic regression model.\n\nlogit_summary = logit_model.summary()\nprint(logit_summary)\n\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:            counterfeit   No. Observations:                  140\nModel:                          Logit   Df Residuals:                      136\nMethod:                           MLE   Df Model:                            3\nDate:                Fri, 16 May 2025   Pseudo R-squ.:                  0.9240\nTime:                        10:49:47   Log-Likelihood:                -7.3684\nconverged:                       True   LL-Null:                       -96.983\nCovariance Type:            nonrobust   LLR p-value:                 1.293e-38\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst       -103.4996    352.096     -0.294      0.769    -793.595     586.596\nLeft          -0.0688      2.785     -0.025      0.980      -5.528       5.390\nBottom         4.6022      1.852      2.485      0.013       0.972       8.232\nTop            6.5456      2.225      2.941      0.003       2.184      10.907\n==============================================================================\n\nPossibly complete quasi-separation: A fraction 0.51 of observations can be\nperfectly predicted. This might indicate that there is complete\nquasi-separation. In this case some parameters will not be identified."
  },
  {
    "objectID": "Classification/Classification.slides.html#estimated-classifier",
    "href": "Classification/Classification.slides.html#estimated-classifier",
    "title": "Logistic Regression",
    "section": "Estimated classifier",
    "text": "Estimated classifier\n\nAfter estimating the coefficients, we obtain the estimated logistic regression model\n\\[\\hat{P}(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x}) = \\frac{e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X_1 + \\hat{\\beta}_2 X_2 + \\cdots + \\hat{\\beta}_p X_p} }{1 + e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X_1 + \\hat{\\beta}_2 X_2 + \\cdots + \\hat{\\beta}_p X_p}},\\]\nwhere we have replaced the coefficients \\(\\beta_j\\)’s with their estimates \\(\\hat{\\beta}_j\\)’s."
  },
  {
    "objectID": "Classification/Classification.slides.html#section-12",
    "href": "Classification/Classification.slides.html#section-12",
    "title": "Logistic Regression",
    "section": "",
    "text": "Using the estimated logistic regression model, we can build an approximation to the Bayes classifier \\(C(\\boldsymbol{X})\\).\n\nThe approximation is\n\\[\\hat{C}(\\boldsymbol{X}) =\n    \\begin{cases}\n      1, & \\text{if}\\ \\hat{P}(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x}) &gt; 0.5 \\\\\n      0, & \\text{if}\\ \\hat{P}(Y = 1 | \\boldsymbol{X} = \\boldsymbol{x}) \\leq 0.5\n    \\end{cases}.\\]\n\nWe call \\(\\hat{C}(\\boldsymbol{X})\\) the logistic regression classifier.\n\\(\\hat{C}(\\boldsymbol{X})\\) can be used to classify new observations."
  },
  {
    "objectID": "Classification/Classification.slides.html#evaluation",
    "href": "Classification/Classification.slides.html#evaluation",
    "title": "Logistic Regression",
    "section": "Evaluation",
    "text": "Evaluation\n\nWe evaluate a the logistic regression classifier by classifying observations that were not used for training or estimating it.\nThat is, we use the classifier to predict the categories of the test dataset using the predictor values in this set only.\nIn Python, we use the commands:\n\n# Add constant to the predictor matrix from the test set.\nX_test = sm.add_constant(X_test)\n\n# Predict probabilities.\npredicted_probability = logit_model.predict(X_test)"
  },
  {
    "objectID": "Classification/Classification.slides.html#section-13",
    "href": "Classification/Classification.slides.html#section-13",
    "title": "Logistic Regression",
    "section": "",
    "text": "The predict() function outputs probabilities instead of actual classes.\n\npredicted_probability.head()\n\n28     0.000064\n30     0.002347\n196    0.999963\n77     0.000125\n52     0.005469\ndtype: float64\n\n\nThese are the probabilities of a banknote being “counterfeit” according to its characteristics (values of the predictors).\n\nTo turn the probabilities to actual classes, we round them:\n\npredicted_classes = round(predicted_probability).astype('int')"
  },
  {
    "objectID": "Classification/Classification.slides.html#section-14",
    "href": "Classification/Classification.slides.html#section-14",
    "title": "Logistic Regression",
    "section": "",
    "text": "predicted_classes.head()\n\n28     0\n30     0\n196    1\n77     0\n52     0\ndtype: int64\n\n\n\nObservations with probabilities higher than 0.5 are classified as “counterfeit”.\nObservations with probabilities lower than 0.5 are classified as “genuine”.\n\nNow, we compare the predictions with the actual categories in the validation dataset. A good logistic regression model has a good agreement between its predictions and the actual categories."
  },
  {
    "objectID": "Classification/Classification.slides.html#confusion-matrix",
    "href": "Classification/Classification.slides.html#confusion-matrix",
    "title": "Logistic Regression",
    "section": "Confusion matrix",
    "text": "Confusion matrix\n\nA table used to evaluate the performance of a classifier.\nCompares the actual values with the predicted values of a classifier.\nUseful for both binary and multiclass classification problems."
  },
  {
    "objectID": "Classification/Classification.slides.html#in-python-1",
    "href": "Classification/Classification.slides.html#in-python-1",
    "title": "Logistic Regression",
    "section": "In Python",
    "text": "In Python\n\nWe compute the confusion matrix using the function with the same name of scikit-learn.\n\n# Create dummy variables for test set.\nY_dummies = pd.get_dummies(Y_test, dtype = 'int')\n\n# Select target variable from test set.\nY_target_test = Y_dummies['counterfeit']\n\n# Compute confusion matrix.\ncm = confusion_matrix(Y_target_test, predicted_classes)\n\n# Show confusion matrix.\nprint(cm)\n\n[[32  0]\n [ 2 26]]"
  },
  {
    "objectID": "Classification/Classification.slides.html#section-15",
    "href": "Classification/Classification.slides.html#section-15",
    "title": "Logistic Regression",
    "section": "",
    "text": "We can visualize the confusion matrix using the ConfusionMatrixDisplay() function.\n\nConfusionMatrixDisplay(cm).plot()"
  },
  {
    "objectID": "Classification/Classification.slides.html#accuracy",
    "href": "Classification/Classification.slides.html#accuracy",
    "title": "Logistic Regression",
    "section": "Accuracy",
    "text": "Accuracy\nA simple metric to summarize the information of the confusion matrix is accuracy. It is the proportion of correct classifications for both classes, out of the total classifications made.\nIn Python, we compute the accuracy using the function accuracy_score() from scikit-learn.\n\naccuracy = accuracy_score(Y_target_test, predicted_classes)\nprint( round(accuracy, 2) )\n\n0.97\n\n\n\n\nThe higher the accuracy, the better the performance of the classifier."
  },
  {
    "objectID": "Classification/Classification.slides.html#remarks",
    "href": "Classification/Classification.slides.html#remarks",
    "title": "Logistic Regression",
    "section": "Remarks",
    "text": "Remarks\n\n\nAccuracy is simple to calculate and interpret.\nIt works well when the dataset has a balanced class distribution (i.e., roughly equal 1 and 0 cases).\nNot ideal for imbalanced datasets. When one class is much more frequent than the other, accuracy can be misleading.\nOther summaries of the confusion matrix such as Precision, Recall, and F1-Score are better suited for imbalanced data."
  },
  {
    "objectID": "LinearRegression/LinearRegression.slides.html#agenda",
    "href": "LinearRegression/LinearRegression.slides.html#agenda",
    "title": "Multiple Linear Regression",
    "section": "Agenda",
    "text": "Agenda\n\n\nIntroduction\nMultiple linear regression model\nParameter estimation"
  },
  {
    "objectID": "LinearRegression/LinearRegression.slides.html#statsmodels-library",
    "href": "LinearRegression/LinearRegression.slides.html#statsmodels-library",
    "title": "Multiple Linear Regression",
    "section": "statsmodels library",
    "text": "statsmodels library\n\nstatsmodels is a powerful Python library for statistical modeling, data analysis, and hypothesis testing.\nIt provides classes and functions for estimating statistical models.\nIt is built on top of libraries such as NumPy, SciPy, and pandas\nhttps://www.statsmodels.org/stable/index.html"
  },
  {
    "objectID": "LinearRegression/LinearRegression.slides.html#load-the-libraries",
    "href": "LinearRegression/LinearRegression.slides.html#load-the-libraries",
    "title": "Multiple Linear Regression",
    "section": "Load the libraries",
    "text": "Load the libraries\n\nLet’s import statsmodels into Python together with the other relevant libraries.\n\n# Importing necessary libraries\nimport pandas as pd\nimport statsmodels.api as sm"
  },
  {
    "objectID": "LinearRegression/LinearRegression.slides.html#example",
    "href": "LinearRegression/LinearRegression.slides.html#example",
    "title": "Multiple Linear Regression",
    "section": "Example",
    "text": "Example\n\nA group of engineers conducted an experiment to determine the influence of five factors on an appropriate measure of the whiteness of rayon (\\(Y\\)). The factors (predictors) are\n\n\\(X_1\\): acid bath temperature.\n\\(X_2\\): cascade acid concentration.\n\\(X_3\\): water temperature.\n\\(X_4\\): sulfide concentration.\n\\(X_5\\): amount of chlorine bleach."
  },
  {
    "objectID": "LinearRegression/LinearRegression.slides.html#the-dataset",
    "href": "LinearRegression/LinearRegression.slides.html#the-dataset",
    "title": "Multiple Linear Regression",
    "section": "The dataset",
    "text": "The dataset\n\nThe dataset for the file is in “rayon.xlsx”. It has 26 observations.\n\nrayon_data = pd.read_excel(\"rayon.xlsx\")\nrayon_data.head()\n\n\n\n\n\n\n\n\nX1\nX2\nX3\nX4\nX5\nY\n\n\n\n\n0\n35\n0.3\n82\n0.2\n0.3\n76.5\n\n\n1\n35\n0.3\n82\n0.3\n0.5\n76.0\n\n\n2\n35\n0.3\n88\n0.2\n0.5\n79.9\n\n\n3\n35\n0.3\n88\n0.3\n0.3\n83.5\n\n\n4\n35\n0.7\n82\n0.2\n0.5\n89.5"
  },
  {
    "objectID": "LinearRegression/LinearRegression.slides.html#multiple-linear-regression-model-1",
    "href": "LinearRegression/LinearRegression.slides.html#multiple-linear-regression-model-1",
    "title": "Multiple Linear Regression",
    "section": "Multiple linear regression model",
    "text": "Multiple linear regression model\n\\[Y = f(\\boldsymbol{X}) + \\epsilon\\]\n\n\\(f(\\boldsymbol{X}) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p\\) (constant).\n\\(p\\) is the number of predictors.\n\\(\\epsilon\\) is a random variable describing everything that is not captured by our model.\n\nAssumptions:\n\nThe expected or average value of \\(\\epsilon\\) is zero.\nThe dispersion or variance of \\(\\epsilon\\) is \\(\\sigma^2\\) (unknown constant)."
  },
  {
    "objectID": "LinearRegression/LinearRegression.slides.html#in-our-example",
    "href": "LinearRegression/LinearRegression.slides.html#in-our-example",
    "title": "Multiple Linear Regression",
    "section": "In our example",
    "text": "In our example\n\\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + \\beta_4 X_4 + \\beta_5 X_5 + \\epsilon\\]\n\n\n\\(X_1\\): acid bath temperature.\n\\(X_2\\): cascade acid concentration.\n\\(X_3\\): water temperature.\n\\(X_4\\): sulfide concentration.\n\\(X_5\\): amount of chlorine bleach.\n\\(Y\\): whiteness of rayon.\n\\(p = 5\\) and \\(\\epsilon\\) is the error of the model assumed to be 0 and of constant dispersion \\(\\sigma^2\\)."
  },
  {
    "objectID": "LinearRegression/LinearRegression.slides.html#interpretation-of-coefficients",
    "href": "LinearRegression/LinearRegression.slides.html#interpretation-of-coefficients",
    "title": "Multiple Linear Regression",
    "section": "Interpretation of coefficients",
    "text": "Interpretation of coefficients\n\\[f(\\boldsymbol{X}) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p,\\]\nwhere the unknown parameter \\(\\beta_0\\) is called the “intercept,” and \\(\\beta_j\\) is the “coefficient” of the j-th predictor.\nFor the j-th predictor, we have that:\n\n\\(\\beta_j = 0\\) implies no dependence.\n\\(\\beta_j &gt; 0\\) implies positive dependence.\n\\(\\beta_j &lt; 0\\) implies negative dependence."
  },
  {
    "objectID": "LinearRegression/LinearRegression.slides.html#section",
    "href": "LinearRegression/LinearRegression.slides.html#section",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "\\[f(\\boldsymbol{X}) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p,\\]\nInterpretation:\n\n\\(\\beta_0\\) is the average response when all predictors \\(X_j\\) equal 0.\n\\(\\beta_j\\) is the amount of increase in the average response by a 1 unit increase in the predictor \\(X_j\\), when all other predictors are fixed to an arbitrary value."
  },
  {
    "objectID": "LinearRegression/LinearRegression.slides.html#training-data",
    "href": "LinearRegression/LinearRegression.slides.html#training-data",
    "title": "Multiple Linear Regression",
    "section": "Training Data",
    "text": "Training Data\nThe parameters \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) and \\(\\sigma^2\\) are unknown. To learn about them, we use our training data.\n\n\n\n\n\n\n\n\n\nX1\nX2\nX3\nX4\nX5\nY\n\n\n\n\n0\n35\n0.3\n82\n0.2\n0.3\n76.5\n\n\n1\n35\n0.3\n82\n0.3\n0.5\n76.0\n\n\n2\n35\n0.3\n88\n0.2\n0.5\n79.9\n\n\n3\n35\n0.3\n88\n0.3\n0.3\n83.5\n\n\n4\n35\n0.7\n82\n0.2\n0.5\n89.5"
  },
  {
    "objectID": "LinearRegression/LinearRegression.slides.html#notation",
    "href": "LinearRegression/LinearRegression.slides.html#notation",
    "title": "Multiple Linear Regression",
    "section": "Notation",
    "text": "Notation\n\n\\(X_{ij}\\) denotes the i-th observed value of predictor \\(X_j\\).\n\\(Y_i\\) denotes the i-th observed value of response \\(Y\\).\n\n\n\n\n\n\n\n\n\n\nX1\nX2\nX3\nX4\nX5\nY\n\n\n\n\n0\n35\n0.3\n82\n0.2\n0.3\n76.5\n\n\n1\n35\n0.3\n82\n0.3\n0.5\n76.0\n\n\n2\n35\n0.3\n88\n0.2\n0.5\n79.9\n\n\n3\n35\n0.3\n88\n0.3\n0.3\n83.5\n\n\n4\n35\n0.7\n82\n0.2\n0.5\n89.5"
  },
  {
    "objectID": "LinearRegression/LinearRegression.slides.html#section-1",
    "href": "LinearRegression/LinearRegression.slides.html#section-1",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "Since we believe in the multiple linear regression model, then the observations in the data set must comply with\n\\[Y_i= \\beta_0+\\beta_1 X_{i1} + \\beta_2 X_{i2} + \\cdots + \\beta_p X_{ip} + \\epsilon_i.\\]\nwhere:\n\n\\(i=1, \\ldots, n.\\)\n\\(n\\) is the number of observations. In our example, \\(n = 26\\).\nThe \\(\\epsilon_1, \\epsilon_2, \\ldots, \\epsilon_n\\) are random errors."
  },
  {
    "objectID": "LinearRegression/LinearRegression.slides.html#assumptions-of-the-errors",
    "href": "LinearRegression/LinearRegression.slides.html#assumptions-of-the-errors",
    "title": "Multiple Linear Regression",
    "section": "Assumptions of the errors",
    "text": "Assumptions of the errors\n\\[Y_i= \\beta_0+\\beta_1 X_{i1} + \\beta_2 X_{i2} + \\cdots + \\beta_p X_{ip} + \\epsilon_i \\]\nThe error \\(\\epsilon_i\\)’s must satisfy the following assumptions:\n\n\nOn average, they are close to zero for any values of the predictors \\(X_j\\).\nFor any value of a predictor \\(X_i\\), the dispersion or variance is constant and equal to \\(\\sigma^2\\).\nThe \\(\\epsilon_i\\)’s are all independent from each other.\nThe \\(\\epsilon_i\\)’s follow normal distribution with mean 0 and variance \\(\\sigma^2\\)."
  },
  {
    "objectID": "LinearRegression/LinearRegression.slides.html#questions",
    "href": "LinearRegression/LinearRegression.slides.html#questions",
    "title": "Multiple Linear Regression",
    "section": "Questions",
    "text": "Questions\n\n\n\nHow can we estimate \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) and \\(\\sigma^2\\)?\nHow can we make inferences about \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\)?\nHow can we validate the model and all its assumptions?\nHow can we make predictions of future responses using the multiple linear regression model?"
  },
  {
    "objectID": "LinearRegression/LinearRegression.slides.html#matrix-notation",
    "href": "LinearRegression/LinearRegression.slides.html#matrix-notation",
    "title": "Multiple Linear Regression",
    "section": "Matrix notation",
    "text": "Matrix notation\nIn what follows, it is useful to denote the multiple linear regression model using matrix notation:\n\\(\\mathbf{Y} = \\begin{pmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{pmatrix}\\) and \\(\\mathbf{X} = \\begin{pmatrix}\n1& X_{11} & X_{12} & \\cdots & X_{1p} \\\\\n1& X_{21} & X_{22} & \\cdots & X_{2p} \\\\\n\\vdots &\\vdots & \\vdots & \\cdots & \\vdots \\\\\n1& X_{n1} & X_{n2} & \\cdots & X_{np} \\\\\n\\end{pmatrix}\\), where\n\n\\(\\mathbf{Y}\\) is an \\(n \\times 1\\) vector.\n\\(\\mathbf{X}\\) is a \\(n \\times (p+1)\\) matrix."
  },
  {
    "objectID": "LinearRegression/LinearRegression.slides.html#section-2",
    "href": "LinearRegression/LinearRegression.slides.html#section-2",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "And,\n\\(\\boldsymbol{\\beta} = \\begin{pmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{pmatrix}\\) and \\(\\boldsymbol{\\epsilon} = \\begin{pmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{pmatrix}\\), where\n\n\\(\\boldsymbol{\\beta}\\) is an \\((p+1) \\times 1\\) vector.\n\\(\\boldsymbol{\\epsilon}\\) is an \\(n \\times 1\\) vector."
  },
  {
    "objectID": "LinearRegression/LinearRegression.slides.html#section-3",
    "href": "LinearRegression/LinearRegression.slides.html#section-3",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "The multiple linear regression model then is\n\\[\\mathbf{Y} =  \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}.\\] This expression means\n\\(\\begin{pmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{pmatrix} = \\begin{pmatrix}\n\\beta_0 + \\beta_1 X_{11} + \\beta_2 X_{12} + \\cdots + \\beta_p X_{1p} \\\\\n\\beta_0 + \\beta_1 X_{21} + \\beta_2 X_{22} + \\cdots + \\beta_p X_{2p} \\\\\n\\vdots  \\\\\n\\beta_0 + \\beta_1 X_{n1} + \\beta_2 X_{n2} + \\cdots + \\beta_p X_{np} \\\\\n\\end{pmatrix} + \\begin{pmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{pmatrix}\\)"
  },
  {
    "objectID": "LinearRegression/LinearRegression.slides.html#an-estimator-for-boldsymbolbeta",
    "href": "LinearRegression/LinearRegression.slides.html#an-estimator-for-boldsymbolbeta",
    "title": "Multiple Linear Regression",
    "section": "An estimator for \\(\\boldsymbol{\\beta}\\)",
    "text": "An estimator for \\(\\boldsymbol{\\beta}\\)\nOur goal is to find an estimator for the vector \\(\\boldsymbol{\\beta}\\) (and all its components). For the moment, let’s assume that we have one:\n\\(\\hat{\\boldsymbol{\\beta}} = \\begin{pmatrix} \\hat{\\beta}_0 \\\\ \\hat{\\beta}_1 \\\\ \\vdots\\\\ \\hat{\\beta}_p \\end{pmatrix}\\), where \\(\\hat{\\beta}_j\\) is an estimator for \\(\\beta_j\\), \\(j = 0, \\ldots, p\\).\nUsing this estimator, we can compute the predicted responses of our model \\(\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\), where \\(\\hat{\\mathbf{Y}} = (\\hat{Y}_1, \\hat{Y}_2, \\ldots, \\hat{Y}_n)^{T}\\) and \\(\\hat{Y}_i\\) is the i-th predicted response."
  },
  {
    "objectID": "LinearRegression/LinearRegression.slides.html#section-4",
    "href": "LinearRegression/LinearRegression.slides.html#section-4",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "The expression \\(\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\) means\n\\[\\begin{pmatrix} \\hat{Y}_1 \\\\ \\hat{Y}_2 \\\\ \\vdots \\\\ \\hat{Y}_n \\end{pmatrix}  = \\begin{pmatrix}\n\\hat{\\beta}_0 + X_{11} \\hat{\\beta}_1 + X_{12} \\hat{\\beta}_2 + \\cdots + X_{1p} \\hat{\\beta}_p\\\\\n\\hat{\\beta}_0 + X_{21} \\hat{\\beta}_1 + X_{22}\\hat{\\beta}_2 + \\cdots + X_{2p}\\hat{\\beta}_p \\\\\n\\vdots  \\\\\n\\hat{\\beta}_0 + X_{n1} \\hat{\\beta}_1 + X_{n2}\\hat{\\beta}_2 + \\cdots + X_{np} \\hat{\\beta}_p \\\\\n\\end{pmatrix}\\]"
  },
  {
    "objectID": "LinearRegression/LinearRegression.slides.html#section-5",
    "href": "LinearRegression/LinearRegression.slides.html#section-5",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "This means that the residuals of the estimated model are\n\\[\\hat{\\boldsymbol{\\epsilon}} = \\mathbf{Y} - \\hat{\\mathbf{Y}} = \\mathbf{Y} - \\mathbf{X} \\hat{\\boldsymbol{\\beta}},\\]\nwhere \\(\\hat{\\boldsymbol{\\epsilon}} = (\\hat{\\epsilon}_1, \\hat{\\epsilon}_2, \\ldots, \\hat{\\epsilon}_n)^{T}\\) and \\(\\hat{\\epsilon}_i = Y_i - \\hat{Y}_i\\) is the i-th residual."
  },
  {
    "objectID": "LinearRegression/LinearRegression.slides.html#least-squares-estimator",
    "href": "LinearRegression/LinearRegression.slides.html#least-squares-estimator",
    "title": "Multiple Linear Regression",
    "section": "Least squares estimator",
    "text": "Least squares estimator\nTo find the best estimator for \\(\\boldsymbol{\\beta}\\) (and all its elements), we use the method of least squares. This method finds the best \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes the residual sum of squares (RSS):\n\\[RSS = \\left(\\mathbf{y} - \\mathbf{X} \\hat{\\boldsymbol{\\beta}}\\right)^{T} \\left(\\mathbf{y} - \\mathbf{X} \\hat{\\boldsymbol{\\beta}}\\right) = \\sum_{i=1}^{n} \\hat{\\epsilon}^2_i = \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2.\\]\nThe estimator that minimizes the expression above is called the least squares estimator:\n\\[\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^{T}\\mathbf{X})^{-1} \\mathbf{X}^{T}\\mathbf{y}\\]"
  },
  {
    "objectID": "LinearRegression/LinearRegression.slides.html#computation-of-hatboldsymbolbeta-mathbfxtmathbfx-1-mathbfxtmathbfy",
    "href": "LinearRegression/LinearRegression.slides.html#computation-of-hatboldsymbolbeta-mathbfxtmathbfx-1-mathbfxtmathbfy",
    "title": "Multiple Linear Regression",
    "section": "Computation of \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^{T}\\mathbf{X})^{-1} \\mathbf{X}^{T}\\mathbf{y}\\)",
    "text": "Computation of \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^{T}\\mathbf{X})^{-1} \\mathbf{X}^{T}\\mathbf{y}\\)\n\n\nCompute the transpose of a matrix: \\(\\mathbf{X}^{T}\\).\nCompute the product of a matrix and a vector: \\(\\mathbf{X}^{T}\\mathbf{Y}\\).\nCompute the product of two matrices: \\(\\mathbf{X}^{T} \\mathbf{X}\\).\nCompute the inverse of a matrix: \\((\\mathbf{X}^{T} \\mathbf{X})^{-1}\\)."
  },
  {
    "objectID": "LinearRegression/LinearRegression.slides.html#remarks",
    "href": "LinearRegression/LinearRegression.slides.html#remarks",
    "title": "Multiple Linear Regression",
    "section": "Remarks",
    "text": "Remarks\n\nCompute the inverse of a matrix: \\((\\mathbf{X}^{T} \\mathbf{X})^{-1}\\).\n\n\nNot all matrices have an inverse.\nIf it does not have an inverse then the matrix is called singular. Otherwise, it is called non-singular.\nFor the inverse to exist, the columns in \\(\\mathbf{X}\\) must be linearly independent.\nOr, equivalently, the determinant \\(|\\mathbf{X}^{T} \\mathbf{X}| &gt; 0\\)."
  },
  {
    "objectID": "LinearRegression/LinearRegression.slides.html#computation-in-python",
    "href": "LinearRegression/LinearRegression.slides.html#computation-in-python",
    "title": "Multiple Linear Regression",
    "section": "Computation in Python",
    "text": "Computation in Python\n\nTo compute the least squares estimates, we first split the data set into a matrix with the values of the predictors only, and a matrix with the response values.\n\n# Matrix with predictors.\nrayon_predictors = rayon_data.drop(columns=['Y'])\n\n# Add intercept.\nrayon_X_train = sm.add_constant(rayon_predictors)\n\n# Matrix with response.\nrayon_Y_train = rayon_data['Y']"
  },
  {
    "objectID": "LinearRegression/LinearRegression.slides.html#section-7",
    "href": "LinearRegression/LinearRegression.slides.html#section-7",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "Next, we use the functions OLS() and fit() from statsmodels.\n\n# Create linear regression object\nregr = sm.OLS(rayon_Y_train, rayon_X_train)\n\n# Train the model using the training sets\nlinear_model = regr.fit()"
  },
  {
    "objectID": "LinearRegression/LinearRegression.slides.html#section-8",
    "href": "LinearRegression/LinearRegression.slides.html#section-8",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "To show the estimated coefficients, we use the argument params of the linear_model object created previously.\n\n# The estimated coefficients.\nprint(linear_model.params)\n\nconst   -35.262607\nX1        0.745417\nX2       20.229167\nX3        0.793056\nX4       25.583333\nX5       17.208333\ndtype: float64\n\n\nThe elements in the vector above are the estimates \\(\\hat{\\beta}_0\\), \\(\\hat{\\beta}_1\\), \\(\\hat{\\beta}_2\\), \\(\\hat{\\beta}_3\\), \\(\\hat{\\beta}_4\\), and \\(\\hat{\\beta}_5\\)."
  },
  {
    "objectID": "LinearRegression/LinearRegression.slides.html#interpretation-of-estimated-coefficients",
    "href": "LinearRegression/LinearRegression.slides.html#interpretation-of-estimated-coefficients",
    "title": "Multiple Linear Regression",
    "section": "Interpretation of estimated coefficients",
    "text": "Interpretation of estimated coefficients\n\n\nThe average whiteness of a rayon is \\(\\hat{\\beta}_0 = -35.26\\) when all predictors are equal to 0.\nIncreasing the acid bath temperature by 1 unit increases the average whiteness of a rayon by \\(\\hat{\\beta}_1 = 0.745\\) units.\nIncreasing the cascade acid concentration by 1 unit increases the average whiteness of a rayon by \\(\\hat{\\beta}_2 = 20.23\\) units."
  },
  {
    "objectID": "LinearRegression/LinearRegression.slides.html#section-9",
    "href": "LinearRegression/LinearRegression.slides.html#section-9",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "Increasing the water temperature by 1 unit increases the average whiteness of a rayon by \\(\\hat{\\beta}_3 = 0.793\\) units.\nIncreasing the sulfide concentration by 1 unit increases the average whiteness of a rayon by \\(\\hat{\\beta}_4 = 25.583\\) units.\nIncreasing the amount of chlorine bleach by 1 unit increases the average whiteness of a rayon by \\(\\hat{\\beta}_5 = 17.208\\) units."
  },
  {
    "objectID": "LinearRegression/LinearRegression.slides.html#properties-of-least-squares-estimators",
    "href": "LinearRegression/LinearRegression.slides.html#properties-of-least-squares-estimators",
    "title": "Multiple Linear Regression",
    "section": "Properties of least squares estimators",
    "text": "Properties of least squares estimators\n\nIf all the assumptions of the linear regression model are satisfied, the least squares estimators have some attractive properties.\n\nFor example:\n\nOn average, \\(\\hat{\\beta}_{j}\\) equals the true parameter value \\(\\beta_{j}\\).\nEach \\(\\hat{\\beta}_{j}\\) follows a normal distribution with a specific mean and variance."
  },
  {
    "objectID": "LinearRegression/LinearRegression.slides.html#predictions",
    "href": "LinearRegression/LinearRegression.slides.html#predictions",
    "title": "Multiple Linear Regression",
    "section": "Predictions",
    "text": "Predictions\n\nOnce we estimate the intercept and model coefficients, we make predictions as follows:\n\\[\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_{i1} + \\hat{\\beta}_2 X_{i2} + \\cdots + \\hat{\\beta}_p X_{ip}\\]\nwhere \\(\\hat{Y}_i\\) is the i-th fitted or predicted response.\nIn Python, we use the argument fittedvalues to show the predicted responses of the estimated model.\n\n# Make predictions using the the model\nrayon_Y_pred = linear_model.fittedvalues"
  },
  {
    "objectID": "LinearRegression/LinearRegression.slides.html#section-10",
    "href": "LinearRegression/LinearRegression.slides.html#section-10",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "Predictions of the 26 observations in the training dataset.\n\nprint(rayon_Y_pred)\n\n0      72.205449\n1      78.205449\n2      80.405449\n3      79.522115\n4      83.738782\n5      82.855449\n6      85.055449\n7      91.055449\n8      90.555449\n9      89.672115\n10     91.872115\n11     97.872115\n12     95.205449\n13    101.205449\n14    103.405449\n15    102.522115\n16     74.176282\n17    103.992949\n18     80.992949\n19     97.176282\n20     84.326282\n21     93.842949\n22     86.526282\n23     91.642949\n24     85.642949\n25     92.526282\ndtype: float64"
  },
  {
    "objectID": "LinearRegression/LinearRegression.slides.html#residuals",
    "href": "LinearRegression/LinearRegression.slides.html#residuals",
    "title": "Multiple Linear Regression",
    "section": "Residuals",
    "text": "Residuals\n\nNow that we have introduced the estimator \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p\\), we can be more specific in our terminology of the linear model.\n\nThe errors of the estimated model are called residuals \\(\\hat{\\epsilon}_i = Y_i - \\hat{Y}_i\\), \\(i = 1, \\ldots, n.\\)\n\nIf the model is correct, the residuals \\(\\hat{\\epsilon}_1, \\hat{\\epsilon}_2, \\ldots, \\hat{\\epsilon}_n\\) give us a good idea of the errors \\(\\epsilon_1, \\epsilon_2, \\ldots, \\epsilon_n\\)."
  },
  {
    "objectID": "LinearRegression/LinearRegression.slides.html#section-11",
    "href": "LinearRegression/LinearRegression.slides.html#section-11",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "In Python, we compute the residuals using the following command.\n\nresiduals = linear_model.resid\n\nprint(residuals)\n\n0      4.294551\n1     -2.205449\n2     -0.505449\n3      3.977885\n4      5.761218\n5      1.344551\n6      0.644551\n7      8.444551\n8     -1.155449\n9      7.827885\n10    11.327885\n11    10.827885\n12    19.994551\n13    10.294551\n14    -1.105449\n15     5.577885\n16     6.023718\n17   -14.892949\n18    -3.792949\n19   -12.076282\n20   -12.826282\n21    -9.342949\n22    -9.026282\n23   -12.442949\n24   -14.642949\n25    -2.326282\ndtype: float64"
  },
  {
    "objectID": "LinearRegression/LinearRegression.slides.html#estimation-of-variance",
    "href": "LinearRegression/LinearRegression.slides.html#estimation-of-variance",
    "title": "Multiple Linear Regression",
    "section": "Estimation of variance",
    "text": "Estimation of variance\n\nThe variance \\(\\sigma^2\\) of the errors is estimated by\n\\[\\hat{\\sigma}^2=\\frac{1}{n-p-1}\\sum_{i=1}^{n} \\hat{\\epsilon}_i^{2}.\\]\nIn Python, we compute \\(\\hat{\\sigma}^2\\) as follows.\n\nerror_variance = linear_model.scale\n\nprint( round(error_variance, 3) )\n\n105.931"
  },
  {
    "objectID": "LinearRegression/LinearRegression.slides.html#section-12",
    "href": "LinearRegression/LinearRegression.slides.html#section-12",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "The smaller the value of \\(\\hat{\\sigma}^2\\), the closer our predictions are to the actual responses.\nIn practice, it is better to use the standard deviation of the errors. That is,\n\\[\\hat{\\sigma}=\\left(\\frac{1}{n-p-1}\\sum_{i=1}^{n} \\hat{\\epsilon}_i^{2}\\right)^{1/2}.\\] In Python, we compute \\(\\hat{\\sigma}\\) as follows:\n\nprint( round(error_variance**(1/2), 3) )\n\n10.292"
  },
  {
    "objectID": "LinearRegression/LinearRegression.slides.html#interpretation-of-hatsigma",
    "href": "LinearRegression/LinearRegression.slides.html#interpretation-of-hatsigma",
    "title": "Multiple Linear Regression",
    "section": "Interpretation of \\(\\hat{\\sigma}\\)",
    "text": "Interpretation of \\(\\hat{\\sigma}\\)\n\n\nThe smaller the \\(\\hat{\\sigma}\\), the closer our predictions are to the actual responses.\nThe \\(\\hat{\\sigma} = 10.292\\) implies that, on average, the predictions of our model are off or incorrect by 10.292 mpg."
  },
  {
    "objectID": "Tools/Tools1.slides.html#agenda",
    "href": "Tools/Tools1.slides.html#agenda",
    "title": "Tools for Data Science: Part I",
    "section": "Agenda",
    "text": "Agenda\n\n\nIntroduction to Python\nReading data with Python\nData manipulation with pandas"
  },
  {
    "objectID": "Tools/Tools1.slides.html#python",
    "href": "Tools/Tools1.slides.html#python",
    "title": "Tools for Data Science: Part I",
    "section": "Python",
    "text": "Python\n\n\n\nA versatile programming language.\nIt is free!\nIt is widely used for data cleaning, data visualization, and data modelling.\nIt can be extended with packages (libraries) developed by other users."
  },
  {
    "objectID": "Tools/Tools1.slides.html#google-colab",
    "href": "Tools/Tools1.slides.html#google-colab",
    "title": "Tools for Data Science: Part I",
    "section": "Google Colab",
    "text": "Google Colab\nGoogle’s free cloud collaboration platform for creating Python documents.\n\nRun Python and collaborate on Jupyter notebooks for free.\nHarness the power of GPUs for free to accelerate your data science projects.\nEasily save and upload your notebooks to Google Drive."
  },
  {
    "objectID": "Tools/Tools1.slides.html#lets-try-a-command-in-python",
    "href": "Tools/Tools1.slides.html#lets-try-a-command-in-python",
    "title": "Tools for Data Science: Part I",
    "section": "Let’s try a command in Python",
    "text": "Let’s try a command in Python\nWhat do you think will happen if we run this command?\n\nprint(\"Hello world!\")\n\n\n\n\nHello world!"
  },
  {
    "objectID": "Tools/Tools1.slides.html#lets-try-another-command",
    "href": "Tools/Tools1.slides.html#lets-try-another-command",
    "title": "Tools for Data Science: Part I",
    "section": "Let’s try another command",
    "text": "Let’s try another command\nWhat do you think will happen if we run this command?\n\nsum([1, 5, 10])\n\n\n\n\n16"
  },
  {
    "objectID": "Tools/Tools1.slides.html#use-python-as-a-basic-calculator",
    "href": "Tools/Tools1.slides.html#use-python-as-a-basic-calculator",
    "title": "Tools for Data Science: Part I",
    "section": "Use Python as a basic calculator",
    "text": "Use Python as a basic calculator\n\n5 + 1\n\n6\n\n\n\n10 - 3\n\n7\n\n\n\n2 * 4\n\n8\n\n\n\n9 / 3\n\n3.0"
  },
  {
    "objectID": "Tools/Tools1.slides.html#comments",
    "href": "Tools/Tools1.slides.html#comments",
    "title": "Tools for Data Science: Part I",
    "section": "Comments",
    "text": "Comments\n\nSometimes we write things in the coding window that we want Python to ignore. These are called comments and start with #.\n\nPython will ignore the comments and just execute the code.\n\n# you can put whatever after #\n# for example... blah blah blah\n\n\nSi desea escribir un comentario que ocupe más de una línea, es una buena idea poner un # al principio de cada línea."
  },
  {
    "objectID": "Tools/Tools1.slides.html#introduction-to-functions-in-python",
    "href": "Tools/Tools1.slides.html#introduction-to-functions-in-python",
    "title": "Tools for Data Science: Part I",
    "section": "Introduction to functions in Python",
    "text": "Introduction to functions in Python\n\nOne of the best things about Python is that there are many built-in commands you can use. These are called functions.\n\nFunctions have two basic parts:\n\n\nThe first part is the name of the function (for example, sum).\nThe second part is the input to the function, which goes inside the parentheses (sum([1, 5, 15]))."
  },
  {
    "objectID": "Tools/Tools1.slides.html#python-is-strict",
    "href": "Tools/Tools1.slides.html#python-is-strict",
    "title": "Tools for Data Science: Part I",
    "section": "Python is strict",
    "text": "Python is strict\nPython, like all programming languages, is very strict. For example, if you write\n\nsum([1, 100])\n\n101\n\n\nit will tell you the answer, 101.\n\nBut if you write\n\nSum([1, 100])\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[11], line 1\n----&gt; 1 Sum([1, 100])\n\nNameError: name 'Sum' is not defined\n\n\n\nwith the “s” capitalized, he will act like he has no idea what we are talking about!\n\nlo mismo si olvidas incluir un parentesis"
  },
  {
    "objectID": "Tools/Tools1.slides.html#save-your-work-in-python-objects",
    "href": "Tools/Tools1.slides.html#save-your-work-in-python-objects",
    "title": "Tools for Data Science: Part I",
    "section": "Save your work in Python objects",
    "text": "Save your work in Python objects\n\nVirtually anything, including the results of any Python function, can be saved in an object.\nThis is accomplished by using an assignment operator, which can be an equals symbol (=).\n\nYou can make up any name you want for a Python object. However, there are two basic rules for this:\n\n\nIt has to be different from a function name in Python.\nIt has to be as specific as possible."
  },
  {
    "objectID": "Tools/Tools1.slides.html#for-example",
    "href": "Tools/Tools1.slides.html#for-example",
    "title": "Tools for Data Science: Part I",
    "section": "For example",
    "text": "For example\n\n\n# This code will assign the number 18\n# to the object called my_favorite_number\n\nmy_favorite_number = 18\n\nAfter running this code, nothing happens. But if we run the object on its own, we can see what’s inside it.\n\nmy_favorite_number\n\n18\n\n\nYou can also use print(my_favorite_number)."
  },
  {
    "objectID": "Tools/Tools1.slides.html#lists",
    "href": "Tools/Tools1.slides.html#lists",
    "title": "Tools for Data Science: Part I",
    "section": "Lists",
    "text": "Lists\n\nSo far we have used Python objects to store a single number. But in statistics we are dealing with variation, which by definition needs more than one number.\n\nA Python object can also store a complete set of numbers, called a list.\nYou can think of a list as a vector of numbers (or values).\n\n\nThe [] command can be used to combine several individual values into a list.\n\npuedes pensar que el c es por combinar"
  },
  {
    "objectID": "Tools/Tools1.slides.html#for-example-1",
    "href": "Tools/Tools1.slides.html#for-example-1",
    "title": "Tools for Data Science: Part I",
    "section": "For example",
    "text": "For example\n\nThis code creates two vectors\n\nmy_list = [1, 2, 3, 4, 5]\nmy_list_2 = [10, 10, 10, 10, 10]\n\nLet’s see its content\n\nmy_list\n\n[1, 2, 3, 4, 5]\n\n\n\nmy_list_2\n\n[10, 10, 10, 10, 10]"
  },
  {
    "objectID": "Tools/Tools1.slides.html#operations",
    "href": "Tools/Tools1.slides.html#operations",
    "title": "Tools for Data Science: Part I",
    "section": "Operations",
    "text": "Operations\n\nWe can do simple operations with vectors. For example, we can sum all the elements of a list.\n\nmy_list = [1, 2, 3, 4, 5]\nsum(my_list)\n\n15"
  },
  {
    "objectID": "Tools/Tools1.slides.html#indexing",
    "href": "Tools/Tools1.slides.html#indexing",
    "title": "Tools for Data Science: Part I",
    "section": "Indexing",
    "text": "Indexing\n\nWe can index a position in the vector using square brackets with a number like this: [1].\nSo, if we wanted to print the contents of the first position in my_list, we could write\n\nmy_list[1]\n\n2\n\n\nAn feature of Python is that the first element of a list or vector is indexed using the number 0.\n\nmy_list[0]\n\n1"
  },
  {
    "objectID": "Tools/Tools1.slides.html#a-little-more-about-objects-in-python",
    "href": "Tools/Tools1.slides.html#a-little-more-about-objects-in-python",
    "title": "Tools for Data Science: Part I",
    "section": "A little more about objects in Python",
    "text": "A little more about objects in Python\n\nYou can think of Python objects as containers that hold values.\nA Python object can hold a single value, or it can hold a group of values (as in a vector).\nSo far, we’ve only put numbers into Python objects.\n\n\nPython objects can actually contain three types of values: numbers, characters, and booleans."
  },
  {
    "objectID": "Tools/Tools1.slides.html#character-values",
    "href": "Tools/Tools1.slides.html#character-values",
    "title": "Tools for Data Science: Part I",
    "section": "Character values",
    "text": "Character values\n\nCharacters are made up of text, such as words or sentences. An example of a list with characters as elements is:\n\n\nmany_greetings = [\"hi\", \"hello\", \"hola\", \"bonjour\", \"ni hao\", \"merhaba\"]\nmany_greetings\n\n['hi', 'hello', 'hola', 'bonjour', 'ni hao', 'merhaba']\n\n\n\n\nIt is important to know that numbers can also be treated as characters, depending on the context.\nFor example, when 20 is enclosed in quotes (\"20\") it will be treated as a character value, even though it encloses a number in quotes."
  },
  {
    "objectID": "Tools/Tools1.slides.html#boolean-values",
    "href": "Tools/Tools1.slides.html#boolean-values",
    "title": "Tools for Data Science: Part I",
    "section": "Boolean values",
    "text": "Boolean values\n\nBoolean values are True or False.\nWe may have a question like:\n\nIs the first element of the vector many_greetings \"hola\"?\n\n\nWe can ask Python to find out and return the answer True or False.\n\nmany_greetings[1] == \"hola\"\n\nFalse"
  },
  {
    "objectID": "Tools/Tools1.slides.html#logical-operators",
    "href": "Tools/Tools1.slides.html#logical-operators",
    "title": "Tools for Data Science: Part I",
    "section": "Logical operators",
    "text": "Logical operators\n\nMost of the questions we ask Python to answer with True or False involve comparison operators like &gt;, &lt;, &gt;=, &lt;=, and ==.\nThe double == sign checks whether two values are equal. There is even a comparison operator to check whether values are not equal: !=.\nFor example, 5 != 3 is a True statement."
  },
  {
    "objectID": "Tools/Tools1.slides.html#common-logical-operators",
    "href": "Tools/Tools1.slides.html#common-logical-operators",
    "title": "Tools for Data Science: Part I",
    "section": "Common logical operators",
    "text": "Common logical operators\n\n\n&gt; (larger than)\n&gt;= (larger than or equal to)\n&lt; (smaller than)\n&lt;= (smaller than or equal to)\n== (equal to)\n!= (not equal to)"
  },
  {
    "objectID": "Tools/Tools1.slides.html#question",
    "href": "Tools/Tools1.slides.html#question",
    "title": "Tools for Data Science: Part I",
    "section": "Question",
    "text": "Question\n\nRead this code and predict its response. Then, run the code in Google Colab and validate if you were correct.\n\nA = 1\nB = 5\ncompare = A &gt; B\ncompare"
  },
  {
    "objectID": "Tools/Tools1.slides.html#programming-culture-trial-and-error",
    "href": "Tools/Tools1.slides.html#programming-culture-trial-and-error",
    "title": "Tools for Data Science: Part I",
    "section": "Programming culture: Trial and error",
    "text": "Programming culture: Trial and error\n\nThe best way to learn programming is to try things out and see what happens. Write some code, run it, and think about why it didn’t work.\nThere are many ways to make small mistakes in programming (for example, typing a capital letter when a lowercase letter is needed).\nWe often have to find these mistakes through trial and error."
  },
  {
    "objectID": "Tools/Tools1.slides.html#python-libraries",
    "href": "Tools/Tools1.slides.html#python-libraries",
    "title": "Tools for Data Science: Part I",
    "section": "Python libraries",
    "text": "Python libraries\n\nLibraries are the fundamental units of reproducible Python code. They include reusable Python functions, documentation describing how to use them, and sample data.\nIn this course, we will be working mostly with the following libraries:\n\npandas for data manipulation\nmatplotlib and seaborn for data visualization\nstatsmodels and scikit-learn for data modelling"
  },
  {
    "objectID": "Tools/Tools1.slides.html#data-organization",
    "href": "Tools/Tools1.slides.html#data-organization",
    "title": "Tools for Data Science: Part I",
    "section": "Data organization",
    "text": "Data organization\nIn data science, we organize data into rows and columns.\n\n    Condition  Age   Wt    Wt2\n1  Uninformed   35  136  135.8\n2  Uninformed   45  162  161.8\n3    Informed   52  117  116.8\n4    Informed   29  184  182.8\n5  Uninformed   38  134  136.6\n6    Informed   39  189  183.2\n\n\nThe rows are the sampled cases. In this example, the rows are housekeepers from different hotels. There are six rows, so there are six housekeepers in this data set.\n\n\nDepending on the study, the rows could be people, states, couples, mice—any case you’re taking a sample from to study."
  },
  {
    "objectID": "Tools/Tools1.slides.html#section",
    "href": "Tools/Tools1.slides.html#section",
    "title": "Tools for Data Science: Part I",
    "section": "",
    "text": "The columns represent variables or attributes of each case that were measured.\n\n    Condition  Age   Wt    Wt2\n1  Uninformed   35  136  135.8\n2  Uninformed   45  162  161.8\n3    Informed   52  117  116.8\n4    Informed   29  184  182.8\n5  Uninformed   38  134  136.6\n6    Informed   39  189  183.2\n\n\n\nIn this study, housekeepers were either informed or not that their daily work of cleaning hotel rooms was equivalent to getting adequate exercise for good health.\n\n\n\nSo one of the variables, Condition, indicates whether they were informed of this fact or not."
  },
  {
    "objectID": "Tools/Tools1.slides.html#section-1",
    "href": "Tools/Tools1.slides.html#section-1",
    "title": "Tools for Data Science: Part I",
    "section": "",
    "text": "Other variables include the age of the housekeeper (Age), her weight before starting the study (Wt), and her weight at the end of the study (Wt2), measured four weeks later.\n\nTherefore, the values in each row represent the values of that particular case in each of the variables measured.\n    Condition  Age   Wt    Wt2\n1  Uninformed   35  136  135.8\n2  Uninformed   45  162  161.8\n3    Informed   52  117  116.8\n4    Informed   29  184  182.8\n5  Uninformed   38  134  136.6\n6    Informed   39  189  183.2\n\n¿Cuántas variables hay en este conjunto de datos?"
  },
  {
    "objectID": "Tools/Tools1.slides.html#loading-data-in-python",
    "href": "Tools/Tools1.slides.html#loading-data-in-python",
    "title": "Tools for Data Science: Part I",
    "section": "Loading data in Python",
    "text": "Loading data in Python\nIn this course, we will assume that data is stored in an Excel file with the above organization. As an example, let’s use the file penguins.xlsx.\n\n\n\n\n\n\n\nThe file must be previously uploaded to Google Colab."
  },
  {
    "objectID": "Tools/Tools1.slides.html#section-2",
    "href": "Tools/Tools1.slides.html#section-2",
    "title": "Tools for Data Science: Part I",
    "section": "",
    "text": "The dataset penguins.xlsx contains data from penguins living in three islands."
  },
  {
    "objectID": "Tools/Tools1.slides.html#pandas-library",
    "href": "Tools/Tools1.slides.html#pandas-library",
    "title": "Tools for Data Science: Part I",
    "section": "pandas library",
    "text": "pandas library\n\n\n\n\n\n\n\n\n\n\npandas is an open-source Python library for data manipulation and analysis.\nIt is built on top of numpy for high-performance data operations..\nIt allows the user to import, clean, transform, and analyze data efficiently\nhttps://pandas.pydata.org/"
  },
  {
    "objectID": "Tools/Tools1.slides.html#importing-pandas",
    "href": "Tools/Tools1.slides.html#importing-pandas",
    "title": "Tools for Data Science: Part I",
    "section": "Importing pandas",
    "text": "Importing pandas\nFortunately, the pandas library is already pre-installed in Google Colab.\n\nHowever, we need to inform Google Colab that we want to use pandas and its functions using the following command:\n\nimport pandas as pd\n\n\nThe command as pd allows us to have a short name for pandas. To use a function of pandas, we use the command pd.function()."
  },
  {
    "objectID": "Tools/Tools1.slides.html#loading-data-using-pandas",
    "href": "Tools/Tools1.slides.html#loading-data-using-pandas",
    "title": "Tools for Data Science: Part I",
    "section": "Loading data using pandas",
    "text": "Loading data using pandas\n\nThe following code shows how to read the data in the file “penguins.xlsx” into Python.\n\n# Load the Excel file into a pandas DataFrame.\npenguins_data = pd.read_excel(\"penguins.xlsx\")"
  },
  {
    "objectID": "Tools/Tools1.slides.html#the-function-head",
    "href": "Tools/Tools1.slides.html#the-function-head",
    "title": "Tools for Data Science: Part I",
    "section": "The function head()",
    "text": "The function head()\nThe function head() allows you to print the first rows of a pandas data frame.\n\n# Print the first 4 rows of the dataset.\npenguins_data.head(4)\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007"
  },
  {
    "objectID": "Tools/Tools1.slides.html#indexing-variables-a-dataset",
    "href": "Tools/Tools1.slides.html#indexing-variables-a-dataset",
    "title": "Tools for Data Science: Part I",
    "section": "Indexing variables a dataset",
    "text": "Indexing variables a dataset\nWe can select a specific variables of a data frame using the syntaxis below.\n\npenguins_data['bill_length_mm']\n\n0      39.1\n1      39.5\n2      40.3\n3       NaN\n4      36.7\n       ... \n339    55.8\n340    43.5\n341    49.6\n342    50.8\n343    50.2\nName: bill_length_mm, Length: 344, dtype: float64\n\n\nHere, we selected the variable bill_length_mm in the penguins_data dataset."
  },
  {
    "objectID": "Tools/Tools1.slides.html#section-3",
    "href": "Tools/Tools1.slides.html#section-3",
    "title": "Tools for Data Science: Part I",
    "section": "",
    "text": "To index multiple variables of a data frame, we put the names of the variables in a list object. For example, we select bill_length_mm, species, and island as follows:\n\nsub_penguins_data = penguins_data[ ['bill_length_mm',  'species', 'island'] ]\nsub_penguins_data.head()\n\n\n\n\n\n\n\n\nbill_length_mm\nspecies\nisland\n\n\n\n\n0\n39.1\nAdelie\nTorgersen\n\n\n1\n39.5\nAdelie\nTorgersen\n\n\n2\n40.3\nAdelie\nTorgersen\n\n\n3\nNaN\nAdelie\nTorgersen\n\n\n4\n36.7\nAdelie\nTorgersen"
  },
  {
    "objectID": "Tools/Tools1.slides.html#indexing-rows",
    "href": "Tools/Tools1.slides.html#indexing-rows",
    "title": "Tools for Data Science: Part I",
    "section": "Indexing rows",
    "text": "Indexing rows\nTo index rows in a dataset, we use the argument loc from pandas. For example, we select the rows 3 to 6 of the penguins_dataset dataset:\n\nrows_penguins_data = penguins_data.loc[2:5]\nrows_penguins_data\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nmale\n2007"
  },
  {
    "objectID": "Tools/Tools1.slides.html#section-4",
    "href": "Tools/Tools1.slides.html#section-4",
    "title": "Tools for Data Science: Part I",
    "section": "",
    "text": "rows_penguins_data\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nmale\n2007\n\n\n\n\n\n\n\nNote that the index 2 and 5 refer to observations 3 and 7, respectively, in the dataset. This is because the first index in Python is 0."
  },
  {
    "objectID": "Tools/Tools1.slides.html#indexing-rows-and-columns",
    "href": "Tools/Tools1.slides.html#indexing-rows-and-columns",
    "title": "Tools for Data Science: Part I",
    "section": "Indexing rows and columns",
    "text": "Indexing rows and columns\nUsing loc, we can also retrieve a subset from the dataset by selecting specific columns and rows.\n\nsub_rows_pdata = penguins_data.loc[2:5, ['bill_length_mm',  'species', 'island'] ]\nsub_rows_pdata\n\n\n\n\n\n\n\n\nbill_length_mm\nspecies\nisland\n\n\n\n\n2\n40.3\nAdelie\nTorgersen\n\n\n3\nNaN\nAdelie\nTorgersen\n\n\n4\n36.7\nAdelie\nTorgersen\n\n\n5\n39.3\nAdelie\nTorgersen"
  },
  {
    "objectID": "Tools/Tools1.slides.html#chaining-operations-with-pandas",
    "href": "Tools/Tools1.slides.html#chaining-operations-with-pandas",
    "title": "Tools for Data Science: Part I",
    "section": "Chaining operations with pandas",
    "text": "Chaining operations with pandas\nOne of the most important techniques in pandas is chaining, which allows for cleaner and more readable data manipulation.\nThe general structure of chaining looks like this:"
  },
  {
    "objectID": "Tools/Tools1.slides.html#key-pandas-methods",
    "href": "Tools/Tools1.slides.html#key-pandas-methods",
    "title": "Tools for Data Science: Part I",
    "section": "Key pandas methods",
    "text": "Key pandas methods\npandas provides methods or functions to solve common data manipulation tasks:\n\n\n.filter() selects specific columns or rows.\n.query() filters observations based on conditions.\n.assign() adds new variables that are functions of existing variables.\n.sort_values() changes the order of rows.\n.agg() reduces multiple values to a single numerical summary."
  },
  {
    "objectID": "Tools/Tools1.slides.html#section-5",
    "href": "Tools/Tools1.slides.html#section-5",
    "title": "Tools for Data Science: Part I",
    "section": "",
    "text": "To practice, we will use the dataset penguins_data."
  },
  {
    "objectID": "Tools/Tools1.slides.html#selecting-columns-with-.filter",
    "href": "Tools/Tools1.slides.html#selecting-columns-with-.filter",
    "title": "Tools for Data Science: Part I",
    "section": "Selecting columns with .filter()",
    "text": "Selecting columns with .filter()\nSelect the columns species, body_mass_g and sex.\n\n(penguins_data\n  .filter([\"species\", \"body_mass_g\", \"sex\"], axis = 1)\n).head()\n\n\n\n\n\n\n\n\nspecies\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\n3750.0\nmale\n\n\n1\nAdelie\n3800.0\nfemale\n\n\n2\nAdelie\n3250.0\nfemale\n\n\n3\nAdelie\nNaN\nNaN\n\n\n4\nAdelie\n3450.0\nfemale"
  },
  {
    "objectID": "Tools/Tools1.slides.html#section-6",
    "href": "Tools/Tools1.slides.html#section-6",
    "title": "Tools for Data Science: Part I",
    "section": "",
    "text": "The axis argument tells .filter() whether to select rows (0) or columns (1) from the dataframe.\n\n(penguins_data\n  .filter([\"species\", \"body_mass_g\", \"sex\"], axis = 1)\n).head()\n\n\n\nThe .head() command allows us to print the first six rows of the newly produced dataframe. We must remove it to have the entire new dataframe."
  },
  {
    "objectID": "Tools/Tools1.slides.html#section-7",
    "href": "Tools/Tools1.slides.html#section-7",
    "title": "Tools for Data Science: Part I",
    "section": "",
    "text": "We can also use .filter() to select rows too. To this end, we set axis = 1. We can select specific rows, such as 0 and 10.\n\n(penguins_data\n  .filter([0, 10], axis = 0)\n)\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n10\nAdelie\nTorgersen\n37.8\n17.1\n186.0\n3300.0\nNaN\n2007"
  },
  {
    "objectID": "Tools/Tools1.slides.html#section-8",
    "href": "Tools/Tools1.slides.html#section-8",
    "title": "Tools for Data Science: Part I",
    "section": "",
    "text": "Or, we can select a set of rows using the function range(). For example, let’s select the first 5 rows.\n\n(penguins_data\n  .filter(range(5), axis = 0)\n)\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007"
  },
  {
    "objectID": "Tools/Tools1.slides.html#filtering-rows-with-.query",
    "href": "Tools/Tools1.slides.html#filtering-rows-with-.query",
    "title": "Tools for Data Science: Part I",
    "section": "Filtering rows with .query()",
    "text": "Filtering rows with .query()\n\nAn alternative way of selecting rows is .query(). Compared to .filter(), .query() allows us to filter the data using statements or queries involving the variables.\n\nFor example, let’s filter the data for the species “Gentoo.”\n\n(penguins_data\n  .query(\"species == 'Gentoo'\")\n)"
  },
  {
    "objectID": "Tools/Tools1.slides.html#section-9",
    "href": "Tools/Tools1.slides.html#section-9",
    "title": "Tools for Data Science: Part I",
    "section": "",
    "text": "(penguins_data\n  .query(\"species == 'Gentoo'\")\n).head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n152\nGentoo\nBiscoe\n46.1\n13.2\n211.0\n4500.0\nfemale\n2007\n\n\n153\nGentoo\nBiscoe\n50.0\n16.3\n230.0\n5700.0\nmale\n2007\n\n\n154\nGentoo\nBiscoe\n48.7\n14.1\n210.0\n4450.0\nfemale\n2007\n\n\n155\nGentoo\nBiscoe\n50.0\n15.2\n218.0\n5700.0\nmale\n2007\n\n\n156\nGentoo\nBiscoe\n47.6\n14.5\n215.0\n5400.0\nmale\n2007"
  },
  {
    "objectID": "Tools/Tools1.slides.html#section-10",
    "href": "Tools/Tools1.slides.html#section-10",
    "title": "Tools for Data Science: Part I",
    "section": "",
    "text": "We can also filter the data to get penguins with a body mass greater than 5000g.\n\n(penguins_data\n  .query(\"body_mass_g &gt; 5000\")\n).head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n153\nGentoo\nBiscoe\n50.0\n16.3\n230.0\n5700.0\nmale\n2007\n\n\n155\nGentoo\nBiscoe\n50.0\n15.2\n218.0\n5700.0\nmale\n2007\n\n\n156\nGentoo\nBiscoe\n47.6\n14.5\n215.0\n5400.0\nmale\n2007\n\n\n159\nGentoo\nBiscoe\n46.7\n15.3\n219.0\n5200.0\nmale\n2007\n\n\n161\nGentoo\nBiscoe\n46.8\n15.4\n215.0\n5150.0\nmale\n2007"
  },
  {
    "objectID": "Tools/Tools1.slides.html#section-11",
    "href": "Tools/Tools1.slides.html#section-11",
    "title": "Tools for Data Science: Part I",
    "section": "",
    "text": "We can even combine .filter() and .query(). For example, let’s select the columns species, body_mass_g and sex, then filter the data for the “Gentoo” species.\n\n(penguins_data\n  .filter([\"species\", \"body_mass_g\", \"sex\"], axis = 1)\n  .query(\"species == 'Gentoo'\")\n).head(4)\n\n\n\n\n\n\n\n\nspecies\nbody_mass_g\nsex\n\n\n\n\n152\nGentoo\n4500.0\nfemale\n\n\n153\nGentoo\n5700.0\nmale\n\n\n154\nGentoo\n4450.0\nfemale\n\n\n155\nGentoo\n5700.0\nmale"
  },
  {
    "objectID": "Tools/Tools1.slides.html#create-new-columns-with-.assign",
    "href": "Tools/Tools1.slides.html#create-new-columns-with-.assign",
    "title": "Tools for Data Science: Part I",
    "section": "Create new columns with .assign()",
    "text": "Create new columns with .assign()\nWith .assign(), we can create new columns (variables) that are functions of existing ones. This function uses a special Python keyword called lambda. Technically, this keyword defines an anonymous function.\nFor example, we create a new variable LDRatio equaling the ratio of bill_length_mm and bill_depth_mm.\n\n(penguins_data\n  .assign(LDRatio = lambda df: df[\"bill_length_mm\"] / df[\"bill_depth_mm\"])\n)"
  },
  {
    "objectID": "Tools/Tools1.slides.html#section-12",
    "href": "Tools/Tools1.slides.html#section-12",
    "title": "Tools for Data Science: Part I",
    "section": "",
    "text": "In this code, the df after lambda indicates that the dataframe (penguins_data) will be referred to as df inside the function. The colon : sets the start of the function.\n\n(penguins_data\n  .assign(LDRatio = lambda df: df[\"bill_length_mm\"] / df[\"bill_depth_mm\"])\n)\n\nThe code appends the new variable to the end of the resulting dataframe."
  },
  {
    "objectID": "Tools/Tools1.slides.html#section-13",
    "href": "Tools/Tools1.slides.html#section-13",
    "title": "Tools for Data Science: Part I",
    "section": "",
    "text": "We can see the new variable using .filter().\n\n(penguins_data\n  .assign(LDRatio = lambda df: df[\"bill_length_mm\"] / df[\"bill_depth_mm\"])\n  .filter([\"bill_length_mm\", \"bill_depth_mm\", \"LDRatio\"], axis = 1)\n).head()\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nLDRatio\n\n\n\n\n0\n39.1\n18.7\n2.090909\n\n\n1\n39.5\n17.4\n2.270115\n\n\n2\n40.3\n18.0\n2.238889\n\n\n3\nNaN\nNaN\nNaN\n\n\n4\n36.7\n19.3\n1.901554"
  },
  {
    "objectID": "Tools/Tools1.slides.html#sorting-with-.sort_values",
    "href": "Tools/Tools1.slides.html#sorting-with-.sort_values",
    "title": "Tools for Data Science: Part I",
    "section": "Sorting with .sort_values()",
    "text": "Sorting with .sort_values()\nWe can sort the data based on a column like bill_length_mm.\n\n(penguins_data\n  .sort_values(\"bill_length_mm\")\n).head(4)\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n142\nAdelie\nDream\n32.1\n15.5\n188.0\n3050.0\nfemale\n2009\n\n\n98\nAdelie\nDream\n33.1\n16.1\n178.0\n2900.0\nfemale\n2008\n\n\n70\nAdelie\nTorgersen\n33.5\n19.0\n190.0\n3600.0\nfemale\n2008\n\n\n92\nAdelie\nDream\n34.0\n17.1\n185.0\n3400.0\nfemale\n2008"
  },
  {
    "objectID": "Tools/Tools1.slides.html#section-14",
    "href": "Tools/Tools1.slides.html#section-14",
    "title": "Tools for Data Science: Part I",
    "section": "",
    "text": "To sort in descending order, use ascending=False inside sort_values().\n\n(penguins_data\n  .sort_values(\"bill_length_mm\", ascending=False)\n).head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n185\nGentoo\nBiscoe\n59.6\n17.0\n230.0\n6050.0\nmale\n2007\n\n\n293\nChinstrap\nDream\n58.0\n17.8\n181.0\n3700.0\nfemale\n2007\n\n\n253\nGentoo\nBiscoe\n55.9\n17.0\n228.0\n5600.0\nmale\n2009\n\n\n339\nChinstrap\nDream\n55.8\n19.8\n207.0\n4000.0\nmale\n2009\n\n\n267\nGentoo\nBiscoe\n55.1\n16.0\n230.0\n5850.0\nmale\n2009"
  },
  {
    "objectID": "Tools/Tools1.slides.html#summarizing-with-.agg",
    "href": "Tools/Tools1.slides.html#summarizing-with-.agg",
    "title": "Tools for Data Science: Part I",
    "section": "Summarizing with .agg()",
    "text": "Summarizing with .agg()\nWe can calculate summary statistics of the columns bill_length_mm, bill_depth_mm, and body_mass_g.\n\n(penguins_data\n  .filter([\"bill_length_mm\", \"bill_depth_mm\", \"body_mass_g\"], axis = 1)\n  .agg([\"mean\"])\n)\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nbody_mass_g\n\n\n\n\nmean\n43.92193\n17.15117\n4201.754386\n\n\n\n\n\n\n\n\n\nBy default, agg() ignores missing values."
  },
  {
    "objectID": "Tools/Tools1.slides.html#saving-results-in-new-objects",
    "href": "Tools/Tools1.slides.html#saving-results-in-new-objects",
    "title": "Tools for Data Science: Part I",
    "section": "Saving results in new objects",
    "text": "Saving results in new objects\n\nAfter performing operations on our data, we can save the modified dataset as a new object.\n\nmean_penguins_data = (penguins_data\n  .filter([\"bill_length_mm\", \"bill_depth_mm\", \"body_mass_g\"], axis = 1)\n  .agg([\"mean\"])\n)\n\nmean_penguins_data\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nbody_mass_g\n\n\n\n\nmean\n43.92193\n17.15117\n4201.754386"
  },
  {
    "objectID": "Tools/Tools1.slides.html#more-on-pandas",
    "href": "Tools/Tools1.slides.html#more-on-pandas",
    "title": "Tools for Data Science: Part I",
    "section": "More on pandas",
    "text": "More on pandas\n\n\nhttps://wesmckinney.com/book/"
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#agenda",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#agenda",
    "title": "Model Evaluation and Inference",
    "section": "Agenda",
    "text": "Agenda\n\n\nResidual analysis\nInference about individual \\(\\beta\\)’s using t-tests\nMultiple and adjusted \\(R^2\\) statistics"
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#load-the-libraries",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#load-the-libraries",
    "title": "Model Evaluation and Inference",
    "section": "Load the libraries",
    "text": "Load the libraries\n\nLet’s import statsmodels into Python together with the other relevant libraries.\n\n# Importing necessary libraries\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm"
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#multiple-linear-regression-model",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#multiple-linear-regression-model",
    "title": "Model Evaluation and Inference",
    "section": "Multiple linear regression model",
    "text": "Multiple linear regression model\n\n\\[Y_i = \\beta_0 + \\beta_1 X_{i1} + \\cdots + \\beta_p X_{ip} + \\epsilon_i\\]\n\n\\(p\\) is the number of predictors.\n\\(n\\) is the number of observations.\n\\(X_{ij}\\) is the i-th observation of the j-th predictor.\n\\(Y_{i}\\) is the i-th observation of the response.\n\\(\\epsilon_i\\) is the i-th random error."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#assumptions",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#assumptions",
    "title": "Model Evaluation and Inference",
    "section": "Assumptions",
    "text": "Assumptions\n\nThe error \\(\\epsilon_i\\)’s must then satisfy the following assumptions:\n\n\nOn average, they are close to zero for any value of the predictors \\(X_j\\).\nFor any value of the predictor \\(X_j\\), the dispersion or variance is constant and equal to \\(\\sigma^2\\).\nThe \\(\\epsilon_i\\)’s are all independent from each other.\nThe \\(\\epsilon_i\\)’s follow normal distribution with mean 0 and variance \\(\\sigma^2\\)."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#residuals",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#residuals",
    "title": "Model Evaluation and Inference",
    "section": "Residuals",
    "text": "Residuals\n\nThe errors \\(\\epsilon_1, \\ldots, \\epsilon_n\\) are not observed. To overcome this issue, we use the residuals of our model.\nSuppose that the multiple linear regression model is correct and consider the fitted responses \\(\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_{i1} + \\cdots + \\hat{\\beta}_p X_{ip}\\), where \\(\\hat{\\beta}_{j}\\) is the least squares estimator for the j-th predictor.\nWe define the residual residuals \\(\\hat{\\epsilon}_i = Y_i - \\hat{Y}_i\\), \\(i = 1, \\ldots, n.\\)\n\nThe residuals \\(\\hat{\\epsilon}_i\\) are the estimates of the random errors \\(\\epsilon_i\\)."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "If the model structure is correctly specified and assuming that the least-squares estimates \\(\\hat{\\beta}_j\\)’s are close to the true \\(\\beta_j\\)’s, respectively, we have that\n\\[\\hat{\\epsilon}_i = Y_i - \\hat{Y}_i = Y_i - \\hat{\\beta}_0 + \\hat{\\beta}_1 X_{i1} + \\cdots + \\hat{\\beta}_p X_{ip} \\approx \\epsilon_i\\]\nSo, the residuals \\(\\hat{\\epsilon}_i\\) should resemble the random errors \\(\\epsilon\\).\nTo evaluate the assumption of a (simple and) multiple linear regression model, we use a Residual analysis."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#residual-analysis-1",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#residual-analysis-1",
    "title": "Model Evaluation and Inference",
    "section": "Residual Analysis",
    "text": "Residual Analysis\n\nTo check the validity of these assumptions, we will follow a graphical approach. Specifically, we will construct three informative plots of the residuals.\n\n\nResiduals vs Fitted Values Plot. To assess the structure of the model and check for constant variance\nResiduals Vs Time Plot. To check independence.\nNormal Quantile-Quantile Plot. To assess if the residuals follow a normal distribution"
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#example-1",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#example-1",
    "title": "Model Evaluation and Inference",
    "section": "Example 1",
    "text": "Example 1\n\n\nThis example is inspired by Foster, Stine and Waterman (1997, pages 191–199).\nThe data are in the form of the time taken (in minutes) for a production run, \\(Y\\), and the number of items produced, \\(X\\), for 20 randomly selected orders as supervised by a manager.\nWe wish to develop an equation to model the relationship between the run time (\\(Y\\)) and the run size (\\(X\\))."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#dataset",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#dataset",
    "title": "Model Evaluation and Inference",
    "section": "Dataset",
    "text": "Dataset\nThe dataset is in the file “production.xlsx”.\n\nproduction_data = pd.read_excel('production.xlsx')\nproduction_data.head()\n\n\n\n\n\n\n\n\nCase\nRunTime\nRunSize\n\n\n\n\n0\n1\n195\n175\n\n\n1\n2\n215\n189\n\n\n2\n3\n243\n344\n\n\n3\n4\n162\n88\n\n\n4\n5\n185\n114"
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#intuition-behind",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#intuition-behind",
    "title": "Model Evaluation and Inference",
    "section": "Intuition behind …",
    "text": "Intuition behind …\nthe Residuals versus Fitted Values plot."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#calculation-of-fitted-values-and-residuals",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#calculation-of-fitted-values-and-residuals",
    "title": "Model Evaluation and Inference",
    "section": "Calculation of fitted values and residuals",
    "text": "Calculation of fitted values and residuals\nRecall that we can calculate the predicted values and residuals using commands from statsmodels.\n\n# Defining the predictor (X) and the response variable (Y).\nprod_Y_train = production_data['RunTime']\nprod_X_pred = production_data['RunSize']\nprod_X_train = sm.add_constant(prod_X_pred)\n\n# Fitting the simple linear regression model.\nregr = sm.OLS(prod_Y_train, prod_X_train)\nlinear_model = regr.fit()\n\n# Make predictions using the the model\nprod_Y_pred = linear_model.fittedvalues\n\n# Calculate residuals.\nresiduals = linear_model.resid"
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#residuals-vs-fitted-values",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#residuals-vs-fitted-values",
    "title": "Model Evaluation and Inference",
    "section": "Residuals vs Fitted Values",
    "text": "Residuals vs Fitted Values\n\n\n\n\nCode\n# Residual vs Fitted Values Plot\nplt.figure(figsize=(5, 5))\nsns.scatterplot(x = prod_Y_pred, y = residuals)\nplt.axhline(y=0, color='red', linestyle='--')\nplt.title('Residuals vs Fitted Values')\nplt.xlabel('Fitted (predicted) Values')\nplt.ylabel('Residuals')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nIf there is a trend, the model is misspecified.\nA “funnel” shape indicates that the assumption of constant variance is not met."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-1",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-1",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "Examples of plots that do not support the conclusion of constant variance."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-2",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-2",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "Another example.\n\nThe phenomenon of non-constant variance is called heteroscedasticity."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#residuals-vs-time-plot",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#residuals-vs-time-plot",
    "title": "Model Evaluation and Inference",
    "section": "Residuals vs Time Plot",
    "text": "Residuals vs Time Plot\n\n\nBy “time,” we mean that time the observation was taken or the order in which it was taken. The plot should not show any structure or pattern in the residuals.\nDependence on time is a common source of lack of independence, but other plots might also detect lack of independence.\nIdeally, we plot the residuals versus each variable of interest we could think of, either included or excluded in the model.\nAssessing the assumption of independence is hard in practice."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-3",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-3",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "Code\n# Residuals vs Time (Case) Plot\nplt.figure(figsize=(7, 5))\nsns.scatterplot(x = production_data['Case'], y = residuals)\nplt.axhline(y=0, color='red', linestyle='--')\nplt.title('Residuals vs Time (Case)')\nplt.xlabel('Case')\nplt.xticks(production_data['Case'])\nplt.ylabel('Residuals')\nplt.show()"
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-4",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-4",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "Examples of plots that do and do not support the independence assumption."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#checking-for-normality",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#checking-for-normality",
    "title": "Model Evaluation and Inference",
    "section": "Checking for normality",
    "text": "Checking for normality\n\nThis assumption is generally checked by looking at the distribution of the residuals.\nTwo plots:\n\nHistogram.\nNormal Quantile-Quantile Plot (also called normal probability plot)."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#histogram",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#histogram",
    "title": "Model Evaluation and Inference",
    "section": "Histogram",
    "text": "Histogram\nIdeally, the histogram resembles a normal distribution around 0. If the number of observations is small, the histogram may not be an effective visualization.\n\n\nCode\n# Histogram of residuals\nplt.figure(figsize=(5, 3))\nsns.histplot(residuals)\nplt.title('Histogram of Residuals')\nplt.xlabel('Residuals')\nplt.show()"
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#normal-quantile-quantile-qq-plot",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#normal-quantile-quantile-qq-plot",
    "title": "Model Evaluation and Inference",
    "section": "Normal Quantile-Quantile (QQ) Plot",
    "text": "Normal Quantile-Quantile (QQ) Plot\n\nA normal QQ plot is helpful for deciding whether a sample was drawn from a distribution that is approximately normal.\n\nFirst, let \\(\\hat{\\epsilon}_{[1]}, \\hat{\\epsilon}_{[2]}, \\ldots, \\hat{\\epsilon}_{[n]}\\) be the residuals ranked in an increasing order, where \\(\\hat{\\epsilon}_{[1]}\\) is the minimum and \\(\\hat{\\epsilon}_{[n]}\\) is the maximum. These points define the sample percentiles (or quantiles) of the distribution of the residuals.\n\n\nNext, calculate the theoretical percentiles of a (standard) Normal distribution calculated using Python."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-5",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-5",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "The normal QQ plot displays the (sample) percentiles of the residuals versus the percentiles of a normal distribution.\nIf these percentiles agree with each other, then they would approximate a straight line.\nThe straight line is usually determined visually, with emphasis on the central values rather than the extremes.\nFor a nice explanation, see this YouTube video."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#qq-plot-in-python",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#qq-plot-in-python",
    "title": "Model Evaluation and Inference",
    "section": "QQ plot in Python",
    "text": "QQ plot in Python\n\nTo construct a QQ plot, we use the function qqplot() statsmodels library.\n\n# QQ plot to assess normality of residuals\nplt.figure(figsize=(5, 3))\nsm.qqplot(residuals, fit = True, line = '45')\nplt.title('QQ Plot of Residuals')\nplt.show()"
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-6",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-6",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "Substantial departures from a straight line indicate that the distribution is not normal.\n\n\n&lt;Figure size 672x384 with 0 Axes&gt;"
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-7",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-7",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "This plot suggests that the residuals are consistent with a Normal curve.\n\n\n&lt;Figure size 672x384 with 0 Axes&gt;"
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#comments",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#comments",
    "title": "Model Evaluation and Inference",
    "section": "Comments",
    "text": "Comments\nThese data are truly Normally distributed. But note that we still see deviations. These are entirely due to chance.\n\nWhen \\(n\\) is relatively small, you tend to see deviations, particularly in the tails."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-8",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-8",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "Normal probability plots for data sets following various distributions. 100 observations in each data set."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#consequences-of-faulty-assumptions",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#consequences-of-faulty-assumptions",
    "title": "Model Evaluation and Inference",
    "section": "Consequences of faulty assumptions",
    "text": "Consequences of faulty assumptions\n\nIf the model structure is incorrect, the estimated coefficients \\(\\hat{\\beta}_j\\) will be biased and the predictions \\(\\hat{Y}_i\\) will be inaccurate.\n\nIf the residuals do not follow a normal distribution, we have two cases:\n\nIf sample size is large, we still get accurate p-values for the t-tests (discussed later) for the coefficients thanks to the Central Limit Theorem.\nHowever, the t-tests and all inference tools are invalidated."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-9",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-9",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "If the residuals do not have constant variance, then the linear model is incorrect and everything falls apart!\nIf the residuals are dependent, then the linear model is incorrect and everything falls apart!"
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#example-2",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#example-2",
    "title": "Model Evaluation and Inference",
    "section": "Example 2",
    "text": "Example 2\n\nLet’s consider the “auto.xlsx” dataset.\n\nauto_data = pd.read_excel('auto.xlsx')\n\n\nLet’s assume that we want to study the following model.\n\\[\nY_i = \\beta_0 + \\beta_1 X_{i1}+ \\beta_2 X_{i2}+ \\epsilon_i,\n\\]\nwhere \\(Y_i\\) is the mpg, \\(X_{i1}\\) is the weight, and \\(X_{i2}\\) is the acceleration of the \\(i\\)-th car, \\(i = 1, \\ldots, 392\\)."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#research-question",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#research-question",
    "title": "Model Evaluation and Inference",
    "section": "Research question",
    "text": "Research question\n\n\nDo the weight and acceleration have a significant association with the mpg of a car?"
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#the-two-cultures-of-statistical-models",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#the-two-cultures-of-statistical-models",
    "title": "Model Evaluation and Inference",
    "section": "The two cultures of statistical models",
    "text": "The two cultures of statistical models\n\n\n\nInference: develop a model that fits the data well. Then make inferences about the data-generating process based on the structure of such model.\nPrediction: Silent about the underlying mechanism generating the data and allow for many predictive algorithms, which only care about accuracy of predictions.\n\n\n\nThey overlap very often."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-10",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-10",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "The least squares estimators \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p\\) are subject to uncertainty, since they are calculated based on a random sample of data.\nTherefore, assessing the amount of the uncertainty in these estimators is important. To this end, we use hypothesis tests on individual coefficients."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#hypothesis-test",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#hypothesis-test",
    "title": "Model Evaluation and Inference",
    "section": "Hypothesis test",
    "text": "Hypothesis test\n\n\nA statistical hypothesis is a statement about the coefficients of a model.\n\n\nIn our case, we are interested in testing:\n\n\\(H_0: \\beta_j = 0\\) v.s. \\(H_1: \\beta_j \\neq 0\\) (Two-tailed Test)\n\n\n\nRejecting \\(H_0\\) (in favor of \\(H_1\\)) implies that the predictor has a significant association with the response.\nNot rejecting \\(H_0\\) implies that the predictor does not have a significant association with the response."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-11",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-11",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "\\(H_0: \\beta_j = 0\\) v.s. \\(H_1: \\beta_j \\neq 0\\) (Two-tailed Test)\n\n\nTesting this hypothesis consists of the following steps:\n\nTake a random sample.\nCompute the appropriate test statistic.\nReject or fail to reject the null hypothesis based on a computed p-value."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#step-1.-random-sample",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#step-1.-random-sample",
    "title": "Model Evaluation and Inference",
    "section": "Step 1. Random sample",
    "text": "Step 1. Random sample\n\nThe random sample is the data we use to train or fit the model.\n\n# Dataset with predictors.\npred_auto = auto_data[['weight', 'acceleration']]\n\n# Add the column for the intercept.\nauto_X_train = sm.add_constant(pred_auto)\n\n# Dataset with the response only.\nauto_Y_train = auto_data['mpg']"
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#step-2.-test-statistic",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#step-2.-test-statistic",
    "title": "Model Evaluation and Inference",
    "section": "Step 2. Test statistic",
    "text": "Step 2. Test statistic\n\nThe test statistic is\n\\[t_0 = \\frac{\\hat{\\beta}_{j}}{\\sqrt{ \\hat{v}_{jj}} }\\]\n\n\\(\\hat{\\beta}_j\\) is the least squares estimate of the true coefficient \\(\\beta_j\\).\n\\(\\hat{v}_{jj}\\) is the standard error of the estimate \\(\\hat{\\beta}_j\\) calculated using Python."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-12",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-12",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "Recall that we obtain the least squares estimates (\\(\\hat{\\beta}_{j}\\)) in Python using:\n\n# Fitting the simple linear regression model.\nregr = sm.OLS(auto_Y_train, auto_X_train)\nlinear_model = regr.fit()\nlinear_model.params\n\nconst           41.095329\nweight          -0.007293\nacceleration     0.261650\ndtype: float64\n\n\nLater, we will see how to obtain the standard error of the estimate \\(\\hat{\\beta}_j\\)."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-13",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-13",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "\\[t_0 = \\frac{\\hat{\\beta}_{j}}{\\sqrt{ \\hat{v}_{jj}} }\\]\n\n\nWe like this statistic because it follows a well-known distribution.\nIf the null hypothesis (\\(H_0: \\beta_j = 0\\)) is true, the statistic \\(t_0\\) follows a \\(t\\) distribution with \\(n-p-1\\) degrees of freedom\nRemember that \\(n\\) is the number of observations and \\(p\\) the number of predictors."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#t-distribution",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#t-distribution",
    "title": "Model Evaluation and Inference",
    "section": "t distribution",
    "text": "t distribution\n\nThis distribution is also known as the student’s t-distribution.\nIt was invented by William Gosset when he worked at the Guinness Brewery in Ireland.\nIt has one parameter \\(\\nu\\) which generally equals a number of degrees of freedom.\nThe parameter \\(\\nu\\) controls the shape of the distribution."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-14",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-14",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "The t-distribution resembles a standard normal distribution when \\(\\nu\\) goes to infinity."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#step-3.-calculate-the-p-value",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#step-3.-calculate-the-p-value",
    "title": "Model Evaluation and Inference",
    "section": "Step 3. Calculate the p-value",
    "text": "Step 3. Calculate the p-value\n\nThe p-value is the probability that the \\(t\\) test statistic will get a value that is at least as extreme as the observed \\(t_0\\) value when the null hypothesis (\\(H_0\\)) is true."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-15",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-15",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "For \\(H_0: \\beta_j = 0\\) v.s. \\(H_1: \\beta_j \\neq 0\\), the p-value is calculated using both tails of the \\(t\\) distribution. It is the blue area under the curve below.\n\nWe use the two tails because \\(H_1\\) includes the possibility that the value of \\(\\beta_j\\) is positive or negative."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#the-p-value-is-not-the-probability-that-h_0-is-true",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#the-p-value-is-not-the-probability-that-h_0-is-true",
    "title": "Model Evaluation and Inference",
    "section": "The p-value is not the probability that \\(H_0\\) is true",
    "text": "The p-value is not the probability that \\(H_0\\) is true\nSince the p-value is a probability, and since small p-values indicate that \\(H_0\\) is unlikely to be true, it is tempting to think that the p-value represents the probability that \\(H_0\\) is true.\n\n\n\nThis is not the case!\n\n\n\nRemember that the p-value is the probability that the \\(t\\) test statistic will get a value that is at least as extreme as the observed \\(t_0\\) value when \\(H_0\\) is true."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#how-small-must-the-p-value-be-to-reject-h_0",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#how-small-must-the-p-value-be-to-reject-h_0",
    "title": "Model Evaluation and Inference",
    "section": "How small must the p-value be to reject \\(H_0\\)?",
    "text": "How small must the p-value be to reject \\(H_0\\)?\n\n\n\n\nAnswer: Very small!\nBut, how small?\nAnswer: Very small!\nBut, tell me, how small?\nAnswer: Okay, it must be less than a value called \\(\\alpha\\) which is usually 0.1, 0.05, or 0.01.\nThank you!\n\n\n\n Sir Ronald Fisher"
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#decision",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#decision",
    "title": "Model Evaluation and Inference",
    "section": "Decision",
    "text": "Decision\n\nFor a significance level of \\(\\alpha = 0.05\\):\n\nIf the p-value is smaller than \\(\\alpha\\), we reject \\(H_0: \\beta_j = 0\\) in favor of \\(H_1: \\beta_j \\neq 0\\).\nIf the p-value is larger than \\(\\alpha\\), we do not reject \\(H_0: \\beta_j = 0\\).\n\nNo scientific basis for this advice. In practice, report the p-value and explore the data using descriptive statistics."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#t-tests-in-python",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#t-tests-in-python",
    "title": "Model Evaluation and Inference",
    "section": "t-tests in Python",
    "text": "t-tests in Python\nWe obtain the t-tests of the coefficients using the function summary() together with the linear_model object.\n\nmodel_summary = linear_model.summary()\nprint(model_summary)\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    mpg   R-squared:                       0.700\nModel:                            OLS   Adj. R-squared:                  0.698\nMethod:                 Least Squares   F-statistic:                     453.2\nDate:                Fri, 16 May 2025   Prob (F-statistic):          2.43e-102\nTime:                        10:50:05   Log-Likelihood:                -1125.4\nNo. Observations:                 392   AIC:                             2257.\nDf Residuals:                     389   BIC:                             2269.\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n================================================================================\n                   coef    std err          t      P&gt;|t|      [0.025      0.975]\n--------------------------------------------------------------------------------\nconst           41.0953      1.868     21.999      0.000      37.423      44.768\nweight          -0.0073      0.000    -25.966      0.000      -0.008      -0.007\nacceleration     0.2617      0.086      3.026      0.003       0.092       0.432\n==============================================================================\nOmnibus:                       31.397   Durbin-Watson:                   0.818\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               39.962\nSkew:                           0.632   Prob(JB):                     2.10e-09\nKurtosis:                       3.922   Cond. No.                     2.67e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.67e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#table-of-t-tests",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#table-of-t-tests",
    "title": "Model Evaluation and Inference",
    "section": "Table of t-tests",
    "text": "Table of t-tests\n\n\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;\\(|t|\\)\n\\([0.025\\)\n\\(0.975 ]\\)\n\n\n\n\nconst\n41.0953\n1.868\n21.999\n0.000\n37.423\n44.768\n\n\nweight\n-0.0073\n0.000\n-25.966\n0.000\n-0.008\n-0.007\n\n\nacceleration\n0.2617\n0.086\n3.026\n0.003\n0.092\n0.432\n\n\n\nThe column std err contains the values of the estimated standard error (\\(\\hat{v}_{jj}\\)) of the estimates \\(\\hat{\\beta}_j\\). They represent the variability in the estimates."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-17",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-17",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "coef\nstd err\nt\nP&gt;\\(|t|\\)\n\\([0.025\\)\n\\(0.975 ]\\)\n\n\n\n\nconst\n41.0953\n1.868\n21.999\n0.000\n37.423\n44.768\n\n\nweight\n-0.0073\n0.000\n-25.966\n0.000\n-0.008\n-0.007\n\n\nacceleration\n0.2617\n0.086\n3.026\n0.003\n0.092\n0.432\n\n\n\nThe column t contains the values of the observed statistic \\(t_0\\) for the hypothesis tests."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-18",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-18",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "coef\nstd err\nt\nP&gt;\\(|t|\\)\n\\([0.025\\)\n\\(0.975 ]\\)\n\n\n\n\nconst\n41.0953\n1.868\n21.999\n0.000\n37.423\n44.768\n\n\nweight\n-0.0073\n0.000\n-25.966\n0.000\n-0.008\n-0.007\n\n\nacceleration\n0.2617\n0.086\n3.026\n0.003\n0.092\n0.432\n\n\n\nThe column P&gt;|t| contains the p-values for the hypothesis tests.\nSince the p-value is smaller than \\(\\alpha = 0.05\\), we reject \\(H_0\\) for acceleration and weight. Therefore, weight and acceleration have a significant association with the mpg of a car."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-19",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-19",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "coef\nstd err\nt\nP&gt;\\(|t|\\)\n\\([0.025\\)\n\\(0.975 ]\\)\n\n\n\n\nconst\n41.0953\n1.868\n21.999\n0.000\n37.423\n44.768\n\n\nweight\n-0.0073\n0.000\n-25.966\n0.000\n-0.008\n-0.007\n\n\nacceleration\n0.2617\n0.086\n3.026\n0.003\n0.092\n0.432\n\n\n\nThe columns \\([0.025\\) and \\(0.975]\\) show the limits of a 95% confidence interval on each true coefficient \\(\\beta_j\\). This interval contains the true parameter value with a confidence of 95%."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#total-sum-of-squares",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#total-sum-of-squares",
    "title": "Model Evaluation and Inference",
    "section": "Total Sum of Squares",
    "text": "Total Sum of Squares\n\nLet’s consider the total sum of squares\n\\[SS_{Total} = \\sum_{i=1}^{n} (Y_i - \\bar{Y})^2 \\text{ where } \\bar{Y} = \\sum_{i=1}^{n} \\frac{Y_i}{n}.\\]\nThis quantity measures the total variation of the response.\nIn other words, it is the amount of variability inherent in the response before we fit a regression model."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#residual-sum-of-squares",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#residual-sum-of-squares",
    "title": "Model Evaluation and Inference",
    "section": "Residual Sum of Squares",
    "text": "Residual Sum of Squares\nLet’s also consider the residual sum of squares\n\\[RSS = \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2.\\]\nRSS is the sum of squares due to residuals of the linear regression model; or, residual variation left unexplained by this model.\nThe better the predictions of the model, the smaller the RSS value."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#coefficient-of-determination",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#coefficient-of-determination",
    "title": "Model Evaluation and Inference",
    "section": "Coefficient of determination",
    "text": "Coefficient of determination\n\\[R^2 = 1 - \\frac{RSS}{SS_{total}}\\]\n\\(R^2\\) measures the “proportion of variation in the response explained by the full regression model.”"
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-20",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-20",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "\\[R^2 = 1 - \\frac{RSS}{SS_{total}}\\]\nWhat would you conclude about RSS if \\(R^2 = 1\\)?\n\n\nIn this case, \\(RSS = 0\\) and the model fits the data perfectly.\nIf \\(R^2\\) is small, then large RSS: lots of scatter and the model’s fit is not good.\nIt turns out that \\(R^2\\) is the square of the correlation between the observed (\\(Y_i\\)) and predicted (\\(\\hat{Y}_i\\)) responses."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#remarks-on-r2",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#remarks-on-r2",
    "title": "Model Evaluation and Inference",
    "section": "Remarks on \\(R^2\\)",
    "text": "Remarks on \\(R^2\\)\n\n\nThe statistic \\(R^2\\) should be used with caution because it is always possible to make it unity by simply adding more and more predictors.\nIf \\(R^2\\) is large, it does not necessarily imply that the full model will provide accurate predictions of future observations."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#adjusted-r2-statistic",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#adjusted-r2-statistic",
    "title": "Model Evaluation and Inference",
    "section": "Adjusted \\(R^2\\) statistic",
    "text": "Adjusted \\(R^2\\) statistic\nAdjusted \\(R^2\\) is a better measure to decide whether to add a new variable into the model or not. It is:\n\\[R_{adj}^2=1-\\frac{RSS/(n-k-1)}{SS_{total}/(n-1)},\\]\nwhere \\(k\\) is the number of variables in the model. For the full model, \\(k = p\\).\n\n\nIf adjusted \\(R^2\\) goes down or stays the same, then new variable is not important.\nIf adjusted \\(R^2\\) goes up, then it is probably useful."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-23",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-23",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "Why is adjusted \\(R^2\\) more useful than \\(R^2\\) for adding new variables?\n\\[R_{adj}^2=1-\\frac{RSS/(n-k-1)}{SS_{total}/(n-1)}\\]\n\n\nAs we explain more variability, the numerator gets smaller and adjusted \\(R^2\\) gets closer to 1. So, we want to make the numerator small.\nIf we add a “good” variable to the model, RSS will go down. However, \\(n - k - 1\\) will decrease a little bit (because \\(k\\) is now bigger by 1.) So the numerator does not go down by as much as it does in ‘plain’ \\(R^2\\)."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-24",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#section-24",
    "title": "Model Evaluation and Inference",
    "section": "",
    "text": "\\[R_{adj}^2=1-\\frac{RSS/(n-k-1)}{SS_{total}/(n-1)}\\]\n\nIf we add a useless predictor, RSS goes down a tiny bit, but because we divide by \\(n - k - 1\\), the numerator might actually get bigger or change very little.\nSo, adjusted \\(R^2\\) is a better measure of whether adding a new predictor is an improvement. If adjusted \\(R^2\\) goes down or stays the same, the new predictor is irrelevant. If it goes up, then it is probably useful."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#in-python",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#in-python",
    "title": "Model Evaluation and Inference",
    "section": "In Python",
    "text": "In Python\n\nTo show the \\(R^2\\) value, we use the rsquared argument of the linear_model object.\n\nprint( round(linear_model.rsquared, 3) )\n\n0.7\n\n\nTo show the adjusted \\(R^2\\) value, we use the rsquared argument of the linear_model object.\n\nprint( round(linear_model.rsquared_adj, 3) )\n\n0.698"
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#model-building",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#model-building",
    "title": "Model Evaluation and Inference",
    "section": "Model building",
    "text": "Model building\n\nModel building refers to a set of techniques to reduce the full model to one that only includes significant predictors.\nNote that it is more a deconstruction than building an actual model. This is because the full model must be a valid one to begin with. Specifically, the full model must provide residuals that satisfy all the assumptions (1)-(4).\nAfter we have obtained a satisfactory full model, we can start the model building process."
  },
  {
    "objectID": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#two-model-building-techniques",
    "href": "ModelInferenceEvaluation/ModelInferenceEvaluation.slides.html#two-model-building-techniques",
    "title": "Model Evaluation and Inference",
    "section": "Two model building techniques",
    "text": "Two model building techniques\n\n\n\nAdjusted \\(R^2\\) statistic. We add one variable at a time and see the change in adjusted \\(R^2\\) statistic. If the value decreases, then we stop and evaluate the resulting model.\nT-tests on the individual coefficients. If \\(H_0\\) is not rejected, this indicates that the predictor \\(X_j\\) can be deleted from the model. We can then delete the predictor and refit the model. We can repeat this process several times until we reach a model in which all variables are significant."
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#agenda",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#agenda",
    "title": "Additional Topics",
    "section": "Agenda",
    "text": "Agenda\n\n\nLinear models with categorical variables\nLinear models with standardized numerical predictors\nRemedies for faulty assumptions\nPredicting future observations"
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#load-the-libraries",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#load-the-libraries",
    "title": "Additional Topics",
    "section": "Load the libraries",
    "text": "Load the libraries\n\nLet’s import scikit-learn into Python together with the other relevant libraries.\n\n# Importing necessary libraries\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nWe will not use all the functions from the scikit-learn library. Instead, we will use specific functions from the sub-libraries preprocessing, model_selection, and metrics."
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#categorical-predictors",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#categorical-predictors",
    "title": "Additional Topics",
    "section": "Categorical predictors",
    "text": "Categorical predictors\n\n\nA categorical predictor takes on values that are categories, say, names or labels.\nTheir use in regression requires dummy variables, which are quantitative variables.\nWhen a categorical predictor has more than two levels, a single dummy variable cannot represent all possible categories.\nIn general, a categorical predictor with \\(k\\) categories requires \\(k-1\\) dummy variables."
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#dummy-coding",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#dummy-coding",
    "title": "Additional Topics",
    "section": "Dummy coding",
    "text": "Dummy coding\n\nTraditionally, dummy variables are binary variables which can only take the values 0 and 1.\nThis approach implies a reference category. Specifically, the category that results when all dummy variables equal 0.\nThis coding impacts the interpretation of the model coefficients:\n\n\\(\\beta_0\\) is the mean response under the reference category.\n\\(\\beta_j\\) is the amount of increase in the mean response when we change from the reference category to another category."
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#example-1",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#example-1",
    "title": "Additional Topics",
    "section": "Example 1",
    "text": "Example 1\n\nThe auto data set includes a categorical variable “Origin” which shows the origin of each car.\n\n# Load the Excel file into a pandas DataFrame.\nauto_data = pd.read_excel(\"auto.xlsx\")\n\n# Set categorical variables.\nauto_data['origin'] = pd.Categorical(auto_data['origin'])"
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#dataset",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#dataset",
    "title": "Additional Topics",
    "section": "Dataset",
    "text": "Dataset\n\n# Show dataset.\nauto_data[['mpg', 'origin']].head(6)\n\n\n\n\n\n\n\n\nmpg\norigin\n\n\n\n\n0\n18.0\nAmerican\n\n\n1\n15.0\nAmerican\n\n\n2\n18.0\nAmerican\n\n\n3\n16.0\nAmerican\n\n\n4\n17.0\nAmerican\n\n\n5\n15.0\nAmerican"
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#dummy-variables",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#dummy-variables",
    "title": "Additional Topics",
    "section": "Dummy variables",
    "text": "Dummy variables\n\nOrigin has 3 categories: European, American, Japanese.\nSo, 2 dummy variables are required:\n\\[d_1 =\n\\begin{cases}\n1 \\text{ if car is European}\\\\\n0 \\text{ if car is not European}\n\\end{cases} \\text{ and }\\]\n\\[d_2 =\n\\begin{cases}\n1 \\text{ if car is Japanese}\\\\\n0 \\text{ if car is not Japanese}\n\\end{cases}.\\]\n“American” acts as the reference category."
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#section",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#section",
    "title": "Additional Topics",
    "section": "",
    "text": "The dataset with the dummy variables looks like this:\n\n\n\norigin\n\\(d_1\\)\n\\(d_2\\)\n\n\n\n\nAmerican\n0\n0\n\n\nAmerican\n0\n0\n\n\nEuropean\n1\n0\n\n\nEuropean\n1\n0\n\n\nAmerican\n0\n0\n\n\nJapanese\n0\n1\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)"
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#the-multiple-linear-regression-model",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#the-multiple-linear-regression-model",
    "title": "Additional Topics",
    "section": "The multiple linear regression model",
    "text": "The multiple linear regression model\n\n\\[Y_i= \\beta_0+\\beta_1 d_{1i} +\\beta_2 d_{2i} +\\epsilon_i \\ \\text{for} \\ i=1,\\ldots,n.\\]\n\n\\(Y_i\\) is the i-th response.\n\\(d_{1i}\\) is 1 if the i-th observation is from a European car, and 0 otherwise.\n\\(d_{2i}\\) is 1 if the i-th observation is from a Japanese car, and 0 otherwise."
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#model-coefficients",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#model-coefficients",
    "title": "Additional Topics",
    "section": "Model coefficients",
    "text": "Model coefficients\n\n\\[Y_i= \\beta_0+\\beta_1 d_{1i} +\\beta_2 d_{2i} +\\epsilon_i \\ \\text{for} \\ i=1,\\ldots,n.\\]\n\n\n\\(\\beta_0\\) is the mean response (mpg) for American cars.\n\\(\\beta_1\\) is the amount of increase in the mean response when changing from an American to a European car.\n\\(\\beta_2\\) is the amount of increase in the mean response when changing from an American to a Japanese car.\n\\(\\epsilon_i\\)’s follow the same assumptions as before."
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#section-1",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#section-1",
    "title": "Additional Topics",
    "section": "",
    "text": "Alternatively, we can write the regression model as:\n\\[Y_i= \\beta_0+\\beta_1 d_{1i} +\\beta_2 d_{2i} +\\epsilon_i = \\begin{cases}\n\\beta_0+\\beta_1 +\\epsilon_i \\text{ if car is European}\\\\\n\\beta_0+\\beta_2 +\\epsilon_i \\text{ if car is Japanese} \\\\\n\\beta_0 +\\epsilon_i\\;\\;\\;\\;\\;\\;\\ \\text{ if car is American}\n\\end{cases}\\]\nGiven this model representation:\n\n\\(\\beta_0\\) is the mean mpg for American cars,\n\\(\\beta_1\\) is the difference in the mean mpg between European and American cars, and\n\\(\\beta_2\\) is the difference in the mean mpg between Japanese and American cars."
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#in-python",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#in-python",
    "title": "Additional Topics",
    "section": "In Python",
    "text": "In Python\n\nWe follow three steps to fit a linear model with a categorical predictor. First, we compute the dummy variables.\n\n# Create linear regression object\ndummy_data = pd.get_dummies(auto_data['origin'], drop_first = True, \n                            dtype = 'int')\ndummy_data.head()\n\nNext, we construct the matrix of predictors with the intercept.\n\n# Create linear regression object\ndummy_X_train = sm.add_constant(dummy_data)\ndummy_X_train"
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#section-2",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#section-2",
    "title": "Additional Topics",
    "section": "",
    "text": "Finally, we fit the model using OLS() and fit() from statsmodels.\n\n# Create linear regression object\nregr = sm.OLS(auto_data['mpg'], dummy_X_train)\n\n# Train the model using the training sets\nlinear_model = regr.fit()\n\n# Summary of the models.\nsummary_model = linear_model.summary()"
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#section-3",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#section-3",
    "title": "Additional Topics",
    "section": "",
    "text": "print(summary_model)\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    mpg   R-squared:                       0.332\nModel:                            OLS   Adj. R-squared:                  0.328\nMethod:                 Least Squares   F-statistic:                     96.60\nDate:                Fri, 16 May 2025   Prob (F-statistic):           8.67e-35\nTime:                        10:50:10   Log-Likelihood:                -1282.2\nNo. Observations:                 392   AIC:                             2570.\nDf Residuals:                     389   BIC:                             2582.\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         20.0335      0.409     49.025      0.000      19.230      20.837\nEuropean       7.5695      0.877      8.634      0.000       5.846       9.293\nJapanese      10.4172      0.828     12.588      0.000       8.790      12.044\n==============================================================================\nOmnibus:                       26.330   Durbin-Watson:                   0.763\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               30.217\nSkew:                           0.679   Prob(JB):                     2.74e-07\nKurtosis:                       3.066   Cond. No.                         3.16\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#analysis-of-covariance",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#analysis-of-covariance",
    "title": "Additional Topics",
    "section": "Analysis of covariance",
    "text": "Analysis of covariance\nModels that mix categorical and numerical predictors are sometimes referred to as analysis of covariance (ANCOVA) models.\nExample (cont): Consider the predictor weight (\\(X\\)).\n\\[Y_i= \\beta_0+\\beta_1 d_{1i} +\\beta_2 d_{2i} + \\beta_3 X_{i} +\\epsilon_i,\\]\nwhere \\(X_i\\) denotes the i-th observed value of weight and \\(\\beta_3\\) is the coefficient of this predictor."
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#ancova-model",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#ancova-model",
    "title": "Additional Topics",
    "section": "ANCOVA model",
    "text": "ANCOVA model\nThe components of the ANCOVA model are individual functions of the coefficients.\nTo gain insight into the model, we write it as follows:\n\\[\n\\begin{align}\nY_i &= \\beta_0+\\beta_1 d_{1i} +\\beta_2 d_{2i} + \\beta_3 X_{i} +\\epsilon_i \\\\ &= \\begin{cases}\n(\\beta_0+\\beta_1)  + \\beta_3 X_{i} +\\epsilon_i \\text{ if car is European} \\\\\n(\\beta_0+\\beta_2) + \\beta_3 X_{i} +\\epsilon_i \\text{ if car is Japanese} \\\\\n\\beta_0 + \\beta_3 X_{i} +\\epsilon_i\\;\\;\\;\\;\\;\\;\\;\\;\\  \\text{ if car is American}\n\\end{cases}.\n\\end{align}\n\\]\nNote that these models have different intercepts but the same slope."
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#section-4",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#section-4",
    "title": "Additional Topics",
    "section": "",
    "text": "To estimate \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\) and \\(\\beta_3\\), we use least squares. To estimate \\(\\sigma^2\\), we use the mean squared error (MSE).\nWe could make individual inferences on \\(\\beta_1\\) and \\(\\beta_2\\) using t-tests and confidence intervals.\nHowever, better tests are possible such as overall and partial F-tests (not discussed here)."
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#in-python-1",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#in-python-1",
    "title": "Additional Topics",
    "section": "In Python",
    "text": "In Python\n\nTo fit an ANCOVA model, we use similar steps as before. The only extra step is to concatenate the data with the dummy variables and the numerical predictor using the function concat() from pandas.\n\n# Concatenate the two data sets.\nX_train = pd.concat([dummy_X_train, auto_data['weight']], axis = 1)\n\n# Create linear regression object\nregr = sm.OLS(auto_data['mpg'], X_train)\n\n# Train the model using the training sets\nancova_model = regr.fit()"
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#model-summary",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#model-summary",
    "title": "Additional Topics",
    "section": "Model summary",
    "text": "Model summary\n\n# Summary of the models.\nsummary_ancova = ancova_model.summary()\nprint(summary_ancova)\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    mpg   R-squared:                       0.702\nModel:                            OLS   Adj. R-squared:                  0.700\nMethod:                 Least Squares   F-statistic:                     304.7\nDate:                Fri, 16 May 2025   Prob (F-statistic):          1.28e-101\nTime:                        10:50:11   Log-Likelihood:                -1123.9\nNo. Observations:                 392   AIC:                             2256.\nDf Residuals:                     388   BIC:                             2272.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         43.7322      1.113     39.277      0.000      41.543      45.921\nEuropean       0.9709      0.659      1.474      0.141      -0.324       2.266\nJapanese       2.3271      0.665      3.501      0.001       1.020       3.634\nweight        -0.0070      0.000    -21.956      0.000      -0.008      -0.006\n==============================================================================\nOmnibus:                       40.731   Durbin-Watson:                   0.832\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               61.938\nSkew:                           0.689   Prob(JB):                     3.55e-14\nKurtosis:                       4.377   Cond. No.                     1.83e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.83e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems."
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#standardization",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#standardization",
    "title": "Additional Topics",
    "section": "Standardization",
    "text": "Standardization\n\n\n\nStandardization refers to centering and scaling each numeric predictor individually.\nTo center a predictor variable, the average predictor value is subtracted from all the values.\nTo scale a predictor, each of its value is divided by its standard deviation."
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#section-5",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#section-5",
    "title": "Additional Topics",
    "section": "",
    "text": "In mathematical terms, we standardize a predictor \\(X\\) as:\n\\[{\\color{blue} \\tilde{X}_{i}} = \\frac{{ X_{i} - \\bar{X}}}{ \\sqrt{\\frac{1}{n -1} \\sum_{i=1}^{n} (X_{i} - \\bar{X})^2 }} \\text{ with } \\bar{X} = \\sum_{i=1}^n \\frac{X_i}{n}.\\]\nThe average value of \\(\\tilde{X}\\) is 0.\nThe standard deviation of \\(\\tilde{X}\\) is 1."
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#benefits-and-limitations",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#benefits-and-limitations",
    "title": "Additional Topics",
    "section": "Benefits and limitations",
    "text": "Benefits and limitations\n\nBenefits:\n\nAll quantitative predictors are on the same scale.\nSize and importance of linear regression coefficients can be compared easily.\n\n\nLimitations:\n\nThe interpretation of the coefficients is affected."
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#interpretation",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#interpretation",
    "title": "Additional Topics",
    "section": "Interpretation",
    "text": "Interpretation\n\\[Y = \\beta_0 + \\beta_1 \\tilde{X}_1 + \\beta_2 \\tilde{X}_2 + \\cdots + \\beta_p \\tilde{X}_p + \\epsilon,\\]\nwhere \\(\\tilde{X}_i\\) is the standardized version of the predictor \\(X_i\\).\nInterpretation:\n\n\n\\(\\beta_0\\) is the mean response when all original predictors \\(X_1, X_2, \\ldots, X_p\\) are set to their average value.\n\\(\\beta_j\\) is the amount of increase in the mean response by an increase of 1 standard deviation in the original predictor \\(X_j,\\) when all other predictors are fixed to an arbitrary value."
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#example-2",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#example-2",
    "title": "Additional Topics",
    "section": "Example 2",
    "text": "Example 2\n\nThe yield of a chemical process (\\(Y\\)) is related to the concentration of the reactant (\\(X_1\\)) and the operating temperature (\\(X_2\\)).\n\nAn experiment was conducted to study the effect between these factors on the yield.\n\nThe dataset is in the file “catalyst.xlsx”."
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#section-6",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#section-6",
    "title": "Additional Topics",
    "section": "",
    "text": "The units of concentration and temperature are percentages and Farenheit degrees, respectively.\n\ncatalyst_data = pd.read_excel(\"catalyst.xlsx\")\nprint(catalyst_data)\n\n   Order  Yield  Concentration  Temperature\n0      1     90              2          180\n1      2     91              2          180\n2      3     84              2          150\n3      4     89              1          180\n4      5     83              2          150\n5      6     79              1          150\n6      7     81              1          150\n7      8     87              1          180"
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#standarization-in-python",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#standarization-in-python",
    "title": "Additional Topics",
    "section": "Standarization in Python",
    "text": "Standarization in Python\n\nRecall that we standardize numerical predictors using the scaler() function from scikit-learn.\n\n# Select predictor matrix.\npredictor_data = catalyst_data[['Concentration', 'Temperature']]\n\n# Define the scaling operator.\nscaler = StandardScaler()\n\n# Apply the scaling operator.\nXs_training = scaler.fit_transform(predictor_data)"
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#section-7",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#section-7",
    "title": "Additional Topics",
    "section": "",
    "text": "Now the predictors are on the same scale\n\n\nCode\nXs_training = pd.DataFrame(Xs_training, columns = predictor_data.columns)\nXs_training \n\n\n\n\n\n\n\n\n\nConcentration\nTemperature\n\n\n\n\n0\n1.0\n1.0\n\n\n1\n1.0\n1.0\n\n\n2\n1.0\n-1.0\n\n\n3\n-1.0\n1.0\n\n\n4\n1.0\n-1.0\n\n\n5\n-1.0\n-1.0\n\n\n6\n-1.0\n-1.0\n\n\n7\n-1.0\n1.0"
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#section-8",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#section-8",
    "title": "Additional Topics",
    "section": "",
    "text": "To fit the model, we follow the same functions as before.\n\n# Add the intercept.\nXs_training_int = sm.add_constant(Xs_training)\n\n# Create linear regression object.\nstd_regr = sm.OLS(catalyst_data['Yield'], Xs_training)\n\n# Train the model using the training sets.\nstd_linear_model = std_regr.fit()\n\n# Summary of the model.\nstd_summary_model = std_linear_model.summary()"
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#section-9",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#section-9",
    "title": "Additional Topics",
    "section": "",
    "text": "print(std_summary_model)\n\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:                  Yield   R-squared (uncentered):                   0.002\nModel:                            OLS   Adj. R-squared (uncentered):             -0.330\nMethod:                 Least Squares   F-statistic:                           0.006694\nDate:                Fri, 16 May 2025   Prob (F-statistic):                       0.993\nTime:                        10:50:11   Log-Likelihood:                         -46.940\nNo. Observations:                   8   AIC:                                      97.88\nDf Residuals:                       6   BIC:                                      98.04\nDf Model:                           2                                                  \nCovariance Type:            nonrobust                                                  \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nConcentration     1.5000     34.907      0.043      0.967     -83.914      86.914\nTemperature       3.7500     34.907      0.107      0.918     -81.664      89.164\n==============================================================================\nOmnibus:                        2.258   Durbin-Watson:                   0.000\nProb(Omnibus):                  0.323   Jarque-Bera (JB):                0.627\nSkew:                           0.000   Prob(JB):                        0.731\nKurtosis:                       1.628   Cond. No.                         1.00\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#discussion",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#discussion",
    "title": "Additional Topics",
    "section": "Discussion",
    "text": "Discussion\nStandardization of predictors has no impact on the overall quality of the linear regression model.\n\n\n\\(R^2\\) and adjusted \\(R^2\\) statistics are identical.\nPredictions are identical.\nResiduals do not change.\n\n\n\nStandardization does not affect the correlation between two predictors. So, it has not effect on collinearity.\n\n\nIdeally, the dummy variables for the categorical predictors are standardized too."
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#incorrect-model",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#incorrect-model",
    "title": "Additional Topics",
    "section": "Incorrect model",
    "text": "Incorrect model\n\nA model is incorrect if\n\n\nThe assumed model structure is incorrect. That is, \\(Y \\neq \\beta_0 + \\beta_1 X + \\epsilon\\).\nThe residuals do not have constant variance.\nThe residuals are not independent."
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#example-3",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#example-3",
    "title": "Additional Topics",
    "section": "Example 3",
    "text": "Example 3\nConsider the fitting the following model to the auto_data:\n\\[Y_i = \\beta_0 + \\beta_1 X_{i} + \\epsilon_i\\] where:\n\n\\(Y_i\\) is the mpg of the i-th car.\n\\(X_i\\) is the horsepower of the i-th car.\n\nWe fit the model:\n\nX_train = sm.add_constant(auto_data['horsepower'])\nregr = sm.OLS(auto_data['mpg'], X_train)\nlinear_model = regr.fit()"
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#residual-analysis",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#residual-analysis",
    "title": "Additional Topics",
    "section": "Residual analysis",
    "text": "Residual analysis"
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#remedies",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#remedies",
    "title": "Additional Topics",
    "section": "Remedies",
    "text": "Remedies\nThe assumed model structure is incorrect. That is, \\(Y \\neq \\beta_0 + \\beta_1 X + \\epsilon\\).\nRemedies: Add high powers of the predictor variable to the model or transform the response (or predictor).\n\nThe residuals of the fitted model do not have constant variance.\nRemedies: Transform the response or predictor variable.\n\nLogarithm transformation\nSquare root transformation"
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#transformations",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#transformations",
    "title": "Additional Topics",
    "section": "Transformations",
    "text": "Transformations\nTwo commonly used transformations are:\nNatural logarithm (ln) \\[\\ln(Y) = \\beta_0 + \\beta_1 X + \\epsilon\\] \\[\\ln(Y) = \\beta_0 + \\beta_1 \\ln(X) + \\epsilon\\]\nSquared root (sqrt)\n\\[\\sqrt{Y} = \\beta_0 + \\beta_1 X + \\epsilon\\] \\[\\sqrt{Y} = \\beta_0 + \\beta_1 \\sqrt{X} + \\epsilon\\]"
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#effect-of-transformations",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#effect-of-transformations",
    "title": "Additional Topics",
    "section": "Effect of transformations",
    "text": "Effect of transformations\n\n\n\nIn many cases, the \\(\\ln{(\\cdot)}\\) transformation Improves the relationship between predictor and response.\nProduces residuals that have constant variance (variance-stabilizing transformation).\nThe \\(\\sqrt{\\cdot}\\) transformation provides similar benefits, except that it is useful for response variables that are counts or follow a Poisson distribution."
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#numpy-library",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#numpy-library",
    "title": "Additional Topics",
    "section": "NumPy library",
    "text": "NumPy library\n\n\n\nnumpy is a powerful, open-source data manipulation and analysis library for Python\nIt is the backbone for scikit-learn and pandas\nhttps://numpy.org/\n\n\n\n\n\n\n\n\nLoad it using:\n\nimport numpy as np"
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#continuation-of-example-3",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#continuation-of-example-3",
    "title": "Additional Topics",
    "section": "Continuation of Example 3",
    "text": "Continuation of Example 3\nTo transform the response (\\(Y\\)) using \\(\\ln{(Y)}\\) or \\(\\sqrt{Y}\\) we use the functions log() and sqrt(), respectively, from numpy.\n\nsqrt_Y = np.sqrt( auto_data['mpg'] )\nlog_Y = np.log( auto_data['mpg'] )\n\nLet’s consider the logarithm transformation. The model then is:\n\\[\\log{(Y_i)} = \\beta_0 + \\beta_1 X_i +\\epsilon_i,\\]\nwhich we fit using the code below.\n\nlog_regr = sm.OLS(log_Y, X_train)\nlog_linear_model = log_regr.fit()"
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#new-residual-analysis",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#new-residual-analysis",
    "title": "Additional Topics",
    "section": "New residual analysis",
    "text": "New residual analysis"
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#quadratic-model",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#quadratic-model",
    "title": "Additional Topics",
    "section": "Quadratic model",
    "text": "Quadratic model\n\nAlthough there is an improvement in the Residuals vs Fitted Values plot when using the logarithm. The two plots suggests that we are missing a term in the model.\n\n\nIn fact, a better model for the data is a quadratic model with the logarithm of the response:\n\\[\\log{(Y_i)} = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 +\\epsilon_i.\\]"
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#section-10",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#section-10",
    "title": "Additional Topics",
    "section": "",
    "text": "To fit this model to the data, we construct a new predictor matrix.\n\nX_quad = pd.concat([X_train, auto_data['horsepower']**2], axis = 1)\n\nNext, we fit the model as before.\n\nquad_regr = sm.OLS(log_Y, X_quad)\nquadratic_model = quad_regr.fit()\n\nAnd calculate the residuals and predicted values.\n\nY_pred = quadratic_model.fittedvalues\nresiduals = quadratic_model.resid"
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#residual-analysis-of-quadratic-model",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#residual-analysis-of-quadratic-model",
    "title": "Additional Topics",
    "section": "Residual analysis of quadratic model",
    "text": "Residual analysis of quadratic model\n\n\n\n\nCode\n# Residual vs Fitted Values Plot\nplt.figure(figsize=(5, 5))\nsns.scatterplot(x = Y_pred, y = residuals)\nplt.axhline(y=0, color='red', linestyle='--')\nplt.title('Residuals vs Fitted Values')\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Residual vs Time Plot\nplt.figure(figsize=(5, 5))\norder = range(residuals.shape[0])\nsns.scatterplot(x = order, y = residuals)\nplt.axhline(y=0, color='red', linestyle='--')\nplt.title('Residuals vs Time')\nplt.xlabel('Time')\nplt.ylabel('Residuals')\nplt.show()"
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#conclusions",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#conclusions",
    "title": "Additional Topics",
    "section": "Conclusions",
    "text": "Conclusions\n\n\nTransformations may help us to develop models that better approximate the data. However, the interpretation of these models may be too complex. For instance, how can you interpret \\(\\beta_1\\) in \\(\\log{(Y)} = \\beta_0 + \\beta_1\\ X + \\epsilon\\)?\nTherefore, transformations are more useful to build good predictive models. That is, models whose goal is to give accurate predictions of future observations.\nNote that, we need to transform back our response predictions to the original scale. For example, if \\(Y' = \\ln{Y}\\) is the transformed response, then our final prediction is \\(Y^\\star = e^{{Y'}^\\star}\\)."
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#recall-that",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#recall-that",
    "title": "Additional Topics",
    "section": "Recall that …",
    "text": "Recall that …\nIn data science, we assume that\n\\[Y = f(\\boldsymbol{X}) + \\epsilon\\]\nwhere \\(f(\\boldsymbol{X})\\) represents the true relationship between \\(\\boldsymbol{X} = (X_1, X_2, \\ldots, X_p)\\) and \\(Y\\), which is unknown and very complex!\nIn our analysis, we further assume that\n\\[f(\\boldsymbol{X}) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p.\\]"
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#two-datasets",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#two-datasets",
    "title": "Additional Topics",
    "section": "Two datasets",
    "text": "Two datasets\n\nThe application of data science models needs two data sets:\n\n\nTraining data is data that we use to train or construct the estimated model \\(\\hat{f}(\\boldsymbol{X}) = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_1 + \\cdots + \\hat{\\beta}_p X_p\\).\nTest data is data that we use to evaluate the predictive performance of \\(\\hat{f}(\\boldsymbol{X})\\) only."
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#section-11",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#section-11",
    "title": "Additional Topics",
    "section": "",
    "text": "A random sample of \\(n\\) observations.\nUse it to construct \\(\\hat{f}(\\boldsymbol{X})\\).\n\n\n\n\n\nAnother random sample of \\(n_t\\) observations, which is independent of the training data.\nUse it to evaluate \\(\\hat{f}(\\boldsymbol{X})\\)."
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#validation-dataset",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#validation-dataset",
    "title": "Additional Topics",
    "section": "Validation Dataset",
    "text": "Validation Dataset\nIn many practical situations, a test dataset is not available. To overcome this issue, we use a validation dataset.\n\n\nIdea: Apply model to your validation dataset to mimic what will happen when you apply it to test dataset."
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#example-4",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#example-4",
    "title": "Additional Topics",
    "section": "Example 4",
    "text": "Example 4\n\nThe “BostonHousing.xlsx” contains data collected by the US Bureau of the Census concerning housing in the area of Boston, Massachusetts. The dataset includes data on 506 census housing tracts in the Boston area in 1970s.\nThe goal is to predict the median house price in new tracts based on information such as crime rate, pollution, and number of rooms.\nThe response is the median value of owner-occupied homes in $1000s, contained in the column MEDV."
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#the-predictors",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#the-predictors",
    "title": "Additional Topics",
    "section": "The predictors",
    "text": "The predictors\n\n\nCRIM: per capita crime rate by town.\nZN: proportion of residential land zoned for lots over 25,000 sq.ft.\nINDUS: proportion of non-retail business acres per town.\nCHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\nNOX: nitrogen oxides concentration (parts per 10 million).\nRM: average number of rooms per dwelling.\nAGE: proportion of owner-occupied units built prior to 1940.\nDIS: weighted mean of distances to five Boston employment centers\nRAD: index of accessibility to radial highways.\nTAX: full-value property-tax rate per $10,000.\nPTRATIO: pupil-teacher ratio by town.\nLSTAT: lower status of the population (percent)."
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#read-the-dataset",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#read-the-dataset",
    "title": "Additional Topics",
    "section": "Read the dataset",
    "text": "Read the dataset\nWe read the dataset and set the variable CHAS as categorical.\n\n# Load Excel file (make sure the file is in your Colab)\nBoston_data = pd.read_excel('BostonHousing.xlsx')\n\n# Drop the categorical variable.\nBoston_data['CHAS'] = pd.Categorical(Boston_data['CHAS'])\n\n# Preview the dataset.\nBoston_data.head(3)\n\n\n\n\n\n\n\n\nCRIM\nZN\nINDUS\nCHAS\nNOX\nRM\nAGE\nDIS\nRAD\nTAX\nPTRATIO\nLSTAT\nMEDV\n\n\n\n\n0\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296\n15.3\n4.98\n24.0\n\n\n1\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n9.14\n21.6\n\n\n2\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n4.03\n34.7"
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#how-do-we-generate-validation-data",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#how-do-we-generate-validation-data",
    "title": "Additional Topics",
    "section": "How do we generate validation data?",
    "text": "How do we generate validation data?\nWe split the current dataset into a training and a validation dataset. To this end, we use the function train_test_split() from scikit-learn.\n\n# Set full matrix of predictors.\nX_full = Boston_data.drop(columns = ['MEDV']) \n\n# Set full matrix of responses.\nY_full = Boston_data['MEDV']\n\n# Split the dataset into training and validation.\nX_train, X_valid, Y_train, Y_valid = train_test_split(X_full, Y_full, \n                                                      test_size=0.3)\n\nThe parameter test_size sets the portion of the dataset that will go to the validation set."
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#section-12",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#section-12",
    "title": "Additional Topics",
    "section": "",
    "text": "The function makes a clever partition of the data using the empirical distribution of the response.\nTechnically, it splits the data so that the distribution of the response under the training and validation sets is similar.\nUsually, the proportion of the dataset that goes to the validation set is 20% or 30%."
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#fit-a-model-using-training-data",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#fit-a-model-using-training-data",
    "title": "Additional Topics",
    "section": "Fit a model using training data",
    "text": "Fit a model using training data\n\nWe fit a multiple linear regression model to predict the MEDV in terms of the 12 predictors using the functions OLS() and fit() from statsmodels.\n\n# Add intercept.\nBoston_X_train = sm.add_constant(X_train)\n\n# Create linear regression object\nregr = sm.OLS(Y_train, Boston_X_train)\n\n# Train the model using the training sets\nlinear_model = regr.fit()"
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#brief-residual-analysis",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#brief-residual-analysis",
    "title": "Additional Topics",
    "section": "Brief Residual Analysis",
    "text": "Brief Residual Analysis\nWe evaluate the model using a “Residual versus Fitted Values” plot. The plot does not show concerning patterns in the residuals. So, we assume that the model satisfices the assumption of constant variance."
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#validation-mean-squared-error",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#validation-mean-squared-error",
    "title": "Additional Topics",
    "section": "Validation Mean Squared Error",
    "text": "Validation Mean Squared Error\n\nWhen the response is numeric, the most common evaluation metric is the validation Mean Squared Error (MSE):\n\\[\n\\frac{1}{n_{v}} \\sum_{i=1}^{n_{v}} \\left( Y_i - \\hat{f}(\\boldsymbol{X}_i) \\right)^2\n\\]\nwhere \\((Y_1, \\boldsymbol{X}_1), \\ldots, (Y_{n_{v} }, \\boldsymbol{X}_{n_{v}} )\\) are the \\(n_{v}\\) observations in the validation dataset, and \\(\\hat{f}(\\boldsymbol{X}_i)\\) is the model prediction of the i-th response."
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#section-13",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#section-13",
    "title": "Additional Topics",
    "section": "",
    "text": "Another useful metric is the validation Root Mean Squared Error (RMSE):\n\\[\n\\sqrt{\\frac{1}{n_{v}} \\sum_{i=1}^{n_{v}} \\left( Y_i - \\hat{f}(\\boldsymbol{X}_i) \\right)^2}\n\\]\nBenefits:\n\n\nThe RMSE is in the same units as the response.\nThe RMSE value is interpreted as either how far (on average) the residuals are from zero.\nIt can also be interpreted as the average distance between the observed response values and the model predictions."
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#in-python-2",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#in-python-2",
    "title": "Additional Topics",
    "section": "In Python",
    "text": "In Python\n\nWe first compute the predictions of our model on the validation dataset. That is, we use the values of the predictors in this dataset and use it as input to our model. Our model then computes the prediction of the response for each combination of values of the predictors.\n\n# Add constant to the predictor matrix from the validation set.\nBoston_X_val = sm.add_constant(X_valid)\n\n# Predict responses using validation data.\npredicted_medv_val = linear_model.predict(Boston_X_val)"
  },
  {
    "objectID": "AdditionalTopics/AdditionalTopics.slides.html#section-14",
    "href": "AdditionalTopics/AdditionalTopics.slides.html#section-14",
    "title": "Additional Topics",
    "section": "",
    "text": "We compute the validation RMSE by first computing the validation MSE using a function with the same name of scikit-learn.\n\nmse = mean_squared_error(Y_valid, predicted_medv_val)\nrmse = mse**(1/2)\nprint( round(rmse, 3) )\n\n5.13\n\n\nThe lower the validation RMSE, the more accurate our model.\nInterpretation: On average, our predictions are off by \\(\\pm\\) 4,465 dollars."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#agenda",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#agenda",
    "title": "Data preprocessing: Part I",
    "section": "Agenda",
    "text": "Agenda\n\n\nIntroduction\nTraining, validation, and test datasets\nDealing with missing values"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#data-preprocessing",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#data-preprocessing",
    "title": "Data preprocessing: Part I",
    "section": "Data preprocessing",
    "text": "Data preprocessing\nData pre-processing techniques generally refer to the addition, deletion, or transformation of data.\n\n\nIt can make or break a model’s predictive ability.\n\n\nFor example, linear regression models (to be discussed later) are relatively insensitive to the characteristics of the predictor data, but advanced methods like K-nearest neighbors, principal component regression, and LASSO are not."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#section",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#section",
    "title": "Data preprocessing: Part I",
    "section": "",
    "text": "We will review some common strategies for processing predictors from the data, without considering how they might be related to the response.\n\nIn particular, we will review:\n\n\nDealing with missing values.\nTransforming predictors.\nReducing the number of predictors.\nStandardizing the units of the predictors."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#scikit-learn-library",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#scikit-learn-library",
    "title": "Data preprocessing: Part I",
    "section": "scikit-learn library",
    "text": "scikit-learn library\n\nscikit-learn is a robust and popular library for machine learning in Python\nIt provides simple, efficient tools for data mining and data analysis\nIt is built on top of libraries such as NumPy, SciPy, and Matplotlib\nhttps://scikit-learn.org/stable/"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#section-1",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#section-1",
    "title": "Data preprocessing: Part I",
    "section": "",
    "text": "Let’s import scikit-learn into Python together with the other relevant libraries.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import power_transform, StandardScaler \n\nWe will not use all the functions from the scikit-learn library. Instead, we will use specific functions from the sub-libraries preprocessing, feature_selection, model_selection and impute."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#recall-that",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#recall-that",
    "title": "Data preprocessing: Part I",
    "section": "Recall that …",
    "text": "Recall that …\n\nIn data science, we assume that\n\\[Y = f(\\boldsymbol{X}) + \\epsilon\\]\nwhere \\(f(\\boldsymbol{X})\\) represents the true relationship between \\(\\boldsymbol{X} = (X_1, X_2, \\ldots, X_p)\\) and \\(Y\\).\n\n\\(f(\\boldsymbol{X})\\) is unknown and very complex!"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#two-datasets",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#two-datasets",
    "title": "Data preprocessing: Part I",
    "section": "Two datasets",
    "text": "Two datasets\n\nThe application of data science models needs two data sets:\n\n\nTraining data is data that we use to train or construct the estimated function \\(\\hat{f}(\\boldsymbol{X})\\).\nTest data is data that we use to evaluate the predictive performance of \\(\\hat{f}(\\boldsymbol{X})\\) only."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#section-2",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#section-2",
    "title": "Data preprocessing: Part I",
    "section": "",
    "text": "A random sample of \\(n\\) observations.\nUse it to construct \\(\\hat{f}(\\boldsymbol{X})\\).\n\n\n\n\n\nAnother random sample of \\(n_t\\) observations, which is independent of the training data.\nUse it to evaluate \\(\\hat{f}(\\boldsymbol{X})\\)."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#validation-dataset",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#validation-dataset",
    "title": "Data preprocessing: Part I",
    "section": "Validation Dataset",
    "text": "Validation Dataset\nIn many practical situations, a test dataset is not available. To overcome this issue, we use a validation dataset.\n\n\nIdea: Apply model to your validation dataset to mimic what will happen when you apply it to test dataset."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#example-1",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#example-1",
    "title": "Data preprocessing: Part I",
    "section": "Example 1",
    "text": "Example 1\n\nThe “BostonHousing.xlsx” contains data collected by the US Bureau of the Census concerning housing in the area of Boston, Massachusetts. The dataset includes data on 506 census housing tracts in the Boston area in 1970s.\nThe goal is to predict the median house price in new tracts based on information such as crime rate, pollution, and number of rooms.\nThe response is the median value of owner-occupied homes in $1000s, contained in the column MEDV."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#the-predictors",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#the-predictors",
    "title": "Data preprocessing: Part I",
    "section": "The predictors",
    "text": "The predictors\n\n\nCRIM: per capita crime rate by town.\nZN: proportion of residential land zoned for lots over 25,000 sq.ft.\nINDUS: proportion of non-retail business acres per town.\nCHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\nNOX: nitrogen oxides concentration (parts per 10 million).\nRM: average number of rooms per dwelling.\nAGE: proportion of owner-occupied units built prior to 1940.\nDIS: weighted mean of distances to five Boston employment centers\nRAD: index of accessibility to radial highways.\nTAX: full-value property-tax rate per $10,000.\nPTRATIO: pupil-teacher ratio by town.\nLSTAT: lower status of the population (percent)."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#read-the-dataset",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#read-the-dataset",
    "title": "Data preprocessing: Part I",
    "section": "Read the dataset",
    "text": "Read the dataset\nWe read the dataset and set the variable CHAS as categorical.\n\nBoston_data = pd.read_excel('BostonHousing.xlsx')\n\nBoston_data['CHAS'] = pd.Categorical(Boston_data['CHAS'])\n\nBoston_data.head(4)\n\n\n\n\n\n\n\n\nCRIM\nZN\nINDUS\nCHAS\nNOX\nRM\nAGE\nDIS\nRAD\nTAX\nPTRATIO\nLSTAT\nMEDV\n\n\n\n\n0\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296\n15.3\n4.98\n24.0\n\n\n1\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n9.14\n21.6\n\n\n2\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n4.03\n34.7\n\n\n3\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n2.94\n33.4"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#how-do-we-generate-validation-data",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#how-do-we-generate-validation-data",
    "title": "Data preprocessing: Part I",
    "section": "How do we generate validation data?",
    "text": "How do we generate validation data?\n\nWe split the current dataset into a training and a validation dataset. To this end, we use the function train_test_split() from scikit-learn.\nThe function has three main inputs:\n\nA pandas data frame with the predictor columns only.\nA pandas data frame with the response column only.\nThe parameter test_size which sets the portion of the dataset that will go to the validation set."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#create-the-predictor-matrix",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#create-the-predictor-matrix",
    "title": "Data preprocessing: Part I",
    "section": "Create the predictor matrix",
    "text": "Create the predictor matrix\nWe use the function .drop() from pandas. This function drops one or more columns from a data frame. Let’s drop the response column MEDV and store the result in X_full.\n\n# Set full matrix of predictors.\nX_full = Boston_data.drop(columns = ['MEDV']) \nX_full.head(4)\n\n\n\n\n\n\n\n\nCRIM\nZN\nINDUS\nCHAS\nNOX\nRM\nAGE\nDIS\nRAD\nTAX\nPTRATIO\nLSTAT\n\n\n\n\n0\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296\n15.3\n4.98\n\n\n1\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n9.14\n\n\n2\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n4.03\n\n\n3\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n2.94"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#create-the-response-column",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#create-the-response-column",
    "title": "Data preprocessing: Part I",
    "section": "Create the response column",
    "text": "Create the response column\nWe use the function .filter() from pandas to extract the column MEDV from the data frame. We store the result in Y_full.\n\n# Set full matrix of responses.\nY_full = Boston_data.filter(['MEDV'])\nY_full.head(4)\n\n\n\n\n\n\n\n\nMEDV\n\n\n\n\n0\n24.0\n\n\n1\n21.6\n\n\n2\n34.7\n\n\n3\n33.4"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#lets-partition-the-dataset",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#lets-partition-the-dataset",
    "title": "Data preprocessing: Part I",
    "section": "Let’s partition the dataset",
    "text": "Let’s partition the dataset\n\n\n# Split the dataset into training and validation.\nX_train, X_valid, Y_train, Y_valid = train_test_split(X_full, Y_full, \n                                                      test_size = 0.3)\n\n\nThe function makes a clever partition of the data using the empirical distribution of the response.\nTechnically, it splits the data so that the distribution of the response under the training and validation sets is similar.\nUsually, the proportion of the dataset that goes to the validation set is 20% or 30%."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#section-3",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#section-3",
    "title": "Data preprocessing: Part I",
    "section": "",
    "text": "The predictors and response in the training dataset are in the objects X_train and Y_train, respectively. We compile these objects into a single dataset using the function .concat() from pandas. The argument axis = 1 tells .concat() to concatenate the datasets by their rows.\n\ntraining_dataset = pd.concat([X_train, Y_train], axis = 1)\ntraining_dataset.head(4)\n\n\n\n\n\n\n\n\nCRIM\nZN\nINDUS\nCHAS\nNOX\nRM\nAGE\nDIS\nRAD\nTAX\nPTRATIO\nLSTAT\nMEDV\n\n\n\n\n63\n0.12650\n25.0\n5.13\n0\n0.453\n6.762\n43.4\n7.9809\n8\n284\n19.7\n9.50\n25.0\n\n\n307\n0.04932\n33.0\n2.18\n0\n0.472\n6.849\n70.3\n3.1827\n7\n222\n18.4\n7.53\n28.2\n\n\n365\n4.55587\n0.0\n18.10\n0\n0.718\n3.561\n87.9\n1.6132\n24\n666\n20.2\n7.12\n27.5\n\n\n73\n0.19539\n0.0\n10.81\n0\n0.413\n6.245\n6.2\n5.2873\n4\n305\n19.2\n7.54\n23.4"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#section-4",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#section-4",
    "title": "Data preprocessing: Part I",
    "section": "",
    "text": "Equivalently, the predictors and response in the validation dataset are in the objects X_valid and Y_valid, respectively.\n\nvalidation_dataset = pd.concat([X_valid, Y_valid], axis = 1)\nvalidation_dataset.head(4)\n\n\n\n\n\n\n\n\nCRIM\nZN\nINDUS\nCHAS\nNOX\nRM\nAGE\nDIS\nRAD\nTAX\nPTRATIO\nLSTAT\nMEDV\n\n\n\n\n135\n0.55778\n0.0\n21.89\n0\n0.624\n6.335\n98.2\n2.1107\n4\n437\n21.2\n16.96\n18.1\n\n\n230\n0.53700\n0.0\n6.20\n0\n0.504\n5.981\n68.1\n3.6715\n8\n307\n17.4\n11.65\n24.3\n\n\n28\n0.77299\n0.0\n8.14\n0\n0.538\n6.495\n94.4\n4.4547\n4\n307\n21.0\n12.80\n18.4\n\n\n347\n0.01870\n85.0\n4.15\n0\n0.429\n6.516\n27.7\n8.5353\n4\n351\n17.9\n6.36\n23.1"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#work-on-your-training-dataset",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#work-on-your-training-dataset",
    "title": "Data preprocessing: Part I",
    "section": "Work on your training dataset",
    "text": "Work on your training dataset\nAfter we have partitioned the data, we work on the training data to develop our predictive pipeline.\nThe pipeline has two main steps:\n\nData preprocessing.\nModel development.\n\nWe will now discuss preprocessing techniques applied to the predictor columns in the training dataset.\nNote that all preprocessing techniques will also be applied to the validation dataset and test dataset to prepare it for your model!"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#missing-values",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#missing-values",
    "title": "Data preprocessing: Part I",
    "section": "Missing values",
    "text": "Missing values\nIn many cases, some predictors have no values for a given observation. It is important to understand why the values are missing.\n\n\nThere four main types of missing data:\n\n\nStructurally missing data is data that is missing for a logical reason or because it should not exist.\nMissing completely at random assumes that the fact that the data is missing is unrelated to the other information in the data."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#section-5",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#section-5",
    "title": "Data preprocessing: Part I",
    "section": "",
    "text": "Missing at random assumes that we can predict the value that is missing based on the other available data.\nMissing not at random assumes that there is a mechanism that generates the missing values, which may include observed and unobserved predictors."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#section-6",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#section-6",
    "title": "Data preprocessing: Part I",
    "section": "",
    "text": "For large data sets, removal of observations based on missing values is not a problem, assuming that the type of missing data is completely at random.\n\nIn a smaller data sets, there is a high price in removing observations. To overcome this issue, we can use methods of imputation, which try to estimate the missing values of a predictor variable using the other predictors’ values.\n\nHere, we will introduce some simple methods for imputing missing values in categorical and numerical variables."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#example-2",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#example-2",
    "title": "Data preprocessing: Part I",
    "section": "Example 2",
    "text": "Example 2\n\nLet’s use the penguins dataset available in the file “penguins.xlsx”.\n\n# Load the Excel file into a pandas DataFrame.\npenguins_data = pd.read_excel(\"penguins.xlsx\")\n\n# Set categorical variables.\npenguins_data['sex'] = pd.Categorical(penguins_data['sex'])\npenguins_data['species'] = pd.Categorical(penguins_data['species'])\npenguins_data['island'] = pd.Categorical(penguins_data['island'])"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#training-and-validation-datasets",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#training-and-validation-datasets",
    "title": "Data preprocessing: Part I",
    "section": "Training and validation datasets",
    "text": "Training and validation datasets\nFor illustrative purposes, we assume that we want to predict the species (in the column species) of a penguin using the predictors bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, and sex.\n\nWe create the predictor matrix and response column.\n\n# Set full matrix of predictors.\nX_full_p = (penguins_data\n           .filter(['bill_length_mm', 'bill_depth_mm', \n           'flipper_length_mm', 'body_mass_g', 'sex']))\n\n# Set full matrix of responses.\nY_full_p = penguins_data.filter(['species'])"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#section-7",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#section-7",
    "title": "Data preprocessing: Part I",
    "section": "",
    "text": "We will use a validation dataset with 20% of the observations in penguins_data. The other 80% will be in the training dataset.\n\n# Split the dataset into training and validation.\nX_train_p, X_valid_p, Y_train_p, Y_valid_p = train_test_split(X_full_p, Y_full_p, \n                                                      test_size = 0.3,\n                                                      random_state = 59227)\n\nIn train_test_split, we use the input random_state to set a random seed. Essentially, this allows you to obtain the same training and validation datasets every time you run the code. The advice is setting random_state to a big integer number generated at random."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#section-8",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#section-8",
    "title": "Data preprocessing: Part I",
    "section": "",
    "text": "Since preprocessing techniques are meant for the predictors, we will work on the X_train_p data frame.\n\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n31\n37.2\n18.1\n178.0\n3900.0\nmale\n\n\n46\n41.1\n19.0\n182.0\n3425.0\nmale\n\n\n195\n49.6\n15.0\n216.0\n4750.0\nmale\n\n\n43\n44.1\n19.7\n196.0\n4400.0\nmale\n\n\n196\n50.5\n15.9\n222.0\n5550.0\nmale"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#section-9",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#section-9",
    "title": "Data preprocessing: Part I",
    "section": "",
    "text": "Let’s check if the dataset has have missing observations is using the function .info() from pandas.\n\nX_train_p.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 240 entries, 31 to 293\nData columns (total 5 columns):\n #   Column             Non-Null Count  Dtype   \n---  ------             --------------  -----   \n 0   bill_length_mm     238 non-null    float64 \n 1   bill_depth_mm      238 non-null    float64 \n 2   flipper_length_mm  238 non-null    float64 \n 3   body_mass_g        238 non-null    float64 \n 4   sex                232 non-null    category\ndtypes: category(1), float64(4)\nmemory usage: 9.7 KB"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#section-10",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#section-10",
    "title": "Data preprocessing: Part I",
    "section": "",
    "text": "In the output of the function, “non-null” refers to the number of entries in a column that have actual values. That is, the number of entries where there are not NaN.\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 240 entries, 31 to 293\nData columns (total 5 columns):\n #   Column             Non-Null Count  Dtype   \n---  ------             --------------  -----   \n 0   bill_length_mm     238 non-null    float64 \n 1   bill_depth_mm      238 non-null    float64 \n 2   flipper_length_mm  238 non-null    float64 \n 3   body_mass_g        238 non-null    float64 \n 4   sex                232 non-null    category\ndtypes: category(1), float64(4)\nmemory usage: 9.7 KB\n\n\nThe value next to “RangeIndex” is the number of observations in the dataset."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#section-11",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#section-11",
    "title": "Data preprocessing: Part I",
    "section": "",
    "text": "The output below shows that there are 240 entries but bill_length_mm has 238 that are “non-null”. Therefore, this column has two missing observations.\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 240 entries, 31 to 293\nData columns (total 5 columns):\n #   Column             Non-Null Count  Dtype   \n---  ------             --------------  -----   \n 0   bill_length_mm     238 non-null    float64 \n 1   bill_depth_mm      238 non-null    float64 \n 2   flipper_length_mm  238 non-null    float64 \n 3   body_mass_g        238 non-null    float64 \n 4   sex                232 non-null    category\ndtypes: category(1), float64(4)\nmemory usage: 9.7 KB\n\n\nThe same is true for the other predictors."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#section-12",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#section-12",
    "title": "Data preprocessing: Part I",
    "section": "",
    "text": "Alternatively, we can use the function .isnull() together with sum() to determine the number of missing values for each column in the dataset.\n\n\nmissing_values = X_train_p.isnull().sum()\nmissing_values\n\nbill_length_mm       2\nbill_depth_mm        2\nflipper_length_mm    2\nbody_mass_g          2\nsex                  8\ndtype: int64\n\n\n\nWe see that each predictor column has two missing values."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#removing-missing-values",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#removing-missing-values",
    "title": "Data preprocessing: Part I",
    "section": "Removing missing values",
    "text": "Removing missing values\nIf we want to remove all rows in the dataset that have at least one missing value, we use the function .dropna().\n\ncomplete_predictors = X_train_p.dropna()\ncomplete_predictors.head()\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n31\n37.2\n18.1\n178.0\n3900.0\nmale\n\n\n46\n41.1\n19.0\n182.0\n3425.0\nmale\n\n\n195\n49.6\n15.0\n216.0\n4750.0\nmale\n\n\n43\n44.1\n19.7\n196.0\n4400.0\nmale\n\n\n196\n50.5\n15.9\n222.0\n5550.0\nmale"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#section-13",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#section-13",
    "title": "Data preprocessing: Part I",
    "section": "",
    "text": "complete_predictors.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 232 entries, 31 to 293\nData columns (total 5 columns):\n #   Column             Non-Null Count  Dtype   \n---  ------             --------------  -----   \n 0   bill_length_mm     232 non-null    float64 \n 1   bill_depth_mm      232 non-null    float64 \n 2   flipper_length_mm  232 non-null    float64 \n 3   body_mass_g        232 non-null    float64 \n 4   sex                232 non-null    category\ndtypes: category(1), float64(4)\nmemory usage: 9.4 KB\n\n\nThe new data is complete because each column has 232 “non-null” values; the total number of observations in complete_predictors.\nHowever, note that we have lost eight of the original observations in X_train_p!"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#imputation-using-the-mean",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#imputation-using-the-mean",
    "title": "Data preprocessing: Part I",
    "section": "Imputation using the mean",
    "text": "Imputation using the mean\nWe can impute the missing values of a numeric variable using the mean or median of its available values. For example, consider the variable bill_length_mm that has two missing values.\n\nX_train_p['bill_length_mm'].info()\n\n&lt;class 'pandas.core.series.Series'&gt;\nIndex: 240 entries, 31 to 293\nSeries name: bill_length_mm\nNon-Null Count  Dtype  \n--------------  -----  \n238 non-null    float64\ndtypes: float64(1)\nmemory usage: 3.8 KB\n\n\nIn scikit-learn, we use the function SimpleImputer() to define the method of imputation of missing values."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#section-14",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#section-14",
    "title": "Data preprocessing: Part I",
    "section": "",
    "text": "Using SimpleImputer(), we set the method to imput missing values using the mean. We also use the function fit_transform() to apply the imputation method to the variable.\n\n# Imputation for numerical variables (using the mean)\nnum_imputer = SimpleImputer(strategy='mean')\n\n# Replace the original variable with new version.\nX_train_p['bill_length_mm'] = num_imputer.fit_transform(X_train_p[ ['bill_length_mm'] ] )"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#section-15",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#section-15",
    "title": "Data preprocessing: Part I",
    "section": "",
    "text": "After imputation, the information of the predictors in the dataset looks like this.\n\nX_train_p.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 240 entries, 31 to 293\nData columns (total 5 columns):\n #   Column             Non-Null Count  Dtype   \n---  ------             --------------  -----   \n 0   bill_length_mm     240 non-null    float64 \n 1   bill_depth_mm      238 non-null    float64 \n 2   flipper_length_mm  238 non-null    float64 \n 3   body_mass_g        238 non-null    float64 \n 4   sex                232 non-null    category\ndtypes: category(1), float64(4)\nmemory usage: 9.7 KB\n\n\nNow, bill_length_mm has 240 complete values."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#section-16",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#section-16",
    "title": "Data preprocessing: Part I",
    "section": "",
    "text": "To imput the missing values using the median, we simply set this method in SimpleImputer(). For example, let’s imput the missing values of bill_depth_mm.\n\n# Imputation for numerical variables (using the mean)\nnum_imputer = SimpleImputer(strategy = 'median')\n\n# Replace the original variable with new version.\nX_train_p['bill_depth_mm'] = num_imputer.fit_transform(X_train_p[ ['bill_depth_mm'] ] )\n\n# Show the information of the predictor.\nX_train_p['bill_depth_mm'].info()\n\n&lt;class 'pandas.core.series.Series'&gt;\nIndex: 240 entries, 31 to 293\nSeries name: bill_depth_mm\nNon-Null Count  Dtype  \n--------------  -----  \n240 non-null    float64\ndtypes: float64(1)\nmemory usage: 3.8 KB"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#using-the-mean-or-the-median",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#using-the-mean-or-the-median",
    "title": "Data preprocessing: Part I",
    "section": "Using the mean or the median?",
    "text": "Using the mean or the median?\n\nWe use the sample mean when the data distribution is roughly symmetrical.\n\nPros: Simple and easy to implement.\nCons: Sensitive to outliers; may not be accurate for skewed distributions"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#section-17",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#section-17",
    "title": "Data preprocessing: Part I",
    "section": "",
    "text": "We use the sample median when the data is skewed (e.g., incomes, prices).\n\nPros: Less sensitive to outliers; robust for skewed distributions.\nCons: May reduce variability in the data."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#imputation-method-for-a-categorical-variable",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#imputation-method-for-a-categorical-variable",
    "title": "Data preprocessing: Part I",
    "section": "Imputation method for a categorical variable",
    "text": "Imputation method for a categorical variable\nIf a categorical variable has missing values, we can use the most frequent of the available values to replace the missing values. To this end, we use similar commands as before.\nFor example, let’s impute the missing values of sex using this strategy.\n\n# Imputation for categorical variables (using the most frequent value)\ncat_imputer = SimpleImputer(strategy = 'most_frequent')\n\n# Apply imputation strategy for categorical variables.\nX_train_p['sex'] = cat_imputer.fit_transform(X_train_p[ ['sex'] ]).ravel()"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#section-18",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#section-18",
    "title": "Data preprocessing: Part I",
    "section": "",
    "text": "Let’s now have a look at the information of the dataset.\n\n# Apply imputation strategy for categorical variables.\nX_train_p.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 240 entries, 31 to 293\nData columns (total 5 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   bill_length_mm     240 non-null    float64\n 1   bill_depth_mm      240 non-null    float64\n 2   flipper_length_mm  238 non-null    float64\n 3   body_mass_g        238 non-null    float64\n 4   sex                240 non-null    object \ndtypes: float64(4), object(1)\nmemory usage: 11.2+ KB\n\n\nThe columns bill_length_mm, bill_depth_mm, and sex have 240 complete values."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#multivariate-imputation-using-k-nearest-neighbours",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#multivariate-imputation-using-k-nearest-neighbours",
    "title": "Data preprocessing: Part I",
    "section": "Multivariate imputation using K-nearest neighbours",
    "text": "Multivariate imputation using K-nearest neighbours\nK-nearest neighbours (KNN) is a multiple imputation method to fill in the missing values of a predictor using the available values from this and the other predictors.\n\nFor each missing value in a predictor \\(X\\):\n\nFind the \\(K\\) most similar rows based on the other predictors.\nImpute the missing value of \\(X\\) using the average of the available values of \\(X\\) in the \\(K\\) closest rows."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#section-19",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#section-19",
    "title": "Data preprocessing: Part I",
    "section": "",
    "text": "Let’s look at the new dataset.\n\nX_train_num_p.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 240 entries, 0 to 239\nData columns (total 4 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   bill_length_mm     240 non-null    float64\n 1   bill_depth_mm      240 non-null    float64\n 2   flipper_length_mm  240 non-null    float64\n 3   body_mass_g        240 non-null    float64\ndtypes: float64(4)\nmemory usage: 7.6 KB\n\n\nAll predictors have 240 complete entries."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#in-python",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#in-python",
    "title": "Data preprocessing: Part I",
    "section": "In Python",
    "text": "In Python\nYou can use KNN using the function KNNImputer(). Its main input is the number of nearest neighbours to use. You set this value using the parameter n_neighbors.\n\n# Set the KNN imputer using two neighbours.\nKNN_imputer = KNNImputer(n_neighbors = 2)\n\n# Select numeric predictors.\nX_train_num_p = X_train_p.drop(columns = ['sex']) \n\n# Apply imputer and store the rulst in a pandas data frame.\nX_train_num_p = pd.DataFrame(KNN_imputer.fit_transform(X_train_num_p), \n                            columns = X_train_num_p.columns)"
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#section-20",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#section-20",
    "title": "Data preprocessing: Part I",
    "section": "",
    "text": "Let’s look at the new dataset.\n\nX_train_num_p.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 240 entries, 0 to 239\nData columns (total 4 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   bill_length_mm     240 non-null    float64\n 1   bill_depth_mm      240 non-null    float64\n 2   flipper_length_mm  240 non-null    float64\n 3   body_mass_g        240 non-null    float64\ndtypes: float64(4)\nmemory usage: 7.6 KB\n\n\nAll predictors have 240 complete entries."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#some-remarks",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#some-remarks",
    "title": "Data preprocessing: Part I",
    "section": "Some remarks",
    "text": "Some remarks\n\n\nIn KNN the distance between two rows is calculated using Euclidean distance.\nKNN only considers rows without missing values in the columns being used.\nKNN works best when the features are on similar scales and missingness is not too high."
  },
  {
    "objectID": "PreProcessing/PreProcessing_Part1.slides.html#notes-on-its-implementation",
    "href": "PreProcessing/PreProcessing_Part1.slides.html#notes-on-its-implementation",
    "title": "Data preprocessing: Part I",
    "section": "Notes on its implementation",
    "text": "Notes on its implementation\n\n\nKNN captures local patterns better than individual predictor imputation using the mean or the median.\nHowever, it only works for numeric predictors!\nThe distance between two rows is calculated using Euclidean distance.\nKNN only considers rows without missing values in the predictor columns being used."
  }
]