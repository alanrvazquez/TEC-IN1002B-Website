{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Model Evaluation and Inference\"\n",
        "subtitle: \"IN1002B: Introduction to Data Science Projects\"\n",
        "author: \n",
        "  - name: Alan R. Vazquez\n",
        "    affiliations:\n",
        "      - name: Department of Industrial Engineering\n",
        "format: \n",
        "  revealjs:\n",
        "    chalkboard: false\n",
        "    multiplex: true\n",
        "    footer: \"Tecnologico de Monterrey\"\n",
        "    logo: IN1002b_logo.png\n",
        "    css: style.css\n",
        "    slide-number: True\n",
        "    html-math-method: mathjax\n",
        "editor: visual\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "\n",
        "## Agenda\n",
        "\n",
        "</br>\n",
        "\n",
        "1.  Residual analysis\n",
        "2.  Inference about individual $\\beta$'s using t-tests\n",
        "3.  Multiple and adjusted $R^2$ statistics\n",
        "\n",
        "## Load the libraries\n",
        "\n",
        "</br></br>\n",
        "\n",
        "Let's import **statsmodels** into Python together with the other relevant libraries.\n"
      ],
      "id": "c76d941f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: false\n",
        "\n",
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm"
      ],
      "id": "99fe98cf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Residual analysis\n",
        "\n",
        "## Multiple linear regression model\n",
        "\n",
        "</br>\n",
        "\n",
        "$$Y_i = \\beta_0 + \\beta_1 X_{i1} + \\cdots + \\beta_p X_{ip} + \\epsilon_i$$\n",
        "\n",
        "-   $p$ is the number of predictors.\n",
        "-   $n$ is the number of observations.\n",
        "-   $X_{ij}$ is the i-th observation of the j-th predictor.\n",
        "-   $Y_{i}$ is the i-th observation of the response.\n",
        "-   $\\epsilon_i$ is the i-th random error.\n",
        "\n",
        "## Assumptions\n",
        "\n",
        "</br>\n",
        "\n",
        "The error $\\epsilon_i$’s must then satisfy the following assumptions:\n",
        "\n",
        "::: incremental\n",
        "1.  On average, they are close to zero for any value of the predictors $X_j$.\n",
        "2.  For any value of the predictor $X_j$, the dispersion or variance is constant and equal to $\\sigma^2$.\n",
        "3.  The $\\epsilon_i$’s are all independent from each other.\n",
        "4.  [The $\\epsilon_i$’s follow normal distribution with mean 0 and variance $\\sigma^2$.]{style=\"color:gray;\"}\n",
        ":::\n",
        "\n",
        "## Residuals\n",
        "\n",
        "</br>\n",
        "\n",
        "The errors $\\epsilon_1, \\ldots, \\epsilon_n$ are not observed. To overcome this issue, we use the residuals of our model.\n",
        "\n",
        "Suppose that the multiple linear regression model is correct and consider the fitted responses $\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_{i1} + \\cdots + \\hat{\\beta}_p X_{ip}$, where $\\hat{\\beta}_{j}$ is the least squares estimator for the j-th predictor.\n",
        "\n",
        "We define the residual [**residuals**]{style=\"color:purple;\"} $\\hat{\\epsilon}_i = Y_i - \\hat{Y}_i$, $i = 1, \\ldots, n.$\n",
        "\n",
        "::: notes\n",
        "The residuals $\\hat{\\epsilon}_i$ are the estimates of the random errors $\\epsilon_i$.\n",
        ":::\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        "If the model structure is correctly specified and assuming that the least-squares estimates $\\hat{\\beta}_j$’s are close to the true $\\beta_j$’s, respectively, we have that\n",
        "\n",
        "$$\\hat{\\epsilon}_i = Y_i - \\hat{Y}_i = Y_i - \\hat{\\beta}_0 + \\hat{\\beta}_1 X_{i1} + \\cdots + \\hat{\\beta}_p X_{ip} \\approx \\epsilon_i$$\n",
        "\n",
        "So, the residuals $\\hat{\\epsilon}_i$ should resemble the random errors $\\epsilon$.\n",
        "\n",
        "To evaluate the assumption of a (simple and) multiple linear regression model, we use a [**Residual analysis**]{style=\"color:green;\"}.\n",
        "\n",
        "## Residual Analysis\n",
        "\n",
        "</br>\n",
        "\n",
        "To check the validity of these assumptions, we will follow a graphical approach. Specifically, we will construct three informative plots of the residuals.\n",
        "\n",
        "::: incremental\n",
        "1.  **Residuals vs Fitted Values** Plot. To assess the structure of the model and check for constant variance\n",
        "\n",
        "2.  **Residuals Vs Time** Plot. To check independence.\n",
        "\n",
        "3.  **Normal Quantile-Quantile** Plot. To assess if the residuals follow a normal distribution\n",
        ":::\n",
        "\n",
        "## Example 1\n",
        "\n",
        "</br>\n",
        "\n",
        "-   This example is inspired by Foster, Stine and Waterman (1997, pages 191–199).\n",
        "\n",
        "-   The data are in the form of the time taken (in minutes) for a production run, $Y$, and the number of items produced, $X$, for 20 randomly selected orders as supervised by a manager.\n",
        "\n",
        "-   We wish to develop an equation to model the relationship between the run time ($Y$) and the run size ($X$).\n",
        "\n",
        "## Dataset\n",
        "\n",
        "The dataset is in the file \"production.xlsx\".\n"
      ],
      "id": "8dce65ef"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "production_data = pd.read_excel('production.xlsx')\n",
        "production_data.head()"
      ],
      "id": "92c58013",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Intuition behind ...\n",
        "\n",
        "the Residuals versus Fitted Values plot.\n"
      ],
      "id": "0b9d3896"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: true\n",
        "#| fig-align: center\n",
        "\n",
        "# Defining the predictor (X) and the response variable (Y)\n",
        "X_pred = production_data['RunSize'] \n",
        "y = production_data['RunTime']\n",
        "\n",
        "# Add intercept.\n",
        "X = sm.add_constant(X_pred)\n",
        "\n",
        "# Fitting the simple linear regression model\n",
        "regr = sm.OLS(y, X)\n",
        "lr_model = regr.fit()\n",
        "\n",
        "plt.figure(figsize=(7, 5))\n",
        "sns.scatterplot(x='RunSize', y='RunTime', data=production_data, color='blue')\n",
        "plt.plot(production_data['RunSize'], lr_model.fittedvalues, color='red', linestyle='--', linewidth=2)\n",
        "plt.title(\"Fitted Regression Line\")\n",
        "plt.xlabel(\"Run size\", fontsize = 14)\n",
        "plt.ylabel(\"Run time\", fontsize = 14)\n",
        "plt.show()"
      ],
      "id": "85cc2d01",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculation of fitted values and residuals\n",
        "\n",
        "Recall that we can calculate the predicted values and residuals using commands from **statsmodels**.\n"
      ],
      "id": "d38cb763"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "# Defining the predictor (X) and the response variable (Y).\n",
        "prod_Y_train = production_data['RunTime']\n",
        "prod_X_pred = production_data['RunSize']\n",
        "prod_X_train = sm.add_constant(prod_X_pred)\n",
        "\n",
        "# Fitting the simple linear regression model.\n",
        "regr = sm.OLS(prod_Y_train, prod_X_train)\n",
        "linear_model = regr.fit()\n",
        "\n",
        "# Make predictions using the the model\n",
        "prod_Y_pred = linear_model.fittedvalues\n",
        "\n",
        "# Calculate residuals.\n",
        "residuals = linear_model.resid"
      ],
      "id": "87da78f2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Residuals vs Fitted Values\n",
        "\n",
        "::::: columns\n",
        "::: {.column width=\"60%\"}"
      ],
      "id": "ac4fbd2e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "#| code-fold: true\n",
        "#| fig-align: center\n",
        "\n",
        "# Residual vs Fitted Values Plot\n",
        "plt.figure(figsize=(5, 5))\n",
        "sns.scatterplot(x = prod_Y_pred, y = residuals)\n",
        "plt.axhline(y=0, color='red', linestyle='--')\n",
        "plt.title('Residuals vs Fitted Values')\n",
        "plt.xlabel('Fitted (predicted) Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.show()"
      ],
      "id": "ec5ee194",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.column width=\"40%\"}\n",
        "</br>\n",
        "\n",
        "-   If there is a trend, the model is misspecified.\n",
        "-   A \"funnel\" shape indicates that the assumption of constant variance is not met.\n",
        ":::\n",
        ":::::\n",
        "\n",
        "## \n",
        "\n",
        "Examples of plots that do not support the conclusion of constant variance.\n",
        "\n",
        "![](images/clipboard-513958775.png){fig-align=\"center\"}\n",
        "\n",
        "## \n",
        "\n",
        "Another example.\n",
        "\n",
        "![](images/clipboard-890785179.png){fig-align=\"center\"}\n",
        "\n",
        "The phenomenon of non-constant variance is called [**heteroscedasticity**]{style=\"color:red;\"}.\n",
        "\n",
        "## Residuals vs Time Plot\n",
        "\n",
        "::: incremental\n",
        "-   By “time,” we mean that time the observation was taken or the order in which it was taken. The plot should not show any structure or pattern in the residuals.\n",
        "\n",
        "-   Dependence on time is a common source of lack of independence, but other plots might also detect lack of independence.\n",
        "\n",
        "-   Ideally, we plot the residuals versus each variable of interest we could think of, either included or excluded in the model.\n",
        "\n",
        "-   Assessing the assumption of independence is hard in practice.\n",
        ":::\n",
        "\n",
        "## \n"
      ],
      "id": "95d12097"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "#| code-fold: true\n",
        "#| fig-align: center\n",
        "\n",
        "# Residuals vs Time (Case) Plot\n",
        "plt.figure(figsize=(7, 5))\n",
        "sns.scatterplot(x = production_data['Case'], y = residuals)\n",
        "plt.axhline(y=0, color='red', linestyle='--')\n",
        "plt.title('Residuals vs Time (Case)')\n",
        "plt.xlabel('Case')\n",
        "plt.xticks(production_data['Case'])\n",
        "plt.ylabel('Residuals')\n",
        "plt.show()"
      ],
      "id": "9a39cf75",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \n",
        "\n",
        "::::: columns\n",
        "::: {.column width=\"30%\"}\n",
        "</br></br>\n",
        "\n",
        "Examples of plots that do and do not support the independence assumption.\n",
        ":::\n",
        "\n",
        "::: {.column width=\"70%\"}\n",
        "![](images/clipboard-2835904055.png){fig-align=\"center\" width=\"411\"}\n",
        ":::\n",
        ":::::\n",
        "\n",
        "## Checking for normality\n",
        "\n",
        "</br>\n",
        "\n",
        "This assumption is generally checked by looking at the distribution of the residuals.\n",
        "\n",
        "Two plots:\n",
        "\n",
        "-   Histogram.\n",
        "\n",
        "-   Normal Quantile-Quantile Plot (also called normal probability plot).\n",
        "\n",
        "## Histogram\n",
        "\n",
        "Ideally, the histogram resembles a normal distribution around 0. If the number of observations is small, the histogram may not be an effective visualization.\n"
      ],
      "id": "c64ad0bc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "#| code-fold: true\n",
        "#| fig-align: center\n",
        "\n",
        "# Histogram of residuals\n",
        "plt.figure(figsize=(5, 3))\n",
        "sns.histplot(residuals)\n",
        "plt.title('Histogram of Residuals')\n",
        "plt.xlabel('Residuals')\n",
        "plt.show()"
      ],
      "id": "3f694e83",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normal Quantile-Quantile (QQ) Plot\n",
        "\n",
        "</br>\n",
        "\n",
        "A normal QQ plot is helpful for deciding whether a sample was drawn from a distribution that is approximately normal.\n",
        "\n",
        ". . .\n",
        "\n",
        "First, let $\\hat{\\epsilon}_{[1]}, \\hat{\\epsilon}_{[2]}, \\ldots, \\hat{\\epsilon}_{[n]}$ be the residuals ranked in an increasing order, where $\\hat{\\epsilon}_{[1]}$ is the minimum and $\\hat{\\epsilon}_{[n]}$ is the maximum. These points define the [**sample**]{style=\"color:lightblue;\"} percentiles (or quantiles) of the distribution of the residuals.\n",
        "\n",
        ". . .\n",
        "\n",
        "Next, calculate the [**theoretical**]{style=\"color:pink;\"} percentiles of a (standard) Normal distribution calculated using Python.\n",
        "\n",
        "## \n",
        "\n",
        "</br></br>\n",
        "\n",
        "The normal QQ plot displays the (sample) percentiles of the residuals versus the percentiles of a normal distribution.\n",
        "\n",
        "If these percentiles agree with each other, then they would approximate a straight line.\n",
        "\n",
        "The straight line is usually determined visually, with emphasis on the central values rather than the extremes.\n",
        "\n",
        "For a nice explanation, see this [YouTube video](https://www.youtube.com/watch?v=okjYjClSjOg&ab_channel=StatQuestwithJoshStarmer).\n",
        "\n",
        "## QQ plot in Python\n",
        "\n",
        "</br></br></br>\n",
        "\n",
        "To construct a QQ plot, we use the function `qqplot()` **statsmodels** library.\n"
      ],
      "id": "4c05bdab"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: false\n",
        "#| code-fold: false\n",
        "#| fig-align: center\n",
        "\n",
        "# QQ plot to assess normality of residuals\n",
        "plt.figure(figsize=(5, 3))\n",
        "sm.qqplot(residuals, fit = True, line = '45')\n",
        "plt.title('QQ Plot of Residuals')\n",
        "plt.show()"
      ],
      "id": "2573e05c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \n",
        "\n",
        "Substantial departures from a straight line indicate that the distribution is not normal.\n"
      ],
      "id": "33794c0b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: true\n",
        "#| code-fold: false\n",
        "#| fig-align: center\n",
        "\n",
        "# QQ plot to assess normality of residuals\n",
        "plt.figure(figsize=(7, 4))\n",
        "sm.qqplot(residuals, fit = True, line = '45')\n",
        "plt.title('QQ Plot of Residuals')\n",
        "plt.show()"
      ],
      "id": "90e596b5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \n",
        "\n",
        "This plot suggests that the residuals are consistent with a Normal curve.\n"
      ],
      "id": "277a4b56"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: true\n",
        "#| code-fold: false\n",
        "#| fig-align: center\n",
        "\n",
        "# QQ plot to assess normality of residuals\n",
        "plt.figure(figsize=(7, 4))\n",
        "sm.qqplot(residuals, fit = True, line = '45')\n",
        "plt.title('QQ Plot of Residuals')\n",
        "plt.show()"
      ],
      "id": "6a61d85b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comments\n",
        "\n",
        "These data are truly Normally distributed. But note that we still see deviations. These are entirely due to chance.\n",
        "\n",
        "![](images/clipboard-191119898.png){fig-align=\"center\"}\n",
        "\n",
        "When $n$ is relatively small, you tend to see deviations, particularly in the tails.\n",
        "\n",
        "## \n",
        "\n",
        "**Normal probability plots for data sets following various distributions.** 100 observations in each data set.\n",
        "\n",
        "![](images/clipboard-1290062732.png){fig-align=\"center\"}\n",
        "\n",
        "## Consequences of faulty assumptions\n",
        "\n",
        "</br>\n",
        "\n",
        "[If the model structure is incorrect]{style=\"color:red;\"}, the estimated coefficients $\\hat{\\beta}_j$ will be biased and the predictions $\\hat{Y}_i$ will be inaccurate.\n",
        "\n",
        ". . .\n",
        "\n",
        "[If the residuals do not follow a normal distribution]{style=\"color:red;\"}, we have two cases:\n",
        "\n",
        "-   If sample size is large, we still get accurate p-values for the t-tests (discussed later) for the coefficients thanks to the *Central Limit Theorem*.\n",
        "-   However, the t-tests and all inference tools are invalidated.\n",
        "\n",
        "## \n",
        "\n",
        "</br></br></br>\n",
        "\n",
        "[If the residuals do not have constant variance]{style=\"color:red;\"}, then the linear model is incorrect and everything falls apart!\n",
        "\n",
        "[If the residuals are dependent]{style=\"color:red;\"}, then the linear model is incorrect and everything falls apart!\n",
        "\n",
        "# Inference about individual $\\beta$’s using t-tests\n",
        "\n",
        "## Example 2\n",
        "\n",
        "</br>\n",
        "\n",
        "Let's consider the \"auto.xlsx\" dataset.\n"
      ],
      "id": "942fdfab"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "auto_data = pd.read_excel('auto.xlsx')"
      ],
      "id": "f681fd19",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "</br>\n",
        "\n",
        "Let's assume that we want to study the following model.\n",
        "\n",
        "$$\n",
        "Y_i = \\beta_0 + \\beta_1 X_{i1}+ \\beta_2 X_{i2}+ \\epsilon_i,\n",
        "$$\n",
        "\n",
        "where $Y_i$ is the mpg, $X_{i1}$ is the weight, and $X_{i2}$ is the acceleration of the $i$-th car, $i = 1, \\ldots, 392$.\n",
        "\n",
        "## Research question\n",
        "\n",
        "</br></br></br>\n",
        "\n",
        "> Do the weight and acceleration have a significant association with the mpg of a car?\n",
        "\n",
        "## The two cultures of statistical models\n",
        "\n",
        "</br>\n",
        "\n",
        "::: incremental\n",
        "-   [**Inference**]{style=\"color:darkblue;\"}: develop a model that fits the data well. Then make inferences about the data-generating process based on the structure of such model.\n",
        "\n",
        "-   [**Prediction**]{style=\"color:darkgreen;\"}: Silent about the underlying mechanism generating the data and allow for many predictive algorithms, which only care about accuracy of predictions.\n",
        ":::\n",
        "\n",
        ". . .\n",
        "\n",
        "They overlap very often.\n",
        "\n",
        "## \n",
        "\n",
        "</br></br></br>\n",
        "\n",
        "The least squares estimators $\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p$ are subject to uncertainty, since they are calculated based on a random sample of data.\n",
        "\n",
        "Therefore, assessing the amount of the uncertainty in these estimators is important. To this end, we use [hypothesis]{style=\"color:blue;\"} tests on individual coefficients.\n",
        "\n",
        "## Hypothesis test\n",
        "\n",
        "</br>\n",
        "\n",
        "> A statistical hypothesis is a statement about the coefficients of a model.\n",
        "\n",
        ". . .\n",
        "\n",
        "In our case, we are interested in testing:\n",
        "\n",
        "::: r-stack\n",
        "$H_0: \\beta_j = 0$ v.s. $H_1: \\beta_j \\neq 0$ (Two-tailed Test)\n",
        ":::\n",
        "\n",
        "::: incremental\n",
        "-   Rejecting $H_0$ (in favor of $H_1$) implies that the predictor has a significant association with the response.\n",
        "-   Not rejecting $H_0$ implies that the predictor does not have a significant association with the response.\n",
        ":::\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        "::: r-stack\n",
        "$H_0: \\beta_j = 0$ v.s. $H_1: \\beta_j \\neq 0$ (Two-tailed Test)\n",
        ":::\n",
        "\n",
        "</br>\n",
        "\n",
        "Testing this hypothesis consists of the following steps:\n",
        "\n",
        "1.  Take a random sample.\n",
        "2.  Compute the appropriate test statistic.\n",
        "3.  Reject or fail to reject the null hypothesis based on a computed p-value.\n",
        "\n",
        "## Step 1. Random sample\n",
        "\n",
        "</br></br>\n",
        "\n",
        "The random sample is the data we use to train or fit the model.\n"
      ],
      "id": "f95f91a5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "# Dataset with predictors.\n",
        "pred_auto = auto_data[['weight', 'acceleration']]\n",
        "\n",
        "# Add the column for the intercept.\n",
        "auto_X_train = sm.add_constant(pred_auto)\n",
        "\n",
        "# Dataset with the response only.\n",
        "auto_Y_train = auto_data['mpg']"
      ],
      "id": "b28e7289",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2. Test statistic\n",
        "\n",
        "</br>\n",
        "\n",
        "The test statistic is\n",
        "\n",
        "$$t_0 = \\frac{\\hat{\\beta}_{j}}{\\sqrt{ \\hat{v}_{jj}} }$$\n",
        "\n",
        "-   $\\hat{\\beta}_j$ is the least squares estimate of the true coefficient $\\beta_j$.\n",
        "-   $\\hat{v}_{jj}$ is the *standard error* of the estimate $\\hat{\\beta}_j$ calculated using Python.\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        "Recall that we obtain the least squares estimates ($\\hat{\\beta}_{j}$) in Python using:\n"
      ],
      "id": "4e71a35c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "# Fitting the simple linear regression model.\n",
        "regr = sm.OLS(auto_Y_train, auto_X_train)\n",
        "linear_model = regr.fit()\n",
        "linear_model.params"
      ],
      "id": "9d6b245c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Later, we will see how to obtain the *standard error* of the estimate $\\hat{\\beta}_j$.\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        "::::: columns\n",
        "::: {.column width=\"30%\"}\n",
        "</br>\n",
        "\n",
        "$$t_0 = \\frac{\\hat{\\beta}_{j}}{\\sqrt{ \\hat{v}_{jj}} }$$\n",
        ":::\n",
        "\n",
        "::: {.column width=\"70%\"}\n",
        "-   We like this statistic because it follows a well-known distribution.\n",
        "\n",
        "-   If the null hypothesis ($H_0: \\beta_j = 0$) is true, the statistic $t_0$ follows a [$t$ distribution]{style=\"color:purple;\"} with $n-p-1$ degrees of freedom\n",
        "\n",
        "-   Remember that $n$ is the number of observations and $p$ the number of predictors.\n",
        ":::\n",
        ":::::\n",
        "\n",
        "## t distribution\n",
        "\n",
        "</br>\n",
        "\n",
        "This distribution is also known as the student’s t-distribution.\n",
        "\n",
        "It was invented by [William Gosset](https://en.wikipedia.org/wiki/William_Sealy_Gosset) when he worked at the Guinness Brewery in Ireland.\n",
        "\n",
        "It has one parameter $\\nu$ which generally equals a number of degrees of freedom.\n",
        "\n",
        "The parameter $\\nu$ controls the shape of the distribution.\n",
        "\n",
        "## \n",
        "\n",
        "The t-distribution resembles a standard normal distribution when $\\nu$ goes to infinity.\n",
        "\n",
        "![](images/clipboard-309922779.png){fig-align=\"center\"}\n",
        "\n",
        "## Step 3. Calculate the p-value\n",
        "\n",
        "</br></br></br>\n",
        "\n",
        "The [**p-value**]{style=\"color:pink;\"} is the probability that the [$t$ test statistic]{style=\"color:darkgreen;\"} will get a value that is at least as extreme as the observed $t_0$ value when the null hypothesis ($H_0$) is true.\n",
        "\n",
        "## \n",
        "\n",
        "For $H_0: \\beta_j = 0$ v.s. $H_1: \\beta_j \\neq 0$, the p-value is calculated using both tails of the $t$ distribution. It is the blue area under the curve below.\n",
        "\n",
        "![](images/critical_region2.png){fig-align=\"center\"}\n",
        "\n",
        "We use the two tails because $H_1$ includes the possibility that the value of $\\beta_j$ is positive or negative.\n",
        "\n",
        "## The p-value is not the probability that $H_0$ is true\n",
        "\n",
        "Since the p-value is a probability, and since small p-values indicate that $H_0$ is unlikely to be true, it is tempting to think that the p-value represents the probability that $H_0$ is true.\n",
        "\n",
        "</br>\n",
        "\n",
        ":::: r-stack\n",
        "::: {style=\"font-size: 110%;\"}\n",
        "[This is not the case!]{style=\"color:red;\"}\n",
        ":::\n",
        "::::\n",
        "\n",
        "</br>\n",
        "\n",
        "Remember that the **p-value** is the probability that the $t$ test statistic will get a value that is at least as extreme as the observed $t_0$ value **when** $H_0$ is true.\n",
        "\n",
        "## How small must the p-value be to reject $H_0$?\n",
        "\n",
        ":::::: columns\n",
        ":::: {.column width=\"60%\"}\n",
        "::: incremental\n",
        "-   **Answer**: Very small!\n",
        "\n",
        "-   But, how small?\n",
        "\n",
        "-   **Answer**: Very small!\n",
        "\n",
        "-   But, tell me, how small?\n",
        "\n",
        "-   **Answer**: Okay, it must be less than a value called $\\alpha$ which is usually 0.1, 0.05, or 0.01.\n",
        "\n",
        "-   Thank you!\n",
        ":::\n",
        "::::\n",
        "\n",
        "::: {.column width=\"40%\"}\n",
        "![](images/clipboard-2242705857.png){width=\"478\"} [Sir Ronald Fisher](https://en.wikipedia.org/wiki/Ronald_Fisher)\n",
        ":::\n",
        "::::::\n",
        "\n",
        "## Decision\n",
        "\n",
        "</br>\n",
        "\n",
        "For a significance level of $\\alpha = 0.05$:\n",
        "\n",
        "-   If the p-value is smaller than $\\alpha$, we reject $H_0: \\beta_j = 0$ in favor of $H_1: \\beta_j \\neq 0$.\n",
        "\n",
        "-   If the p-value is larger than $\\alpha$, we do not reject $H_0: \\beta_j = 0$.\n",
        "\n",
        "No scientific basis for this advice. In practice, report the p-value and explore the data using descriptive statistics.\n",
        "\n",
        "## \n",
        "\n",
        "![](images/clipboard-1571120222.png){fig-align=\"center\"}\n",
        "\n",
        "## t-tests in Python\n",
        "\n",
        "We obtain the t-tests of the coefficients using the function `summary()` together with the `linear_model` object.\n"
      ],
      "id": "16492f17"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "\n",
        "model_summary = linear_model.summary()\n",
        "print(model_summary)"
      ],
      "id": "3d285a46",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of t-tests\n",
        "\n",
        "|              | coef    | std err | t       | P\\>$|t|$ | $[0.025$ | $0.975 ]$ |\n",
        "|--------------|---------|---------|---------|----------|----------|-----------|\n",
        "| const        | 41.0953 | 1.868   | 21.999  | 0.000    | 37.423   | 44.768    |\n",
        "| weight       | -0.0073 | 0.000   | -25.966 | 0.000    | -0.008   | -0.007    |\n",
        "| acceleration | 0.2617  | 0.086   | 3.026   | 0.003    | 0.092    | 0.432     |\n",
        "\n",
        "The column [**std err**]{style=\"color:darkblue;\"} contains the values of the [estimated standard error]{style=\"color:darkblue;\"} ($\\hat{v}_{jj}$) of the estimates $\\hat{\\beta}_j$. They represent the variability in the estimates.\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        "|              | coef    | std err | t       | P\\>$|t|$ | $[0.025$ | $0.975 ]$ |\n",
        "|--------------|---------|---------|---------|----------|----------|-----------|\n",
        "| const        | 41.0953 | 1.868   | 21.999  | 0.000    | 37.423   | 44.768    |\n",
        "| weight       | -0.0073 | 0.000   | -25.966 | 0.000    | -0.008   | -0.007    |\n",
        "| acceleration | 0.2617  | 0.086   | 3.026   | 0.003    | 0.092    | 0.432     |\n",
        "\n",
        "The column [**t**]{style=\"color:darkgreen;\"} contains the values of the [observed statistic $t_0$]{style=\"color:darkgreen;\"} for the hypothesis tests.\n",
        "\n",
        "## \n",
        "\n",
        "|              | coef    | std err | t       | P\\>$|t|$ | $[0.025$ | $0.975 ]$ |\n",
        "|--------------|---------|---------|---------|----------|----------|-----------|\n",
        "| const        | 41.0953 | 1.868   | 21.999  | 0.000    | 37.423   | 44.768    |\n",
        "| weight       | -0.0073 | 0.000   | -25.966 | 0.000    | -0.008   | -0.007    |\n",
        "| acceleration | 0.2617  | 0.086   | 3.026   | 0.003    | 0.092    | 0.432     |\n",
        "\n",
        "The column [**P\\>\\|t\\|**]{style=\"color:pink;\"} contains the [p-values]{style=\"color:pink;\"} for the hypothesis tests.\n",
        "\n",
        "Since the p-value is smaller than $\\alpha = 0.05$, we reject $H_0$ for acceleration and weight. Therefore, **weight and acceleration have a significant association with the mpg of a car**.\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        "|              | coef    | std err | t       | P\\>$|t|$ | $[0.025$ | $0.975 ]$ |\n",
        "|--------------|---------|---------|---------|----------|----------|-----------|\n",
        "| const        | 41.0953 | 1.868   | 21.999  | 0.000    | 37.423   | 44.768    |\n",
        "| weight       | -0.0073 | 0.000   | -25.966 | 0.000    | -0.008   | -0.007    |\n",
        "| acceleration | 0.2617  | 0.086   | 3.026   | 0.003    | 0.092    | 0.432     |\n",
        "\n",
        "The columns $[0.025$ and $0.975]$ show the limits of a 95% confidence interval on each true coefficient $\\beta_j$. This interval *contains* the true parameter value with a confidence of 95%.\n",
        "\n",
        "# Multiple and adjusted $R^2$ statistics\n",
        "\n",
        "## Total Sum of Squares\n",
        "\n",
        "</br>\n",
        "\n",
        "Let’s consider the total sum of squares\n",
        "\n",
        "$$SS_{Total} = \\sum_{i=1}^{n} (Y_i - \\bar{Y})^2 \\text{ where } \\bar{Y} = \\sum_{i=1}^{n} \\frac{Y_i}{n}.$$\n",
        "\n",
        "This quantity measures the total variation of the response.\n",
        "\n",
        "In other words, it is the amount of variability inherent in the response before we fit a regression model.\n",
        "\n",
        "## Residual Sum of Squares\n",
        "\n",
        "Let’s also consider the residual sum of squares\n",
        "\n",
        "$$RSS = \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2.$$\n",
        "\n",
        "RSS is the sum of squares due to residuals of the linear regression model; or, residual variation left unexplained by this model.\n",
        "\n",
        "The better the predictions of the model, the smaller the RSS value.\n",
        "\n",
        "## Coefficient of determination\n",
        "\n",
        "$$R^2 = 1 - \\frac{RSS}{SS_{total}}$$\n",
        "\n",
        "$R^2$ measures the “proportion of variation in the response explained by the full regression model.”\n",
        "\n",
        "![](images/example_R-01.png)\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        "$$R^2 = 1 - \\frac{RSS}{SS_{total}}$$\n",
        "\n",
        "What would you conclude about RSS if $R^2 = 1$?\n",
        "\n",
        "::: incremental\n",
        "-   In this case, $RSS = 0$ and the model fits the data perfectly.\n",
        "\n",
        "-   If $R^2$ is small, then large RSS: lots of scatter and the model’s fit is not good.\n",
        "\n",
        "-   It turns out that $R^2$ is the square of the correlation between the observed ($Y_i$) and predicted ($\\hat{Y}_i$) responses.\n",
        ":::\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        "![](images/Figures.004.jpeg){fig-align=\"center\"}\n",
        "\n",
        "## Remarks on $R^2$\n",
        "\n",
        "</br>\n",
        "\n",
        "-   The statistic $R^2$ should be used with caution because it is always possible to make it unity by simply adding more and more predictors.\n",
        "\n",
        "-   If $R^2$ is large, it does not necessarily imply that the full model will provide accurate predictions of future observations.\n",
        "\n",
        "## Adjusted $R^2$ statistic\n",
        "\n",
        "Adjusted $R^2$ is a better measure to decide whether to add a new variable into the model or not. It is:\n",
        "\n",
        "$$R_{adj}^2=1-\\frac{RSS/(n-k-1)}{SS_{total}/(n-1)},$$\n",
        "\n",
        "where $k$ is the number of variables in the model. For the full model, $k = p$.\n",
        "\n",
        "::: incremental\n",
        "-   If adjusted $R^2$ goes down or stays the same, then new variable is not important.\n",
        "-   If adjusted $R^2$ goes up, then it is probably useful.\n",
        ":::\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        "![](images/Figures.005.jpeg){fig-align=\"center\"}\n",
        "\n",
        "## \n",
        "\n",
        "Why is adjusted $R^2$ more useful than $R^2$ for adding new variables?\n",
        "\n",
        "$$R_{adj}^2=1-\\frac{RSS/(n-k-1)}{SS_{total}/(n-1)}$$\n",
        "\n",
        "::: incremental\n",
        "-   As we explain more variability, the numerator gets smaller and adjusted $R^2$ gets closer to 1. So, we want to make the numerator small.\n",
        "-   If we add a “good” variable to the model, RSS will go down. However, $n - k - 1$ will decrease a little bit (because $k$ is now bigger by 1.) So the numerator does not go down by as much as it does in ‘plain’ $R^2$.\n",
        ":::\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        "$$R_{adj}^2=1-\\frac{RSS/(n-k-1)}{SS_{total}/(n-1)}$$\n",
        "\n",
        "-   If we add a useless predictor, RSS goes down a tiny bit, but because we divide by $n - k - 1$, the numerator might actually get bigger or change very little.\n",
        "\n",
        "-   So, adjusted $R^2$ is a better measure of whether adding a new predictor is an improvement. If adjusted $R^2$ goes down or stays the same, the new predictor is irrelevant. If it goes up, then it is probably useful.\n",
        "\n",
        "## In Python\n",
        "\n",
        "</br>\n",
        "\n",
        "To show the $R^2$ value, we use the `rsquared` argument of the `linear_model` object.\n"
      ],
      "id": "ab1e87fb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "print( round(linear_model.rsquared, 3) )"
      ],
      "id": "64ab1b0c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To show the adjusted $R^2$ value, we use the `rsquared` argument of the `linear_model` object.\n"
      ],
      "id": "444d7653"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "print( round(linear_model.rsquared_adj, 3) )"
      ],
      "id": "0f2dbaba",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model building\n",
        "\n",
        "</br>\n",
        "\n",
        "Model building refers to a set of techniques to reduce the full model to one that only includes significant predictors.\n",
        "\n",
        "Note that it is more a deconstruction than building an actual model. This is because the full model must be a valid one to begin with. Specifically, the full model must provide residuals that satisfy all the assumptions (1)-(4).\n",
        "\n",
        "After we have obtained a satisfactory full model, we can start the model building process.\n",
        "\n",
        "## Two model building techniques\n",
        "\n",
        "</br>\n",
        "\n",
        "::: incremental\n",
        "1.  Adjusted $R^2$ statistic. We add one variable at a time and see the change in adjusted $R^2$ statistic. If the value decreases, then we stop and evaluate the resulting model.\n",
        "\n",
        "2.  T-tests on the individual coefficients. If $H_0$ is not rejected, this indicates that the predictor $X_j$ can be deleted from the model. We can then delete the predictor and refit the model. We can repeat this process several times until we reach a model in which all variables are significant.\n",
        ":::\n",
        "\n",
        "# [Return to main page](https://alanrvazquez.github.io/TEC-IN1002B-Website/)"
      ],
      "id": "f405a970"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}