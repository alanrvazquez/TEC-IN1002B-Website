---
title: "Logistic Regression"
subtitle: "IN1001B: Introduction to Data Science Projects"
author: 
  - name: Alan R. Vazquez
    affiliations:
      - name: Department of Industrial Engineering
format: 
  revealjs:
    chalkboard: false
    multiplex: true
    footer: "Tecnologico de Monterrey"
    logo: IN1002b_logo.png
    css: style.css
    slide-number: True
    html-math-method: mathjax
editor: visual
jupyter: python3
---

## Agenda

1.  Introduction
2.  Training and test set
3.  Logistic regression
4.  Classification performance

# Introduction

## Load the libraries

Before we start, let's import the data science libraries into python.

```{python}
#| echo: true
#| output: false

# Importing necessary libraries
import pandas as pd
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score
```

Here, we will introduce the function `train_test_split()` to split a data set into training and test sets. We will also introduce the functions `confusion_matrix()` and `accuracy_score()` to evaluate the performance of classifiers.

## Two main problems

[**Regression problems**]{style="color:blue;"}. The response $Y$ is quantitative. For example, person’s income, the value of a house, the blood pressure of a patient.

[**Classification problems**]{style="color:green;"}. The response $Y$ is qualitative and involves $K$ different categories. For example, the brand of a product purchased (A, B, C) whether a person defaults on a debt (yes or no).

The predictors ($\boldsymbol{X}$) can be qualitative or quantitative.

## Terminology

</br></br>

Explanatory variables or predictors:

-   $X$ represents an explanatory variable or predictor.
-   $\boldsymbol{X}$ represents a whole collection of predictors.

## 

[Response]{style="text-decoration: underline;"}:

-   $Y$ is a [**categorical variable**]{style="color:darkgreen;"} taking [**2 categories**]{style="color:darkgreen;"} or [**classes**]{style="color:darkgreen;"}.

-   For example, $Y$ can take [0]{style="color:darkgreen;"} or [1]{style="color:darkgreen;"}, [A]{style="color:darkgreen;"} or [B]{style="color:darkgreen;"}, [no]{style="color:darkgreen;"} or [yes]{style="color:darkgreen;"}, [spam]{style="color:darkgreen;"} or [not spam]{style="color:darkgreen;"}.

-   When the classes are strings, it is customary to code them to 0 and 1.

    -   The **target class** is the one for which $Y = 1$.
    -   The **reference class** is the one for which $Y = 0$.

## 

[**Goal**]{style="color:darkgreen;"}: Find the best function $C(\boldsymbol{x})$ for predicting $Y = \{0, 1\}$ from $\boldsymbol{x}$.

</br>

To achieve this, we will consider functions $C(\boldsymbol{x})$ that [**predict the probability**]{style="color:brown;"} that $Y$ takes the value of 1.

</br>

A probability for each class can be very useful for gauging the model’s confidence about the predicted classification.

## Example

Consider a spam e-mail filter.

-   The target class is spam.
-   The reference class is not spam.

![](images/spam.png){fig-align="center" width="556" height="178"}

Both e-mails would be classified as spam. However, we'd have more confidence in our classification for the second email.

## 

In technical terms, $C(\boldsymbol{x})$ will work with the *conditional probability*:

$$P(Y = 1 | X_1 = x_1, X_2 = x_2, \ldots, X_p = x_p) = P(Y = 1 | \boldsymbol{X} = \boldsymbol{x})$$

In words, this is the probability that $Y$ takes a value of 1 [**given that**]{style="color:brown;"} the predictors $\boldsymbol{X}$ have taken the values $x_1, x_2, \ldots, x_p$.

</br>

Note that the conditional probability that $Y$ takes the value of 0 is

$$P(Y = 0 | \boldsymbol{X} = \boldsymbol{x}) = 1 - P(Y = 1 | \boldsymbol{X} = \boldsymbol{x}).$$

## Bayes Classifier

It turns out that, if we have the true structure of $P(Y = 1 | \boldsymbol{X} = \boldsymbol{x})$, we can build a good classification function called the [**Bayes classifier**]{style="color:darkblue;"}:

$$C(\boldsymbol{x}) =
    \begin{cases}
      1, & \text{if}\ P(Y = 1 | \boldsymbol{X} = \boldsymbol{x}) > 0.5 \\
      0, & \text{if}\ P(Y = 1 | \boldsymbol{X} = \boldsymbol{x}) \leq 0.5
    \end{cases}.$$

This function classifies to the most probable class using the conditional distribution $P(Y | \boldsymbol{X} = \boldsymbol{x})$.

## 

</br>

[HOWEVER, we don’t (and will never) know the true form of $P(Y = 1 | \boldsymbol{X} = \boldsymbol{x})$!]{style="color:brown;"}

To overcome this issue, we have a standard solution:

-   Impose an structure on $P(Y = 1 | \boldsymbol{X} = \boldsymbol{x})$. [**Logistic Regression classifier**]{style="color:brown;"}.

Before discussing the logistic regression classifier, we need to introduce the training and test set for building and testing the model.

# Training and test sets

## Example

Consider the task of identifying old-swiss counterfeit banknotes. The response under study is

$$Y =
    \begin{cases}
      1, & \text{if it is a counterfeit banknote} \\
      0, & \text{otherwise}
    \end{cases}.$$

We have four predictors:

::: {style="font-size: 60%;"}
-   Left: length of left edge (mm)
-   Right: length of right edge (mm)
-   Top: distance from the image to top edge
-   Bottom: distance from image to bottom
:::

## Old-Swiss 1000-franc banknote

![](images/clipboard-270396609.png)

## Dataset

The data is in the file “banknotes.xlsx”.

```{python}
#| echo: true
#| output: true

bank_data = pd.read_excel("banknotes.xlsx")

bank_data['Status'] = bank_data['Status'].astype('category')
bank_data.head()
```

## How do we estimate $C(\boldsymbol{x})$?

We use the training data to construct an estimated function $\hat{C}(\boldsymbol{x})$.

```{python}
#| echo: false
#| output: true

bank_data.head(4)
```

Ideally, $\hat{C}(\boldsymbol{x})$ approximates the true Bayes classifier.

## How do we evaluate $\hat{C}(\boldsymbol{x})$?

</br>

We use [**test data**]{style="color:purple;"}!

Test data is a random sample of $n_t$ observations that

-   is generated from the same (probabilistic) process that generated the training data,
-   but it is **independent** from the training data.

## How do we generate test data?

We split the current data set into a training and a test dataset.

To this end, we use the function `train_test_split()` from **scikit-learn**.

```{python}
#| echo: true
#| output: true

# Set full matrix of predictors.
X_full = bank_data.drop(columns = ['Status'])

# Set full matrix of responses.
Y_full = bank_data['Status']

# Split the dataset.
X_train, X_test, Y_train, Y_test = train_test_split(X_full, Y_full, 
                                                    test_size=0.3)
```

The parameter `test_size` sets the portion of the dataset that will go to the test set.

## 

</br></br>

-   The function makes a clever partition of the data using the *empirical* distribution of the response.

-   Technically, it splits the data so that the distribution of the response under the training and test sets is similar.

-   Usually, the proportion of the dataset that goes to the test set is 20% or 30%.

# Introduction to logistic regression

## Logistic Regression (LR)

[Basic Idea]{style="text-decoration: underline;"}: Impose an structure on $P(Y = 1 | \boldsymbol{X} = \boldsymbol{x})$:

</br>

$$P(Y = 1 | \boldsymbol{X} = \boldsymbol{x}) = \frac{e^{\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p} }{1 + e^{\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p}}$$

</br>

This structure is known as the [**logistic function**]{style="color:gold;"}.

## Why logistic regression?

Let’s use some algebra to reveal some of interesting facts about logistic regression.

We start from

$$P(Y = 1 | \boldsymbol{X} = \boldsymbol{x}) = \frac{e^{\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p} }{1 + e^{\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p}}.$$

Next, we have that

$$e^{\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p} = \frac{P(Y = 1 | \boldsymbol{X} = \boldsymbol{x})}{1 - P(Y = 1 | \boldsymbol{X} = \boldsymbol{x})}.$$

## 

</br></br>

The quantity $e^{\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p} = \frac{P(Y = 1 | \boldsymbol{X} = \boldsymbol{x})}{1 - P(Y = 1 | \boldsymbol{X} = \boldsymbol{x})}$ is known as the "odds" ratio.

</br>

The odds ratio is the probability that $Y = 1$ divided by the probability that $Y = 0$, given that the predictors are $\boldsymbol{X}$.

## 

Consider the banknote classification problem where $Y = 1$ implies counterfeit and $Y = 0$ genuine note.

</br>

The odds ratio is $e^{\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p} = \frac{P(Y = 1 | \boldsymbol{X} = \boldsymbol{x})}{1 - P(Y = 1 | \boldsymbol{X} = \boldsymbol{x})}$

::: incremental
-   If the ratio is 1, the odds are 50-50.
-   If the ratio is greater than 1, the banknote is more likely to be counterfeit than genuine.
-   If the ratio is smaller than 1, it is more likely to be genuine than counterfeit.
:::

## 

If we take logarithm on both sides, we obtain the "[log-odds]{style="color:brown;"}" or "[logit]{style="color:brown;"}":

$$\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p = \ln \left( \frac{P(Y = 1 | \boldsymbol{X} = \boldsymbol{x} )}{1 - P(Y = 1 | \boldsymbol{X} = \boldsymbol{x})} \right).$$

::: incremental
-   The log-odds can take any real value.
-   The log-odds is a linear combination of the predictors.
-   $\beta_j$ can then be [***interpreted***]{style="color:drakblue;"} as the average change in the log-odds ratio given by a one-unit increase in $X_j$ (when all the other predictors have fixed values).
:::

## 

</br>

If $\beta_j$ is positive, increasing $X_j$ will be associated with increasing the odds ratio or $P(Y = 1 | \boldsymbol{X} = \boldsymbol{x})$.

</br>

If $\beta_j$ is negative, increasing $X_j$ will be associated with decreasing $P(Y = 1 | \boldsymbol{X} = \boldsymbol{x})$, and with increasing $P(Y = 0 | \boldsymbol{X} = \boldsymbol{x})$.

</br>

<https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/>

## Parameter estimation

We estimate the parameters in the logistic function using ***maximum likelihood estimation***.

Essentially, we optimize a non-linear objective function using the so-called Iteratively Re-weighted Least Squares (IRLS) algorithm.

The IRLS algorithm (and consequently maximum likelihood estimation and Logistic Regression) fails when:

-   There is [***severe multicollinearity***]{style="color:purple;"} among the predictors.
-   We can [***perfectly separate***]{style="color:purple;"} the observations belonging to the two groups defined by $Y$.

## In python

Using the training dataset, we estimate a logistic regression classifier using the function `Logit()` from **statsmodel**. To this end, we first define the target category using the `get_dummies()`

```{python}
#| echo: true
#| output: true

# Create dummy variables.
Y_dummies = pd.get_dummies(Y_train, dtype = 'int')

# Select target variable.
Y_target_train = Y_dummies['counterfeit']
```

We also define the matrix of predictors with the intercept.

```{python}
#| echo: true
#| output: true

# Add the intercept to the predictor matrix.
X_train_int = sm.add_constant(X_train)
```

## 

After that, we fit the model using the function `Logit()`.

```{python}
#| echo: true
#| output: true

# Create logistic regression object.
logit = sm.Logit(Y_target_train, X_train_int)

# Train the model using the training set.
logit_model = logit.fit()

# Show estimated coefficients.
print(logit_model.params)
```

## Parameter testing

We can construct significance tests for each coefficient in the logistic regression model. They are called *Wald tests*.

$$W_j = \frac{\hat{\beta}_j}{\mbox{SE}(\hat{\beta}_j) } \sim N(0,1)$$

We can use the p-values of these tests to determine what predictor is important. That is, those with a small p-value, say, smaller than 0.05.

Ideally, the number of observations in the training data ($n$) is large.

## 

The `summary()` function of **statsmodel** contains the Wald tests of the coefficients in the logistic regression classifier.

```{python}
#| echo: true
#| output: true

print(logit_model.summary())
```

# Classification performance

## Evaluation

We evaluate a the logistic regression classifier by classifying observations that were not used for training it.

That is, we use the classifier to predict the categories of the test dataset using the predictor values in this set only.

In python, we use the commands:

```{python}
#| echo: true

# Add constant to the predictor matrix from the test set.
X_test = sm.add_constant(X_test)

# Predict probabilities.
predicted_probability = logit_model.predict(X_test)
```

## 

</br>

Technically, the classifier outputs [**probabilities**]{style="color:brown;"} instead of actual classes.

```{python}
#| echo: true
#| output: true

predicted_probability.head()
```

These are the probabilities of a banknote being "counterfeit" according to its characteristics (values of the predictors).

## 

</br>

As seen before, the logistic regression classifier tries to approximate the [**Bayes classifier**]{style="color:darkblue;"}, which predicts to the most probable class.

</br>

Therefore, we turn the probabilities to actual classes by rounding the probabilities.

```{python}
#| echo: true
predicted_classes = round(predicted_probability).astype('int')
```

## 

```{python}
#| echo: true
predicted_classes.head()
```

-   Observations with probabilities higher than 0.5 are classified as "counterfeit".
-   Observations with probabilities lower than 0.5 are classified as "genuine".

Now, we compare the predictions with the actual categories in the test dataset. [A good logistic regression model has a good agreement between its predictions and the actual categories.]{style="color:darkblue;"}

## Confusion matrix

-   A table used to evaluate the performance of a classifier.

-   Compares the actual values with the predicted values of a classifier.

-   Useful for both binary and multiclass classification problems.

![](images/confusion_matrix.png){fig-align="center"}

## In python

We compute the confusion matrix using the function with the same name of **scikit-learn**. To this end,

```{python}
#| echo: true
#| output: true

# Create dummy variables for test set.
Y_dummies = pd.get_dummies(Y_test, dtype = 'int')

# Select target variable from test set.
Y_target_test = Y_dummies['counterfeit']

# Compute confusion matrix.
cm = confusion_matrix(Y_target_test, predicted_classes)

# Show confusion matrix.
print(cm)
```

## Accuracy

A simple metric to summarize the information of the confusion matrix is **accuracy**. It is the proportion of correct classifications for both classes, out of the total classifications made.

In python, we compute the accuracy using the function `accuracy_score()` from **scikit-learn**.

```{python}
#| echo: true
#| output: true

accuracy = accuracy_score(Y_target_test, predicted_classes)
print( round(accuracy, 2) )
```

The higher the accuracy, the better the performance of the classifier.

## Remarks

-   Accuracy is simple to calculate and interpret.

-   It works well when the dataset has a balanced class distribution (i.e., roughly equal positive and negative cases).

-   Not ideal for imbalanced datasets: When one class is much more frequent than the other, accuracy can be misleading.

-   Other summaries of the confusion matrix such as **Precision**, **Recall**, and **F1-Score** are better suited for imbalanced data.

# [Return to main page](https://alanrvazquez.github.io/TEC-IN1002B-Website/)