{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Additional Topics\"\n",
        "subtitle: \"IN1002B: Introduction to Data Science Projects\"\n",
        "author: \n",
        "  - name: Alan R. Vazquez\n",
        "    affiliations:\n",
        "      - name: Department of Industrial Engineering\n",
        "format: \n",
        "  revealjs:\n",
        "    chalkboard: false\n",
        "    multiplex: true\n",
        "    footer: \"Tecnologico de Monterrey\"\n",
        "    logo: IN1002b_logo.png\n",
        "    css: style.css\n",
        "    slide-number: True\n",
        "    html-math-method: mathjax\n",
        "editor: visual\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "\n",
        "## Agenda\n",
        "\n",
        "</br>\n",
        "\n",
        "1.  Linear models with categorical variables\n",
        "2.  Linear models with standardized numerical predictors\n",
        "3.  Remedies for faulty assumptions\n",
        "4.  Predicting future observations\n",
        "\n",
        "## Load the libraries\n",
        "\n",
        "</br>\n",
        "\n",
        "Let's import **scikit-learn** into Python together with the other relevant libraries.\n"
      ],
      "id": "942f6a9e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: false\n",
        "\n",
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "id": "0900361e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will not use all the functions from the **scikit-learn** library. Instead, we will use specific functions from the sub-libraries **preprocessing**, **model_selection**, and **metrics**.\n",
        "\n",
        "# Linear models with categorical predictors\n",
        "\n",
        "## Categorical predictors\n",
        "\n",
        "::: incremental\n",
        "-   A categorical predictor takes on values that are categories, say, names or labels.\n",
        "-   Their use in regression requires dummy variables, which are quantitative variables.\n",
        "-   When a categorical predictor has more than two levels, a single dummy variable cannot represent all possible categories.\n",
        "-   In general, a categorical predictor with $k$ categories requires $k-1$ dummy variables.\n",
        ":::\n",
        "\n",
        "## Dummy coding\n",
        "\n",
        "-   Traditionally, dummy variables are binary variables which can only take the values 0 and 1.\n",
        "\n",
        "-   This approach implies a reference category. Specifically, the category that results when all dummy variables equal 0.\n",
        "\n",
        "-   This coding impacts the interpretation of the model coefficients:\n",
        "\n",
        "    -   $\\beta_0$ is the mean response under the reference category.\n",
        "    -   $\\beta_j$ is the amount of increase in the mean response when we change from the reference category to another category.\n",
        "\n",
        "## Example 1\n",
        "\n",
        "</br>\n",
        "\n",
        "The auto data set includes a categorical variable “Origin” which shows the origin of each car.\n"
      ],
      "id": "3b5e4ef7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "\n",
        "# Load the Excel file into a pandas DataFrame.\n",
        "auto_data = pd.read_excel(\"auto.xlsx\")\n",
        "\n",
        "# Set categorical variables.\n",
        "auto_data['origin'] = pd.Categorical(auto_data['origin'])"
      ],
      "id": "db051210",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset\n"
      ],
      "id": "6ad5c806"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# Show dataset.\n",
        "auto_data[['mpg', 'origin']].head(6)"
      ],
      "id": "60bac41b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dummy variables\n",
        "\n",
        "</br>\n",
        "\n",
        "Origin has 3 categories: European, American, Japanese.\n",
        "\n",
        "So, 2 dummy variables are required:\n",
        "\n",
        "$$d_1 =\n",
        "\\begin{cases}\n",
        "1 \\text{ if car is European}\\\\\n",
        "0 \\text{ if car is not European}\n",
        "\\end{cases} \\text{ and }$$\n",
        "\n",
        "$$d_2 =\n",
        "\\begin{cases}\n",
        "1 \\text{ if car is Japanese}\\\\\n",
        "0 \\text{ if car is not Japanese}\n",
        "\\end{cases}.$$\n",
        "\n",
        "“American” acts as the reference category.\n",
        "\n",
        "## \n",
        "\n",
        "The dataset with the dummy variables looks like this:\n",
        "\n",
        "| origin   | $d_1$    | $d_2$    |\n",
        "|----------|----------|----------|\n",
        "| American | 0        | 0        |\n",
        "| American | 0        | 0        |\n",
        "| European | 1        | 0        |\n",
        "| European | 1        | 0        |\n",
        "| American | 0        | 0        |\n",
        "| Japanese | 0        | 1        |\n",
        "| $\\vdots$ | $\\vdots$ | $\\vdots$ |\n",
        "\n",
        "## The multiple linear regression model\n",
        "\n",
        "</br>\n",
        "\n",
        "$$Y_i= \\beta_0+\\beta_1 d_{1i} +\\beta_2 d_{2i} +\\epsilon_i \\ \\text{for} \\ i=1,\\ldots,n.$$\n",
        "\n",
        "-   $Y_i$ is the i-th response.\n",
        "-   $d_{1i}$ is 1 if the i-th observation is from a European car, and 0 otherwise.\n",
        "-   $d_{2i}$ is 1 if the i-th observation is from a Japanese car, and 0 otherwise.\n",
        "\n",
        "## Model coefficients\n",
        "\n",
        "</br>\n",
        "\n",
        "$$Y_i= \\beta_0+\\beta_1 d_{1i} +\\beta_2 d_{2i} +\\epsilon_i \\ \\text{for} \\ i=1,\\ldots,n.$$\n",
        "\n",
        "::: incremental\n",
        "-   $\\beta_0$ is the mean response (mpg) for American cars.\n",
        "-   $\\beta_1$ is the amount of increase in the mean response when changing from an American to a European car.\n",
        "-   $\\beta_2$ is the amount of increase in the mean response when changing from an American to a Japanese car.\n",
        "-   $\\epsilon_i$’s follow the same assumptions as before.\n",
        ":::\n",
        "\n",
        "## \n",
        "\n",
        "Alternatively, we can write the regression model as:\n",
        "\n",
        "$$Y_i= \\beta_0+\\beta_1 d_{1i} +\\beta_2 d_{2i} +\\epsilon_i = \\begin{cases}\n",
        "\\beta_0+\\beta_1 +\\epsilon_i \\text{ if car is European}\\\\\n",
        "\\beta_0+\\beta_2 +\\epsilon_i \\text{ if car is Japanese} \\\\\n",
        "\\beta_0 +\\epsilon_i\\;\\;\\;\\;\\;\\;\\ \\text{ if car is American} \n",
        "\\end{cases}$$\n",
        "\n",
        "Given this model representation:\n",
        "\n",
        "-   $\\beta_0$ is the mean mpg for American cars,\n",
        "-   $\\beta_1$ is the difference in the mean mpg between European and American cars, and\n",
        "-   $\\beta_2$ is the difference in the mean mpg between Japanese and American cars.\n",
        "\n",
        "## In Python\n",
        "\n",
        "</br>\n",
        "\n",
        "We follow three steps to fit a linear model with a categorical predictor. First, we compute the dummy variables.\n"
      ],
      "id": "f3f558ef"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: false\n",
        "\n",
        "# Create linear regression object\n",
        "dummy_data = pd.get_dummies(auto_data['origin'], drop_first = True, \n",
        "                            dtype = 'int')\n",
        "dummy_data.head()"
      ],
      "id": "6417e5e5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we construct the matrix of predictors with the intercept.\n"
      ],
      "id": "bb9eca02"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: false\n",
        "\n",
        "# Create linear regression object\n",
        "dummy_X_train = sm.add_constant(dummy_data)\n",
        "dummy_X_train"
      ],
      "id": "4f45be4a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \n",
        "\n",
        "</br></br>\n",
        "\n",
        "Finally, we fit the model using `OLS()` and `fit()` from **statsmodels**.\n"
      ],
      "id": "dc1ff75a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "# Create linear regression object\n",
        "regr = sm.OLS(auto_data['mpg'], dummy_X_train)\n",
        "\n",
        "# Train the model using the training sets\n",
        "linear_model = regr.fit()\n",
        "\n",
        "# Summary of the models.\n",
        "summary_model = linear_model.summary()"
      ],
      "id": "aee6cf2d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \n",
        "\n",
        "</br>\n"
      ],
      "id": "4ad4977a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "print(summary_model)"
      ],
      "id": "00a253d8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis of covariance\n",
        "\n",
        "Models that mix categorical and numerical predictors are sometimes referred to as analysis of covariance (ANCOVA) models.\n",
        "\n",
        "**Example** (cont): Consider the predictor weight ($X$).\n",
        "\n",
        "$$Y_i= \\beta_0+\\beta_1 d_{1i} +\\beta_2 d_{2i} + \\beta_3 X_{i} +\\epsilon_i,$$\n",
        "\n",
        "where $X_i$ denotes the i-th observed value of weight and $\\beta_3$ is the coefficient of this predictor.\n",
        "\n",
        "## ANCOVA model\n",
        "\n",
        "The components of the ANCOVA model are individual functions of the coefficients.\n",
        "\n",
        "To gain insight into the model, we write it as follows:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "Y_i &= \\beta_0+\\beta_1 d_{1i} +\\beta_2 d_{2i} + \\beta_3 X_{i} +\\epsilon_i \\\\ &= \\begin{cases}\n",
        "(\\beta_0+\\beta_1)  + \\beta_3 X_{i} +\\epsilon_i \\text{ if car is European} \\\\\n",
        "(\\beta_0+\\beta_2) + \\beta_3 X_{i} +\\epsilon_i \\text{ if car is Japanese} \\\\\n",
        "\\beta_0 + \\beta_3 X_{i} +\\epsilon_i\\;\\;\\;\\;\\;\\;\\;\\;\\  \\text{ if car is American}\n",
        "\\end{cases}.\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Note that these models have different intercepts but the same slope.\n",
        "\n",
        "## \n",
        "\n",
        "</br></br></br>\n",
        "\n",
        "-   To estimate $\\beta_0$, $\\beta_1$, $\\beta_2$ and $\\beta_3$, we use least squares. To estimate $\\sigma^2$, we use the mean squared error (MSE).\n",
        "-   We could make individual inferences on $\\beta_1$ and $\\beta_2$ using t-tests and confidence intervals.\n",
        "-   However, better tests are possible such as overall and partial F-tests (not discussed here).\n",
        "\n",
        "## In Python\n",
        "\n",
        "</br>\n",
        "\n",
        "To fit an ANCOVA model, we use similar steps as before. The only extra step is to concatenate the data with the dummy variables and the numerical predictor using the function `concat()` from **pandas**.\n"
      ],
      "id": "fce01d6c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: false\n",
        "\n",
        "# Concatenate the two data sets.\n",
        "X_train = pd.concat([dummy_X_train, auto_data['weight']], axis = 1)\n",
        "\n",
        "# Create linear regression object\n",
        "regr = sm.OLS(auto_data['mpg'], X_train)\n",
        "\n",
        "# Train the model using the training sets\n",
        "ancova_model = regr.fit()"
      ],
      "id": "62e22a1b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model summary\n"
      ],
      "id": "13eb40d3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "# Summary of the models.\n",
        "summary_ancova = ancova_model.summary()\n",
        "print(summary_ancova)"
      ],
      "id": "85e2d192",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Linear models with standardized predictors\n",
        "\n",
        "## Standardization\n",
        "\n",
        "</br></br>\n",
        "\n",
        "::: incremental\n",
        "-   Standardization refers to centering and scaling each numeric predictor individually.\n",
        "\n",
        "-   To center a predictor variable, the average predictor value is subtracted from all the values.\n",
        "\n",
        "-   To scale a predictor, each of its value is divided by its standard deviation.\n",
        ":::\n",
        "\n",
        "## \n",
        "\n",
        "</br></br>\n",
        "\n",
        "In mathematical terms, we standardize a predictor $X$ as:\n",
        "\n",
        "$${\\color{blue} \\tilde{X}_{i}} = \\frac{{ X_{i} - \\bar{X}}}{ \\sqrt{\\frac{1}{n -1} \\sum_{i=1}^{n} (X_{i} - \\bar{X})^2 }} \\text{ with } \\bar{X} = \\sum_{i=1}^n \\frac{X_i}{n}.$$\n",
        "\n",
        "The average value of $\\tilde{X}$ is 0.\n",
        "\n",
        "The standard deviation of $\\tilde{X}$ is 1.\n",
        "\n",
        "## Benefits and limitations\n",
        "\n",
        "</br>\n",
        "\n",
        "Benefits:\n",
        "\n",
        "-   All quantitative predictors are on the same scale.\n",
        "-   Size and importance of linear regression coefficients can be compared easily.\n",
        "\n",
        ". . .\n",
        "\n",
        "Limitations:\n",
        "\n",
        "-   The interpretation of the coefficients is affected.\n",
        "\n",
        "## Interpretation\n",
        "\n",
        "$$Y = \\beta_0 + \\beta_1 \\tilde{X}_1 + \\beta_2 \\tilde{X}_2 + \\cdots + \\beta_p \\tilde{X}_p + \\epsilon,$$\n",
        "\n",
        "where $\\tilde{X}_i$ is the standardized version of the predictor $X_i$.\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "::: incremental\n",
        "-   $\\beta_0$ is the mean response when all [**original**]{style=\"color:darkblue;\"} predictors $X_1, X_2, \\ldots, X_p$ are set to their average value.\n",
        "-   $\\beta_j$ is the amount of increase in the mean response by an increase of 1 standard deviation in the [**original**]{style=\"color:darkblue;\"} predictor $X_j,$ *when all other predictors are fixed to an arbitrary value*.\n",
        ":::\n",
        "\n",
        "## Example 2\n",
        "\n",
        "</br>\n",
        "\n",
        "The yield of a chemical process ($Y$) is related to the concentration of the reactant ($X_1$) and the operating temperature ($X_2$).\n",
        "\n",
        "</br>\n",
        "\n",
        "An experiment was conducted to study the effect between these factors on the yield.\n",
        "\n",
        "</br>\n",
        "\n",
        "The dataset is in the file \"catalyst.xlsx\".\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        "The units of concentration and temperature are percentages and Farenheit degrees, respectively.\n"
      ],
      "id": "d0834538"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "\n",
        "catalyst_data = pd.read_excel(\"catalyst.xlsx\")\n",
        "print(catalyst_data)"
      ],
      "id": "120404f4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Standarization in Python\n",
        "\n",
        "</br>\n",
        "\n",
        "Recall that we standardize numerical predictors using the `scaler()` function from **scikit-learn**.\n"
      ],
      "id": "a3b30e19"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "# Select predictor matrix.\n",
        "predictor_data = catalyst_data[['Concentration', 'Temperature']]\n",
        "\n",
        "# Define the scaling operator.\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Apply the scaling operator.\n",
        "Xs_training = scaler.fit_transform(predictor_data)"
      ],
      "id": "213ecdd1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \n",
        "\n",
        "Now the predictors are on the same scale\n"
      ],
      "id": "0a034891"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| code-fold: true\n",
        "#| output: true\n",
        "\n",
        "Xs_training = pd.DataFrame(Xs_training, columns = predictor_data.columns)\n",
        "Xs_training "
      ],
      "id": "6cc5fa1a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        "To fit the model, we follow the same functions as before.\n"
      ],
      "id": "d21cbea6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "# Add the intercept.\n",
        "Xs_training_int = sm.add_constant(Xs_training)\n",
        "\n",
        "# Create linear regression object.\n",
        "std_regr = sm.OLS(catalyst_data['Yield'], Xs_training)\n",
        "\n",
        "# Train the model using the training sets.\n",
        "std_linear_model = std_regr.fit()\n",
        "\n",
        "# Summary of the model.\n",
        "std_summary_model = std_linear_model.summary()"
      ],
      "id": "af53e4ee",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \n"
      ],
      "id": "cae0f5a9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "\n",
        "print(std_summary_model)"
      ],
      "id": "63e85018",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Discussion\n",
        "\n",
        "Standardization of predictors has no impact on the overall quality of the linear regression model.\n",
        "\n",
        "::: incremental\n",
        "-   $R^2$ and adjusted $R^2$ statistics are identical.\n",
        "-   Predictions are identical.\n",
        "-   Residuals do not change.\n",
        ":::\n",
        "\n",
        ". . .\n",
        "\n",
        "Standardization does not affect the correlation between two predictors. So, it has not effect on collinearity.\n",
        "\n",
        ". . .\n",
        "\n",
        "Ideally, the dummy variables for the categorical predictors are standardized too.\n",
        "\n",
        "# Remedies for faulty assumptions\n",
        "\n",
        "## Questions\n",
        "\n",
        "</br>\n",
        "\n",
        "1.  How can we estimate $\\beta_0, \\beta_1, \\ldots, \\beta_p$ and $\\sigma^2$?\n",
        "\n",
        "2.  How can we validate the model and all its assumptions?\n",
        "\n",
        "3.  How can we make inferences about $\\beta_0, \\beta_1, \\ldots, \\beta_p$?\n",
        "\n",
        "4.  [If some assumptions are not met, can we do something about it?]{style=\"color:blue;\"}\n",
        "\n",
        "5.  How can we make predictions of future responses using the multiple linear regression model?\n",
        "\n",
        "## Incorrect model\n",
        "\n",
        "</br></br>\n",
        "\n",
        "A model is incorrect if\n",
        "\n",
        "::: incremental\n",
        "-   The assumed model structure is incorrect. That is, $Y \\neq \\beta_0 + \\beta_1 X + \\epsilon$.\n",
        "-   The residuals do not have constant variance.\n",
        "-   The residuals are not independent.\n",
        ":::\n",
        "\n",
        "## Example 3\n",
        "\n",
        "Consider the fitting the following model to the `auto_data`:\n",
        "\n",
        "$$Y_i = \\beta_0 + \\beta_1 X_{i} + \\epsilon_i$$ where:\n",
        "\n",
        "-   $Y_i$ is the mpg of the i-th car.\n",
        "-   $X_i$ is the horsepower of the i-th car.\n",
        "\n",
        "We fit the model:\n"
      ],
      "id": "ab2dc0da"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "X_train = sm.add_constant(auto_data['horsepower'])\n",
        "regr = sm.OLS(auto_data['mpg'], X_train)\n",
        "linear_model = regr.fit()"
      ],
      "id": "952ca592",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Residual analysis\n"
      ],
      "id": "c61026b4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: false\n",
        "\n",
        "\n",
        "Y_pred = linear_model.fittedvalues\n",
        "residuals = linear_model.resid"
      ],
      "id": "038196c4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::::: columns\n",
        "::: {.column width=\"50%\"}"
      ],
      "id": "68d6ee95"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: true\n",
        "\n",
        "# Residual vs Fitted Values Plot\n",
        "plt.figure(figsize=(5, 5))\n",
        "sns.scatterplot(x = Y_pred, y = residuals)\n",
        "plt.axhline(y=0, color='red', linestyle='--')\n",
        "plt.title('Residuals vs Fitted Values')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.show()"
      ],
      "id": "741868af",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}"
      ],
      "id": "7324206b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: true\n",
        "\n",
        "# Residual vs Time Plot\n",
        "plt.figure(figsize=(5, 5))\n",
        "order = range(residuals.shape[0])\n",
        "sns.scatterplot(x = order, y = residuals)\n",
        "plt.axhline(y=0, color='red', linestyle='--')\n",
        "plt.title('Residuals vs Time')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Residuals')\n",
        "plt.show()"
      ],
      "id": "a1767d4c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        ":::::\n",
        "\n",
        "## Remedies\n",
        "\n",
        "[The assumed model structure is incorrect.]{style=\"color:red;\"} That is, $Y \\neq \\beta_0 + \\beta_1 X + \\epsilon$.\n",
        "\n",
        "**Remedies**: Add high powers of the predictor variable to the model or transform the response (or predictor).\n",
        "\n",
        ". . .\n",
        "\n",
        "The [residuals]{style=\"color:red;\"} of the fitted model [do not have constant variance]{style=\"color:red;\"}.\n",
        "\n",
        "**Remedies**: Transform the response or predictor variable.\n",
        "\n",
        "-   Logarithm transformation\n",
        "-   Square root transformation\n",
        "\n",
        "## Transformations\n",
        "\n",
        "Two commonly used transformations are:\n",
        "\n",
        "**Natural logarithm** (ln) $$\\ln(Y) = \\beta_0 + \\beta_1 X + \\epsilon$$ $$\\ln(Y) = \\beta_0 + \\beta_1 \\ln(X) + \\epsilon$$\n",
        "\n",
        "**Squared root** (sqrt)\n",
        "\n",
        "$$\\sqrt{Y} = \\beta_0 + \\beta_1 X + \\epsilon$$ $$\\sqrt{Y} = \\beta_0 + \\beta_1 \\sqrt{X} + \\epsilon$$\n",
        "\n",
        "## Effect of transformations\n",
        "\n",
        "</br>\n",
        "\n",
        "::: incremental\n",
        "-   In many cases, the $\\ln{(\\cdot)}$ transformation Improves the relationship between predictor and response.\n",
        "-   Produces residuals that have constant variance (variance-stabilizing transformation).\n",
        "-   The $\\sqrt{\\cdot}$ transformation provides similar benefits, except that it is useful for response variables that are counts or follow a Poisson distribution.\n",
        ":::\n",
        "\n",
        "## NumPy library\n",
        "\n",
        "::::: columns\n",
        "::: {.column width=\"60%\"}\n",
        "-   **numpy** is a powerful, open-source data manipulation and analysis library for Python\n",
        "-   It is the backbone for **scikit-learn** and **pandas**\n",
        "-   <https://numpy.org/>\n",
        ":::\n",
        "\n",
        "::: {.column width=\"40%\"}\n",
        "![](images/NumPy_logo_2020.png){fig-align=\"center\"}\n",
        ":::\n",
        ":::::\n",
        "\n",
        "Load it using:\n"
      ],
      "id": "bd5eae7f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "\n",
        "import numpy as np"
      ],
      "id": "24831ac6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Continuation of Example 3\n",
        "\n",
        "To transform the response ($Y$) using $\\ln{(Y)}$ or $\\sqrt{Y}$ we use the functions `log()` and `sqrt()`, respectively, from **numpy**.\n"
      ],
      "id": "875a83b4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: false\n",
        "\n",
        "sqrt_Y = np.sqrt( auto_data['mpg'] )\n",
        "log_Y = np.log( auto_data['mpg'] )"
      ],
      "id": "e9722cc5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's consider the logarithm transformation. The model then is:\n",
        "\n",
        "$$\\log{(Y_i)} = \\beta_0 + \\beta_1 X_i +\\epsilon_i,$$\n",
        "\n",
        "which we fit using the code below.\n"
      ],
      "id": "1437334f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: false\n",
        "\n",
        "log_regr = sm.OLS(log_Y, X_train)\n",
        "log_linear_model = log_regr.fit()"
      ],
      "id": "8b351b3f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## New residual analysis\n"
      ],
      "id": "3b7a76bf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: false\n",
        "\n",
        "Y_pred = log_linear_model.fittedvalues\n",
        "residuals = log_linear_model.resid"
      ],
      "id": "8a7ec519",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::::: columns\n",
        "::: {.column width=\"50%\"}"
      ],
      "id": "d7271f53"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: true\n",
        "\n",
        "# Residual vs Fitted Values Plot\n",
        "plt.figure(figsize=(5, 5))\n",
        "sns.scatterplot(x = Y_pred, y = residuals)\n",
        "plt.axhline(y=0, color='red', linestyle='--')\n",
        "plt.title('Residuals vs Fitted Values')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.show()"
      ],
      "id": "fa367107",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}"
      ],
      "id": "e0874a35"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: true\n",
        "\n",
        "# Residual vs Time Plot\n",
        "plt.figure(figsize=(5, 5))\n",
        "order = range(residuals.shape[0])\n",
        "sns.scatterplot(x = order, y = residuals)\n",
        "plt.axhline(y=0, color='red', linestyle='--')\n",
        "plt.title('Residuals vs Time')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Residuals')\n",
        "plt.show()"
      ],
      "id": "c02ffe9a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        ":::::\n",
        "\n",
        "## Quadratic model\n",
        "\n",
        "</br>\n",
        "\n",
        "Although there is an improvement in the **Residuals vs Fitted Values** plot when using the logarithm. The two plots suggests that we are missing a term in the model.\n",
        "\n",
        "</br>\n",
        "\n",
        ". . .\n",
        "\n",
        "In fact, a better model for the data is a quadratic model with the logarithm of the response:\n",
        "\n",
        "$$\\log{(Y_i)} = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 +\\epsilon_i.$$\n",
        "\n",
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        "To fit this model to the data, we construct a new predictor matrix.\n"
      ],
      "id": "389f3af8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "X_quad = pd.concat([X_train, auto_data['horsepower']**2], axis = 1)"
      ],
      "id": "9472cb36",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we fit the model as before.\n"
      ],
      "id": "3d08e85d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "quad_regr = sm.OLS(log_Y, X_quad)\n",
        "quadratic_model = quad_regr.fit()"
      ],
      "id": "57ef34a1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And calculate the residuals and predicted values.\n"
      ],
      "id": "6175eebf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: false\n",
        "\n",
        "Y_pred = quadratic_model.fittedvalues\n",
        "residuals = quadratic_model.resid"
      ],
      "id": "c836b35a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Residual analysis of quadratic model\n",
        "\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}"
      ],
      "id": "b5aaf41e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "#| code-fold: true\n",
        "\n",
        "# Residual vs Fitted Values Plot\n",
        "plt.figure(figsize=(5, 5))\n",
        "sns.scatterplot(x = Y_pred, y = residuals)\n",
        "plt.axhline(y=0, color='red', linestyle='--')\n",
        "plt.title('Residuals vs Fitted Values')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.show()"
      ],
      "id": "c10f9c57",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}"
      ],
      "id": "3cd2fdfd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "#| code-fold: true\n",
        "\n",
        "# Residual vs Time Plot\n",
        "plt.figure(figsize=(5, 5))\n",
        "order = range(residuals.shape[0])\n",
        "sns.scatterplot(x = order, y = residuals)\n",
        "plt.axhline(y=0, color='red', linestyle='--')\n",
        "plt.title('Residuals vs Time')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Residuals')\n",
        "plt.show()"
      ],
      "id": "aa37375e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        ":::::\n",
        "\n",
        "## Conclusions\n",
        "\n",
        "::: incremental\n",
        "-   Transformations may help us to develop models that better approximate the data. However, the interpretation of these models may be too complex. For instance, how can you interpret $\\beta_1$ in $\\log{(Y)} = \\beta_0 + \\beta_1\\ X + \\epsilon$?\n",
        "\n",
        "-   Therefore, transformations are more useful to build good predictive models. That is, models whose goal is to give accurate predictions of future observations.\n",
        "\n",
        "-   Note that, we need to transform back our response predictions to the original scale. For example, if $Y' = \\ln{Y}$ is the transformed response, then our final prediction is $Y^\\star = e^{{Y'}^\\star}$.\n",
        ":::\n",
        "\n",
        "# Predicting future observations\n",
        "\n",
        "## Questions\n",
        "\n",
        "</br>\n",
        "\n",
        "1.  How can we estimate $\\beta_0, \\beta_1, \\ldots, \\beta_p$ and $\\sigma^2$?\n",
        "\n",
        "2.  How can we validate the model and all its assumptions?\n",
        "\n",
        "3.  How can we make inferences about $\\beta_0, \\beta_1, \\ldots, \\beta_p$?\n",
        "\n",
        "4.  If some assumptions are not met, can we do something about it?\n",
        "\n",
        "5.  [How can we make predictions of future responses using the multiple linear regression model?]{style=\"color:blue;\"}\n",
        "\n",
        "## Recall that ...\n",
        "\n",
        "In data science, we assume that\n",
        "\n",
        "$$Y = f(\\boldsymbol{X}) + \\epsilon$$\n",
        "\n",
        "where $f(\\boldsymbol{X})$ represents the true relationship between $\\boldsymbol{X} = (X_1, X_2, \\ldots, X_p)$ and $Y$, which is unknown and very complex!\n",
        "\n",
        "In our analysis, we further assume that\n",
        "\n",
        "$$f(\\boldsymbol{X}) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p.$$\n",
        "\n",
        "## Two datasets\n",
        "\n",
        "</br></br>\n",
        "\n",
        "The application of data science models needs two data sets:\n",
        "\n",
        "::: incremental\n",
        "-   [**Training data**]{style=\"color:blue;\"} is data that we use to train or construct the estimated model $\\hat{f}(\\boldsymbol{X}) = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_1 + \\cdots + \\hat{\\beta}_p X_p$.\n",
        "\n",
        "-   [**Test data**]{style=\"color:green;\"} is data that we use to evaluate the predictive performance of $\\hat{f}(\\boldsymbol{X})$ only.\n",
        ":::\n",
        "\n",
        "## \n",
        "\n",
        "::::: columns\n",
        "::: {.column width=\"30%\"}\n",
        "![](images/training.png){width=\"256\"}\n",
        ":::\n",
        "\n",
        "::: {.column width=\"70%\"}\n",
        "</br>\n",
        "\n",
        "A random sample of $n$ observations.\n",
        "\n",
        "Use it to **construct** $\\hat{f}(\\boldsymbol{X})$.\n",
        ":::\n",
        ":::::\n",
        "\n",
        "::::: columns\n",
        "::: {.column width=\"30%\"}\n",
        "![](images/test.png){width=\"262\"}\n",
        ":::\n",
        "\n",
        "::: {.column width=\"70%\"}\n",
        "Another random sample of $n_t$ observations, which is independent of the training data.\n",
        "\n",
        "Use it to **evaluate** $\\hat{f}(\\boldsymbol{X})$.\n",
        ":::\n",
        ":::::\n",
        "\n",
        "## Validation Dataset\n",
        "\n",
        "In many practical situations, a test dataset is not available. To overcome this issue, we use a [**validation dataset**]{style=\"color:orange;\"}.\n",
        "\n",
        "![](images/validation.png){fig-align=\"center\" width=\"645\"}\n",
        "\n",
        ". . .\n",
        "\n",
        "**Idea**: Apply model to your [**validation dataset**]{style=\"color:orange;\"} to mimic what will happen when you apply it to test dataset.\n",
        "\n",
        "## Example 4\n",
        "\n",
        "</br>\n",
        "\n",
        "The \"BostonHousing.xlsx\" contains data collected by the US Bureau of the Census concerning housing in the area of Boston, Massachusetts. The dataset includes data on 506 census housing tracts in the Boston area in 1970s.\n",
        "\n",
        "The goal is to predict the median house price in new tracts based on information such as crime rate, pollution, and number of rooms.\n",
        "\n",
        "The response is the median value of owner-occupied homes in \\$1000s, contained in the column `MEDV`.\n",
        "\n",
        "## The predictors\n",
        "\n",
        "::: {style=\"font-size: 70%;\"}\n",
        "-   `CRIM`: per capita crime rate by town.\n",
        "-   `ZN`: proportion of residential land zoned for lots over 25,000 sq.ft.\n",
        "-   `INDUS`: proportion of non-retail business acres per town.\n",
        "-   `CHAS`: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n",
        "-   `NOX`: nitrogen oxides concentration (parts per 10 million).\n",
        "-   `RM`: average number of rooms per dwelling.\n",
        "-   `AGE`: proportion of owner-occupied units built prior to 1940.\n",
        "-   `DIS`: weighted mean of distances to five Boston employment centers\n",
        "-   `RAD`: index of accessibility to radial highways.\n",
        "-   `TAX`: full-value property-tax rate per \\$10,000.\n",
        "-   `PTRATIO`: pupil-teacher ratio by town.\n",
        "-   `LSTAT`: lower status of the population (percent).\n",
        ":::\n",
        "\n",
        "## Read the dataset\n",
        "\n",
        "We read the dataset and set the variable `CHAS` as categorical.\n"
      ],
      "id": "bbede209"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "# Load Excel file (make sure the file is in your Colab)\n",
        "Boston_data = pd.read_excel('BostonHousing.xlsx')\n",
        "\n",
        "# Drop the categorical variable.\n",
        "Boston_data['CHAS'] = pd.Categorical(Boston_data['CHAS'])\n",
        "\n",
        "# Preview the dataset.\n",
        "Boston_data.head(3)"
      ],
      "id": "29fd6a99",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How do we generate validation data?\n",
        "\n",
        "We split the current dataset into a training and a validation dataset. To this end, we use the function `train_test_split()` from **scikit-learn**.\n"
      ],
      "id": "3f986473"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "# Set full matrix of predictors.\n",
        "X_full = Boston_data.drop(columns = ['MEDV']) \n",
        "\n",
        "# Set full matrix of responses.\n",
        "Y_full = Boston_data['MEDV']\n",
        "\n",
        "# Split the dataset into training and validation.\n",
        "X_train, X_valid, Y_train, Y_valid = train_test_split(X_full, Y_full, \n",
        "                                                      test_size=0.3)"
      ],
      "id": "429cae2e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The parameter `test_size` sets the portion of the dataset that will go to the validation set.\n",
        "\n",
        "## \n",
        "\n",
        "</br></br>\n",
        "\n",
        "-   The function makes a clever partition of the data using the *empirical* distribution of the response.\n",
        "\n",
        "-   Technically, it splits the data so that the distribution of the response under the training and validation sets is similar.\n",
        "\n",
        "-   Usually, the proportion of the dataset that goes to the validation set is 20% or 30%.\n",
        "\n",
        "## Fit a model using training data\n",
        "\n",
        "</br>\n",
        "\n",
        "We fit a multiple linear regression model to predict the `MEDV` in terms of the 12 predictors using the functions `OLS()` and `fit()` from **statsmodels**.\n"
      ],
      "id": "bdad14a8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| output: true\n",
        "\n",
        "# Add intercept.\n",
        "Boston_X_train = sm.add_constant(X_train)\n",
        "\n",
        "# Create linear regression object\n",
        "regr = sm.OLS(Y_train, Boston_X_train)\n",
        "\n",
        "# Train the model using the training sets\n",
        "linear_model = regr.fit()"
      ],
      "id": "28ea13de",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Brief Residual Analysis\n",
        "\n",
        "We evaluate the model using a \"Residual versus Fitted Values\" plot. The plot does not show concerning patterns in the residuals. So, we assume that the model satisfices the assumption of constant variance.\n"
      ],
      "id": "6d696293"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: true\n",
        "#| code-fold: true\n",
        "#| fig-align: center\n",
        "\n",
        "# Residual vs Fitted Values Plot\n",
        "plt.figure(figsize=(7, 4))\n",
        "sns.scatterplot(x = linear_model.fittedvalues, y = linear_model.resid)\n",
        "plt.axhline(y=0, color='red', linestyle='--')\n",
        "plt.title('Residuals vs Fitted Values')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.show()"
      ],
      "id": "216b300d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validation Mean Squared Error\n",
        "\n",
        "</br>\n",
        "\n",
        "When the response is numeric, the most common evaluation metric is the validation [**Mean Squared Error**]{style=\"color:orange;\"} ([**MSE**]{style=\"color:orange;\"}):\n",
        "\n",
        "$$\n",
        "\\frac{1}{n_{v}} \\sum_{i=1}^{n_{v}} \\left( Y_i - \\hat{f}(\\boldsymbol{X}_i) \\right)^2\n",
        "$$\n",
        "\n",
        "where $(Y_1, \\boldsymbol{X}_1), \\ldots, (Y_{n_{v} }, \\boldsymbol{X}_{n_{v}} )$ are the $n_{v}$ observations in the validation dataset, and $\\hat{f}(\\boldsymbol{X}_i)$ is the model prediction of the i-th response.\n",
        "\n",
        "## \n",
        "\n",
        "Another useful metric is the validation [**Root Mean Squared Error**]{style=\"color:orange;\"} ([**RMSE**]{style=\"color:orange;\"}):\n",
        "\n",
        "$$\n",
        "\\sqrt{\\frac{1}{n_{v}} \\sum_{i=1}^{n_{v}} \\left( Y_i - \\hat{f}(\\boldsymbol{X}_i) \\right)^2}\n",
        "$$\n",
        "\n",
        "Benefits:\n",
        "\n",
        "::: {style=\"font-size: 90%;\"}\n",
        "-   The RMSE is in the same units as the response.\n",
        "-   The RMSE value is interpreted as either how far (on average) the residuals are from zero.\n",
        "-   It can also be interpreted as the average distance between the observed response values and the model predictions.\n",
        ":::\n",
        "\n",
        "## In Python\n",
        "\n",
        "</br>\n",
        "\n",
        "We first compute the predictions of our model on the validation dataset. That is, we use the values of the predictors in this dataset and use it as input to our model. Our model then computes the prediction of the response for each combination of values of the predictors.\n"
      ],
      "id": "69664471"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "\n",
        "# Add constant to the predictor matrix from the validation set.\n",
        "Boston_X_val = sm.add_constant(X_valid)\n",
        "\n",
        "# Predict responses using validation data.\n",
        "predicted_medv_val = linear_model.predict(Boston_X_val)"
      ],
      "id": "f1c91a8a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \n",
        "\n",
        "</br>\n",
        "\n",
        "We compute the validation RMSE by first computing the validation MSE using a function with the same name of **scikit-learn**.\n"
      ],
      "id": "7045d145"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "\n",
        "mse = mean_squared_error(Y_valid, predicted_medv_val)\n",
        "rmse = mse**(1/2)\n",
        "print( round(rmse, 3) )"
      ],
      "id": "799972a8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The lower the validation RMSE, the more accurate our model.\n",
        "\n",
        "[*Interpretation*: On average, our predictions are off by $\\pm$ 4,465 dollars.]{style=\"color:darkblue;\"}\n",
        "\n",
        "# [Return to main page](https://alanrvazquez.github.io/TEC-IN1002B-Website/)"
      ],
      "id": "254608ee"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}