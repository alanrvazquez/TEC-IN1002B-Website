---
title: "Data preprocessing"
subtitle: "IN1002B: Introduction to Data Science Projects"
author: 
  - name: Alan R. Vazquez
    affiliations:
      - name: Department of Industrial Engineering
format: 
  revealjs:
    chalkboard: false
    multiplex: true
    footer: "Tecnologico de Monterrey"
    logo: IN1002b_logo.png
    css: style.css
    slide-number: True
    html-math-method: mathjax
editor: visual
jupyter: python3
---

## Agenda

</br>

1.  Introduction
2.  Training, validation, and test datasets
3.  Dealing with missing values
4.  Transforming predictors
5.  Standarization
6.  Dimension reduction


# Introduction

## Data preprocessing

[**Data pre-processing**]{style="color:lightblue;"} techniques generally refer to the addition, deletion, or transformation of data.

</br>

> It can make or break a modelâ€™s predictive ability.

</br>

For example, linear regression models (to be discussed later) are relatively insensitive to the characteristics of the predictor data, but advanced methods like K-nearest neighbors, principal component regression, and LASSO are not.

## 

</br>

We will review some common strategies for removing predictors from the data, without considering how they might be related to the response.

. . .

In particular, we will review:

::: incremental
-   Dealing with missing values.
-   Transforming categorical predictors.
-   Filtering: Removing non-informative or correlated predictors.
-   Standardization.
:::

## scikit-learn library

-   **scikit-learn** is a robust and popular library for machine learning in Python
-   It provides simple, efficient tools for data mining and data analysis
-   It is built on top of libraries such as **NumPy**, **SciPy**, and **Matplotlib**
-   <https://scikit-learn.org/stable/>

![](images/scikitlearn.png){fig-align="center"}

## 

</br>

Let's import **scikit-learn** into Python together with the other relevant libraries.

```{python}
#| echo: true

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import VarianceThreshold
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import power_transform, StandardScaler 
```

We will not use all the functions from the **scikit-learn** library. Instead, we will use specific functions from the sub-libraries **preprocessing**, **feature_selection**, **model_selection** and **impute**.

# Training, validation, and test datasets

## Recall that ...

</br></br>

In data science, we assume that

$$Y = f(\boldsymbol{X}) + \epsilon$$

where $f(\boldsymbol{X})$ represents the true relationship between $\boldsymbol{X} = (X_1, X_2, \ldots, X_p)$ and $Y$.

- $f(\boldsymbol{X})$ is unknown and very complex!


## Two datasets

</br></br>

The application of data science models needs two data sets:

::: incremental
-   [**Training data**]{style="color:blue;"} is data that we use to train or construct the estimated function $\hat{f}(\boldsymbol{X})$.

-   [**Test data**]{style="color:green;"} is data that we use to evaluate the predictive performance of $\hat{f}(\boldsymbol{X})$ only.
:::

## 

::::: columns
::: {.column width="30%"}
![](images/training.png){width="256"}
:::

::: {.column width="70%"}
</br>

A random sample of $n$ observations.

Use it to **construct** $\hat{f}(\boldsymbol{X})$.
:::
:::::

::::: columns
::: {.column width="30%"}
![](images/test.png){width="262"}
:::

::: {.column width="70%"}
Another random sample of $n_t$ observations, which is independent of the training data.

Use it to **evaluate** $\hat{f}(\boldsymbol{X})$.
:::
:::::

## Validation Dataset

In many practical situations, a test dataset is not available. To overcome this issue, we use a [**validation dataset**]{style="color:orange;"}.

![](images/validation.png){fig-align="center" width="645"}

. . .

**Idea**: Apply model to your [**validation dataset**]{style="color:orange;"} to mimic what will happen when you apply it to test dataset.

## Example 1

</br>

The "BostonHousing.xlsx" contains data collected by the US Bureau of the Census concerning housing in the area of Boston, Massachusetts. The dataset includes data on 506 census housing tracts in the Boston area in 1970s.

The goal is to predict the median house price in new tracts based on information such as crime rate, pollution, and number of rooms.

The response is the median value of owner-occupied homes in \$1000s, contained in the column `MEDV`.

## The predictors

::: {style="font-size: 70%;"}
-   `CRIM`: per capita crime rate by town.
-   `ZN`: proportion of residential land zoned for lots over 25,000 sq.ft.
-   `INDUS`: proportion of non-retail business acres per town.
-   `CHAS`: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
-   `NOX`: nitrogen oxides concentration (parts per 10 million).
-   `RM`: average number of rooms per dwelling.
-   `AGE`: proportion of owner-occupied units built prior to 1940.
-   `DIS`: weighted mean of distances to five Boston employment centers
-   `RAD`: index of accessibility to radial highways.
-   `TAX`: full-value property-tax rate per \$10,000.
-   `PTRATIO`: pupil-teacher ratio by town.
-   `LSTAT`: lower status of the population (percent).
:::

## Read the dataset

We read the dataset and set the variable `CHAS` as categorical.

```{python}
#| echo: true
#| output: true

# Load Excel file (make sure the file is in your Colab)
Boston_data = pd.read_excel('BostonHousing.xlsx')

# Drop the categorical variable.
Boston_data['CHAS'] = pd.Categorical(Boston_data['CHAS'])

# Preview the dataset.
Boston_data.head(3)
```

## How do we generate validation data?

We split the current dataset into a training and a validation dataset. To this end, we use the function `train_test_split()` from **scikit-learn**.

```{python}
#| echo: true
#| output: true

# Set full matrix of predictors.
X_full = Boston_data.drop(columns = ['MEDV']) 

# Set full matrix of responses.
Y_full = Boston_data['MEDV']

# Split the dataset into training and validation.
X_train, X_valid, Y_train, Y_valid = train_test_split(X_full, Y_full, 
                                                      test_size=0.3)
```

The parameter `test_size` sets the portion of the dataset that will go to the validation set.

## 

</br></br>

-   The function makes a clever partition of the data using the *empirical* distribution of the response.

-   Technically, it splits the data so that the distribution of the response under the training and validation sets is similar.

-   Usually, the proportion of the dataset that goes to the validation set is 20% or 30%.


##

The predictors and response in the training dataset are in the objects `X_train` and `Y_train`, respectively. We can compile these objects into a single dataset using the function `.concat` from **pandas**.

```{python}
#| echo: true
#| output: true


# Split the dataset into training and validation.
training_dataset = pd.concat([X_train, Y_train])
training_dataset.head(4)
```


##

Equivalently, the predictors and response in the validation dataset are in the objects `X_valid` and `Y_valid`, respectively. 

```{python}
#| echo: true
#| output: true


# Split the dataset into training and validation.
validation_dataset = pd.concat([X_valid, Y_valid])
validation_dataset.head(4)
```


## Work on your training dataset

After we have partitioned the data, we **work on the** [**training data**]{style="color:blue;"} to develop our predictive pipeline.

The pipeline has two main steps:

1. Data preprocessing.
2. Model development.

We will now discuss preprocessing techniques applied to the training dataset. Note that all the techniques applied to this dataset will also be applied to the [**validation dataset**]{style="color:orange;"} and [**test dataset**]{style="color:green;"} to prepare it for your model. 

# Dealing with missing values

## Missing values

In many cases, some predictors have no values for a given observation. It is important to understand why the values are missing.

</br>

. . .

There four main types of missing data:

::: incremental
-   *Structurally missing* data is data that is missing for a logical reason or because it should not exist.

-   *Missing completely at random* assumes that the fact that the data is missing is unrelated to the other information in the data.
:::

## 

</br></br>

-   *Missing at random* assumes that we can predict the value that is missing based on the other available data.

-   *Missing not at random* assumes that there is a mechanism that generates the missing values, which may include observed and unobserved predictors.

## 

For large data sets, removal of observations based on missing values is not a problem, assuming that the type of missing data is completely at random.

</br>

In a smaller data sets, there is a high price in removing observations. To overcome this issue, we can use methods of imputation, which try to estimate the missing values of a predictor variable using the other predictors' values.

</br>

Here, we will introduce three simple methods for imputing missing values in categorical and numerical variables.

## Example 2

</br>

Let's use the penguins dataset available in the file `penguins.xlsx`.

```{python}
#| echo: true

# Load the Excel file into a pandas DataFrame.
penguins_data = pd.read_excel("penguins.xlsx")

# Set categorical variables.
penguins_data['sex'] = pd.Categorical(penguins_data['sex'])
penguins_data['species'] = pd.Categorical(penguins_data['species'])
penguins_data['island'] = pd.Categorical(penguins_data['island'])
```

## 

</br>

The dataset has some missing values denoted as `NaN`.

```{python}
#| echo: false

# Print the first 4 rows of the dataset.
penguins_data.head(5)
```

## 

</br>

One way to know what variables have missing data is using the function `info()` from **pandas**.

```{python}
#| echo: true

penguins_data.info()
```

## 

In the output of the function "non-null" refers to the number of entries in a column that have actual values. That is, the number of entries where there are not `NaN`.

```{python}
#| echo: false

penguins_data.info()
```

The value next to "RangeIndex" is the number of observations in the dataset.

## 

</br>

Alternatively, we can use the function `isnull()` together with `sum()` to determine the number of missing values for each column in the dataset.

</br>

```{python}
#| echo: true

missing_values = penguins_data.isnull().sum()
missing_values
```

## Removing missing values

If we want to remove all rows in the dataset that have at least one missing value, we use the function `dropna()`.

```{python}
#| echo: true
#| output: true

complete_penguins = penguins_data.dropna()
complete_penguins.head()
```

## Imputation using the mean

We can impute the missing values of a numeric variable using the **mean** or **median** of its available values. For example, consider the variable `bill_length_mm` that has missing values.

```{python}
#| echo: true
#| output: true

penguins_data['bill_length_mm'].head()
```

In **scikit-learn**, we use the function `SimpleImputer()` to define the method of imputation of missing values.

## 

</br>

Using `SimpleImputer()`, we set the method to imput missing values using the **mean**. We also use the function `fit_transform()` to apply the imputation method to the variable.

```{python}
#| echo: true
#| output: true

# Imputation for numerical variables (using the mean)
num_imputer = SimpleImputer(strategy='mean')

# Replace the original variable with new version.
penguins_data['bill_length_mm'] = num_imputer.fit_transform(penguins_data[ ['bill_length_mm'] ] )
```

## 

After imputation, the dataset looks like this.

```{python}
#| echo: true
#| output: true

penguins_data.head()
```

Now, `bill_length_mm` has complete values.

## 

To imput the missing values using the **median**, we simply set this method in `SimpleImputer()`. For example, let's imput the missing values of `bill_depth_mm`.

```{python}
#| echo: true
#| output: true

# Imputation for numerical variables (using the mean)
num_imputer = SimpleImputer(strategy = 'median')

# Replace the original variable with new version.
penguins_data['bill_depth_mm'] = num_imputer.fit_transform(penguins_data[ ['bill_depth_mm'] ] )

# Show the first 4 rows of the updated dataset.
penguins_data.head(3)
```

## Using the mean or the median?

</br></br>

We use the **sample mean** when the data distribution is roughly symmetrical.

-   Pros: Simple and easy to implement.

-   Cons: Sensitive to outliers; may not be accurate for skewed distributions

## 

</br></br>

We use the **sample median** when the data is skewed (e.g., incomes, prices).

-   Pros: Less sensitive to outliers; robust for skewed distributions.

-   Cons: May reduce variability in the data.

## Imputation method for a categorical variable

If a categorical variable has missing values, we can use the **most frequent** of the available values to replace the missing values. To this end, we use similar commands as before.

For example, let's imput the missing values of `sex` using this strategy.

```{python}
#| echo: true
#| output: true

# Imputation for categorical variables (using the most frequent value)
cat_imputer = SimpleImputer(strategy = 'most_frequent')

# Apply imputation strategy for categorical variables.
penguins_data['sex'] = cat_imputer.fit_transform(penguins_data[ ['sex'] ]).ravel()
```

# Transforming predictors

## Categorical predictors

A [**categorical predictor**]{style="color:darkgreen;"} takes on values that are nominal categories.

. . .

For example:

-   Type of school: Public or private.

-   Treatment: New or placebo.

-   Grade: Passed or not passed.

. . .

The categories can be represented by names, labels or even numbers. Their use in regression requires **dummy variables**, which are numeric variables.

## Dummy variables

</br></br>

> The traditional choice for a dummy variable is a binary variable, which can only take the values 0 and 1.

</br>

***Initially, a categorical variable with*** $k$ ***categories requires*** $k$ ***dummy variables.***

## Example 2

A market analyst is studying quality characteristics of cars. Specifically, the analyst is investigating the miles per gallon (mpg) of cars can be predicted using:

-   $X_1:$ cylinders. Number of cylinders between 4 and 8
-   $X_2:$ displacement. Engine displacement (cu. inches)
-   $X_3:$ horsepower. Engine horsepower
-   $X_4:$ weight. Vehicle weight (lbs.)
-   $X_5:$ acceleration. Time to accelerate from 0 to 60 mph (sec.)
-   $X_6:$ origin. Origin of car (American, European, Japanese)

## 

</br></br>

The dataset is in the file "auto.xlsx". Let's read the data using **pandas**.

```{python}
#| echo: true

# Load the Excel file into a pandas DataFrame.
auto_data = pd.read_excel("auto.xlsx")

# Set categorical variables.
auto_data['origin'] = pd.Categorical(auto_data['origin'])
```

## 

```{python}
# Print the first 4 rows of the dataset.
auto_data.head(4)
```

## Dealing with missing values

</br>

The dataset has missing values. In this example, we remove each row with at least one missing value.

</br>

```{python}
#| echo: true

# Remove rows with missing values.
complete_Auto = auto_data.dropna()
```

## Example 2 (cont.)

Categorical predictor: Origin of a car. Three categories: American, European and Japanese.

Initially, 3 dummy variables are required:

$$d_1 =
\begin{cases}
1 \text{ if observation is from an American car}\\
0 \text{ otherwise}
\end{cases}$$ $$d_2 =
\begin{cases}
1 \text{ if observation is from an European car}\\
0 \text{ otherwise}
\end{cases}$$ $$d_3 =
\begin{cases}
1 \text{ if observation is from a Japanese car}\\
0 \text{ otherwise}
\end{cases}$$

## 

The variable Origin would then be replaced by the three dummy variables

| Origin ($X$) | $d_1$    | $d_2$    | $d_3$    |
|--------------|----------|----------|----------|
| American     | 1        | 0        | 0        |
| American     | 1        | 0        | 0        |
| European     | 0        | 1        | 0        |
| European     | 0        | 1        | 0        |
| American     | 1        | 0        | 0        |
| Japanese     | 0        | 0        | 1        |
| $\vdots$     | $\vdots$ | $\vdots$ | $\vdots$ |

## A drawback

</br>

::: incremental
-   A drawback with the initial dummy variables is that they are [**linearly dependent**]{style="color:red;"}. That is, $d_1 + d_2 + d_3 = 1$.

-   Therefore, we can determine the value of $d_1 = 1- d_2 - d_3.$

-   Predictive models such as linear regression are sensitive to linear dependencies among predictors.

-   **The solution is to drop one of the predictor**, say, $d_1$, from the data.
:::

## 

The variable Origin would then be replaced by the three dummy variables.

| Origin ($X$) | $d_2$    | $d_3$    |
|--------------|----------|----------|
| American     | 0        | 0        |
| American     | 0        | 0        |
| European     | 1        | 0        |
| European     | 1        | 0        |
| American     | 0        | 0        |
| Japanese     | 0        | 1        |
| $\vdots$     | $\vdots$ | $\vdots$ |

## Dummy variables in Python

</br>

We can get the dummy variables of a categorical variable using the function `pd.get_dummies()` from **pandas**.

The input of the function is the categorical variable.

The function has an extra argument called `drop_first` to drop the first dummy variable. It also has the argument `dtype` to show the values as integers.

```{python}
#| echo: true

dummy_data = pd.get_dummies(complete_Auto['origin'], drop_first = True, 
                            dtype = 'int')
```

## 

</br></br>

```{python}
#| echo: true

dummy_data.head()
```

## 

Now, to add the dummy variables to the dataset, we use the function `concat()` from **pandas**.

```{python}
#| echo: true

augmented_auto = pd.concat([complete_Auto, dummy_data], axis = 1)
augmented_auto.head()
```

## Transforming numerical predictors

</br>

A common problem with a predictor is that it may have a *skewed distribution*. That is, a distribution that accumulates many observations in smaller or larger values of the predictor. 

For example, consider the distribution of the predictor `DIS` (weighted mean of distances to five Boston employment centers) in the `Boston_data`.

##

```{python}
#| echo: true
#| code-fold: true
#| fig-align: center

plt.figure(figsize=(7,4)) # Create space for figure.
sns.histplot(data = Boston_data, x = 'DIS') # Create the histogram.
plt.title("Histogram of DIS") # Plot title.
plt.xlabel("DIS") # X label
plt.show() # Display the plot
```


We see that many of the predictor values are on the right of the plot, while few are on the left.

## Problems with strong skewness

</br>

Strong skewness in a predictor can:

- Distort the relationship with the response.
- Violate the assumptions of the linear regression model (e.g., linearity, homoscedasticity).
- Lead to influential outliers or poor model fit.

To correct the skewness of a predictor's distribution, we can transform the predictor using the [**Box-Cox transformation**]{style="color:orange;"}

## Box-Cox Transformation

</br>

The Box-Cox transformation uses a family of power transformations on a predictor $X$ such that $X' = X^{\lambda}$, where $\lambda$ is a parameter to be determined using the data. When $\lambda = 0,$ this means $X' = \ln(X)$, where $\ln(\cdot)$ is the natural logarithm.

The Box-Cox transformation is the element of this family that results in a transformed variable that follows a normal distribution (approximately).

The variable $X$ must be strictly positive.

## In Python

In Python, we can apply Box-Cox transformation using the `power_transformation` function of the **scikit-learn** library.

```{python}
#| echo: true

DIS_transform = power_transform(Boston_data[ ['DIS']], method = "box-cox")
DIS_transform
```

```{python}
#| echo: true
#| code-fold: true
#| fig-align: center

plt.figure(figsize=(7,4)) # Create space for figure.
sns.histplot(x = DIS_transform) # Create the histogram.
plt.title("Histogram of DIS") # Plot title.
plt.xlabel("DIS") # X label
plt.show() # Display the plot
```

## Discussion

</br></br>

The Box-Cox transformation can:

- Make the distribution of a predictor more symmetric.

- Help to linearize the relationship with the response.

After transformation, the predictor $X'$ must be used instead of the original predictor $X$.

# Standarization

## Predictors with different units

Many good predictive models have issues with [**numeric predictors**]{style="color:blue;"} with different units:

1.  Methods such as K-nearest neighbors are based on the distance between observations. If the predictors are on different units or scales, then some predictors will have a larger weight for computing the distance.

2.  Other methods such as LASSO use the variances of the predictors in their calculations. Predictors with different scales will have different variances and so, those with a higher variance will play a bigger role in the calculations.

## 

</br></br></br></br>

::: {style="font-size: 150%;"}
[***In a nutshell, some predictors will have a higher impact in the model due to its unit and not its information provided to it.***]{style="color:purple;"}
:::

## Standarization

</br>

Standardization refers to *centering* and *scaling* each numeric predictor individually. It puts every predictor on the same scale.

To **center** a predictor variable, the average predictor value is subtracted from all the values.

Therefore, the centered predictor has a zero mean (that is, its average value is zero).

## 

</br>

To **scale** a predictor, each of its value is divided by its standard deviation.

Scaling the data coerce the values to have a common standard deviation of one.

In mathematical terms, we standardize a predictor as:

$${\color{blue} \tilde{x}_{i}} = \frac{{ x_{i} - \bar{x}}}{ \sqrt{\frac{1}{n -1} \sum_{i=1}^{n} (x_{i} - \bar{x})^2}},$$

with $\bar{x} = \sum_{i=1}^n \frac{x_i}{n}$.

## Example 2 (cont.)

</br>

We concentrate on the five numerical predictors in the `complete_Auto` dataset.

```{python}
#| echo: true

# Select specific variables.
complete_sbAuto = complete_Auto[['cylinders', 'displacement', 
                                 'horsepower', 'weight', 
                                 'acceleration']]
```

## Two predictors in original units

Consider the `complete_sbAuto` dataset created previously. Consider two points in the plot: $(175, 5140)$ and $(69, 1613)$.

::::: columns
::: {.column width="50%"}
```{python}
#| echo: false
#| output: true
#| fig-align: center

plt.figure(figsize=(5,5))
sns.scatterplot(data = complete_sbAuto, x = 'horsepower', y = 'weight')
plt.scatter(x = 175, y = 5140, color = 'red')
plt.scatter(x = 69, y = 1613, color = 'red')
plt.xlabel('Horsepower', fontsize=14)
plt.ylabel('Weight', fontsize=14)
plt.show()
```
:::

::: {.column width="50%"}
</br>

The distance between these points is $\sqrt{(69 - 175)^2 + (1613-5140)^2}$ $= \sqrt{11236 + 12439729}$ $= 3528.592$.
:::
:::::

## Standarization in Python

</br>

To standardize **numerical** predictors, we use the function `StandardScaler()`. Moreover, we apply the function to the variables using the function `fit_transform()`.

</br>

```{python}
#| echo: true

scaler = StandardScaler()
Xs = scaler.fit_transform(complete_sbAuto)
```

## 

Unfortunately, the resulting object is not a pandas data frame. We then convert this object to this format. </br>

```{python}
#| echo: true

scaled_df = pd.DataFrame(Xs, columns = complete_sbAuto.columns)
scaled_df.head()
```

## Two predictors in standardized units

In the new scale, the two points are now: $(1.82, 2.53)$ and $(-0.91, -1.60)$.

::::: columns
::: {.column width="50%"}
```{python}
#| echo: false
#| output: true
#| fig-align: center

plt.figure(figsize=(5,5))
sns.scatterplot(data = scaled_df, x = 'horsepower', y = 'weight')
plt.scatter(x = 1.83, y = 2.54, color = 'red')
plt.scatter(x = -0.915, y = -1.605, color = 'red')
plt.xlabel('Standardized horsepower', fontsize=14)
plt.ylabel('Standardized weight', fontsize=14)
plt.show()
```
:::

::: {.column width="50%"}
</br>

The distance between these points is $\sqrt{(-0.91 - 1.82)^2 + (-1.60-2.53)^2}$ $= \sqrt{7.45 + 17.05} = 4.95$.
:::
:::::

## Discussion

::: incremental
-   Standardized predictors are generally used to improve the numerical stability of some calculations.

-   It is generally recommended to always standardize numeric predictors. Perhaps the only exception would be if we consider a linear regression model.

-   A drawback of these transformations is the [***loss of interpretability***]{style="color:pink;"} since the data are no longer in the original units.

-   Standardizing the predictors does not affect their correlation.
:::

# [Return to main page](https://alanrvazquez.github.io/TEC-IN1002B-Website/)


# Dimension reduction

## Dimension reduction

Here, we will discuss methods to reduce the number of predictors in a dataset. In other words, to reduce the dimension of the dataset.

High-dimensional datasets can cause problems when training or fitting linear regression models. It also happens that a high-dimensional dataset can also have poorer predictive performance than a dataset with few, well selected predictors.

In particular, we will discuss the methods:

1. Filtering predictors based on near-zero variance and correlation.
2. Principal component analysis.

## Removing predictors

</br>

There are potential advantages to removing predictors prior to modeling:

::: incremental
-   Fewer predictors means decreased computational time and complexity.

-   If two predictors are highly-correlated, they are measuring the same underlying information. So, removing one should not compromise the performance of the model.
:::

. . .

Here, we will see two techniques to remove predictors.

## Near-Zero variance predictors

</br>

A **near-zero variance predictor variable** is one that has only a handful of unique values that occur with very low frequencies.

</br>

. . .

If the predictor has a single unique value, then it is called a **zero-variance predictor variable**.

</br>

. . .

Since the values of this predictor variable do not vary or change at all, this predictor does not provide any information to the model and must be discarded.

## Example 3

</br>

We consider a data set related to Glass identification. The data has 214 glass samples labeled as one of seven glass categories. There are nine predictors including the refractive index (RI) and percentages of eight elements: Na, Mg, Al, Si, K, Ca, Ba, and Fe.

Let's read the data set. Note that this is a modified version of the original data.

```{python}
#| echo: true
#| output: true

# Load Excel file (make sure the file is in your Colab)
glass_data = pd.read_excel('glass.xlsx')

# Drop the categorical variable.
sb_glass = glass_data.drop(columns=['Type'])
```

## In Python

We use the function `VarianceThreshold()` to set the threshold for determining a *low* sample variance. We also use other functions such as `selector.fit()` and `selector.get_support()`.

```{python}
#| echo: true
#| output: true

# Set threshold
selector = VarianceThreshold(threshold=0.01)

# Apply threshold.
selector.fit(sb_glass)

# Identify predictors with low variance.
low_variance_cols = sb_glass.columns[~selector.get_support()]

# Print the list of predictors.
print(f"Low variance columns: {low_variance_cols}")
```

## 

</br></br>

After identifying predictors with low variability, we remove them from the problem because they do not add much to the problem. To this end, we use the command below.

</br>

```{python}
#| echo: true
#| output: true

# Removing problematic predictors
sb_reduced_glass = sb_glass.drop(columns=low_variance_cols)
```

## Between-predictor correlation

</br>

[**Collinearity**]{style="color:purple;"} is the technical term for the situation where two predictors have a substantial correlation with each other.

If two or more predictors are highly correlated (either negatively or positively), then methods such as the linear regression model will not work!

To visualize the severity of collinearity between predictors, we calculate and visualize the *correlation matrix*.

## Example 2 (cont.)

</br>

We concentrate on the five numerical predictors in the `complete_Auto` dataset.

```{python}
#| echo: true

# Select specific variables.
complete_sbAuto = complete_Auto[['cylinders', 'displacement', 
                                 'horsepower', 'weight', 
                                 'acceleration']]
```

## Correlation matrix

</br>

In Python, we calculate the correlation matrix using the command below.

```{python}
#| echo: true
#| output: true

correlation_matrix = complete_sbAuto.corr()
print(correlation_matrix)
```

## 

Next, we plot the correlation matrix using the function `heatmap()` from **seaborn**. The argument `annot` shows the actual value of the pair-wise correlations, and `cmap` shows a nice color theme.

```{python}
#| echo: true
#| output: true
#| fig-align: center

plt.figure(figsize=(3,3))
sns.heatmap(correlation_matrix, cmap='coolwarm', annot = True)
```

## 

</br>

The predictors cylinders and displacement are highly correlated. In fact, their correlation is 0.95.

```{python}
#| echo: false
#| output: true
#| fig-align: center

plt.figure(figsize=(7,5))
sns.scatterplot(data = complete_sbAuto, x = 'cylinders', y = 'displacement')
plt.xlabel('cylinders', fontsize = 14)
plt.ylabel('displacement', fontsize = 14)
plt.show()
```

## In practice

</br>

We deal with collinearity by removing the minimum number of predictors to ensure that all pairwise correlations are below a certain threshold, say, 0.75.

We can identify the variables that are highly correlated using quite complex code. However, here we will do it manually using the correlation map.

```{python}
#| echo: true
#| output: true

# Dataset without highly correlated predictors.
reduced_auto = complete_sbAuto[ ['weight', 'acceleration']]
```

